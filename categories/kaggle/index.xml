<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kaggle on head spin - the Heads or Tails blog</title>
    <link>https://heads0rtai1s.github.io/categories/kaggle/</link>
    <description>Recent content in kaggle on head spin - the Heads or Tails blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Jan 2022 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://heads0rtai1s.github.io/categories/kaggle/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Yearly goals: 2022 targets</title>
      <link>https://heads0rtai1s.github.io/2022/01/20/goals-2022-targets/</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2022/01/20/goals-2022-targets/</guid>
      <description>


&lt;p&gt;This is the second part in a series of posts about setting and tracking my yearly goals when it comes to Machine Learning &amp;amp; Data Science (ML &amp;amp; DS) as well as Sports &amp;amp; Exercise. In the &lt;a href=&#34;https://heads0rtai1s.github.io/2022/01/13/goals-2021-results/&#34;&gt;first post&lt;/a&gt; I wrote about my past goals for 2021 and the lessons learned from those. This post will focus on my goals and expectations for 2022, and how last year’s experiences shaped and informed them. This might be more fun to read if you’ve seen the &lt;del&gt;comedy show&lt;/del&gt; documentary Silicon Valley.&lt;/p&gt;
&lt;div id=&#34;stretch-to-avoid-injuries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stretch to avoid injuries&lt;/h3&gt;
&lt;p&gt;Here’s the summary tweet with my stretch goals from the 1st of January:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My 2022 stretch goals:&lt;br&gt;&lt;br&gt;- Spend 400 h learning ML/DS&lt;br&gt;&lt;br&gt;- Join 2 NLP + 2 Image competitions on &lt;a href=&#34;https://twitter.com/kaggle?ref_src=twsrc%5Etfw&#34;&gt;@kaggle&lt;/a&gt;&lt;br&gt;&lt;br&gt;- Team up in 3 competitions&lt;br&gt;&lt;br&gt;- Win 2 Kaggle comp medals&lt;br&gt;&lt;br&gt;- Write 50 blog posts&lt;br&gt;&lt;br&gt;- Run a sub-4h marathon&lt;br&gt;&lt;br&gt;- Do 1 muscle up&lt;br&gt;&lt;br&gt;- Sleep at least 8 h/night with a std dev &amp;lt;= 0.5 h&lt;/p&gt;&amp;mdash; Martin Henze (Heads or Tails) (@heads0rtai1s) &lt;a href=&#34;https://twitter.com/heads0rtai1s/status/1477360618432778242?ref_src=twsrc%5Etfw&#34;&gt;January 1, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;The idea of those being &lt;em&gt;stretch&lt;/em&gt; goals is that I’m not expecting to reach all of them. And that would be fine. Because the important aspect is the &lt;strong&gt;journey towards those goals&lt;/strong&gt;, and the skills and experiences that I hope this journey will give me.&lt;/p&gt;
&lt;p&gt;The goals themselves are still important, and I will do my best in reaching them. Some will go better than others. Trying to become better at something can be a vague target, and concrete goals can be very useful in providing a metric to measure progress and success. Similar to ML, finding a good metric can make a big difference. And of course also similar to ML any metric can be gamed; but you would only be fooling yourself. Thus, when striving for those goals it is essential to remember the purpose behind them, and the vision you had when setting them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/mXuPwP0kxQqvu0M168/giphy.gif&#34; title=&#34;Maybe not that kind of vision&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spend-400-h-learning-mlds&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Spend 400 h learning ML/DS&lt;/h3&gt;
&lt;p&gt;As I wrote in the previous post, hours themselves don’t create mastery. Spending your time effectively is more important that spending lots of it with vague intentions. My aspiration here is to grow my ML/DS skills in specific directions. I realised that I’m still lacking the kind of solid understanding of many ML concepts that would allow me to use them as confidently and creatively as I would like to.&lt;/p&gt;
&lt;p&gt;Let me try to illustrate what I mean. When I encounter a certain data problem - be it on Kaggle, at work, or anywhere - then in my head I construct ideas for ways to process the data to make it easier to solve the problem. For instance, for the &lt;a href=&#34;https://www.kaggle.com/c/titanic&#34;&gt;Kaggle Titanic&lt;/a&gt; challenge I would try to come up with feature engineering steps to figure out whether someone was travelling alone or in groups, and what kind of groups those could be (family? friends?). That’s the creative part. Translating these ideas into code is where the skill comes in. For traditional (tabular) ML I’m reasonably good at this translation, but when it comes to Deep Learning (DL) my process is still slow and inefficient.&lt;/p&gt;
&lt;p&gt;Besides the tabular ML and feature engineering flow, I also know what to aim for in my skill development through the analogy to data visualisation. I’m a big fan of dataviz, and it arguably contributed a lot to my early successes in the Kaggle community. When it comes to a visualisation problem, I’m able to envision the data in my head and then find the right (ggplot2) tools to realise my imagination through code. Ideas flow almost unimpeded through the power of dataviz skills and tools. From there, my limitations (which are still plenty) mostly arise from a lack in creativity rather than from my coding skills. And this is where I want to be with ML/DL. Maybe not this year; but I want to get much closer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;join-2-nlp-2-image-competitions-on-kaggle&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Join 2 NLP + 2 Image competitions on Kaggle&lt;/h3&gt;
&lt;p&gt;One way to make my ML goals even more concrete is through &lt;a href=&#34;https://www.kaggle.com/competitions&#34;&gt;Kaggle competitions&lt;/a&gt;. Challenges throughout the year are manifold and diverse. They typically attract a few thousand people, hundreds among whom are sharing their ideas or code. I can’t think of a better way of learning than to immerse yourself into such a competition for (typically) 3 months and soak up the skills and problem solving ingenuity that others display.&lt;/p&gt;
&lt;p&gt;Specifically, I want to improve my skills in Image problems (aka Computer Vision) and Natural Language Processing (NLP). Both of those are solidly in the domain of DL these days. Image-based challenges are usually plentiful throughout the year. NLP competitions can be less frequent, but it should hopefully still be possible to encounter 2 interesting ones in 2022.&lt;/p&gt;
&lt;p&gt;When it comes to Image data, I’m reasonably happy with my progress in 2021. I started learning about &lt;a href=&#34;https://www.fast.ai&#34;&gt;FastAI&lt;/a&gt; and using it more confidently in building my own competition pipeline and tinkering with custom dataloaders in some competitions. I also started to look into the underlying basic functionality in &lt;a href=&#34;https://pytorch.org&#34;&gt;Pytorch&lt;/a&gt; and &lt;a href=&#34;https://torch.mlverse.org&#34;&gt;torch for R&lt;/a&gt; (FastAI is a high-level wrapper for Pytorch). While the way that torch works is pretty intuitive, I need to spend more time with the code itself to become more confident in using it creatively.&lt;/p&gt;
&lt;p&gt;For NLP, the &lt;a href=&#34;https://huggingface.co&#34;&gt;huggingface libraries&lt;/a&gt; have fast become the gold standard, and the ecosystem is growing and evolving rapidly. I enjoyed taking the first &lt;a href=&#34;https://huggingface.co/course/chapter1/1&#34;&gt;huggingface course&lt;/a&gt; last year, and I plan to continue with those courses to learn more. There are a few libraries that bring the huggingface tools into the FastAI ecosystem; which is an ideal combination for me at this stage. I plan to focus on those, besides the general huggingface framework, and will hopefully blog about it every now and then.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/LpR8z8GqoyktA3141E/giphy.gif&#34; title=&#34;I promise I&amp;#39;ll try my best to blog about it.&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;team-up-in-3-competitions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Team up in 3 competitions&lt;/h3&gt;
&lt;p&gt;I thoroughly enjoyed my teaming up experiences in 2021 and plan to continue this approach in 2022. There is a lot that you can learn from your team mate(s) on Kaggle, since many people come from such different professional and technical backgrounds. The goal of 3 competitions seems relatively low compared to the rest of the goals, but I’m aiming for gradual progress here. Last year it was 2 competitions.&lt;/p&gt;
&lt;p&gt;Finding the right balance in a team can be a challenge, as is the way that progress is planned and implemented. Communication is really crucial here; as in so many areas of life. This applies not necessarily to the goal of doing well in a Kaggle competition, but to getting the most out of the learning experiences that are enabled by the process of collaboration. In all of those aspects I consider myself fortunate in having teamed up with &lt;a href=&#34;https://twitter.com/YassineAlouini&#34;&gt;Yassine&lt;/a&gt;, and I’m looking forward to our future collaborations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/jsI8nBXJl6s7r7iuJ5/giphy.gif&#34; title=&#34;Tres competiciones&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;win-2-kaggle-comp-medals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Win 2 Kaggle comp medals&lt;/h3&gt;
&lt;p&gt;In 2021, I purposefully didn’t set a goal of a certain performance in a Kaggle competition. I just wanted to grow my (DL) skills by spending more time on Kaggle problems. Now, I feel like I’m at a stage where I can expect to develop those skills to the extend where my progress translates to better finishes on competition leaderboards.&lt;/p&gt;
&lt;p&gt;I’m a fan of the Kaggle medal system, and of the gamification aspect in general. I consider it an incarnation of the idea of concrete metrics replacing otherwise vague goals that I wrote about at the beginning of this post. But here there can be an even greater incentive to game the system and take the medals as a goal in themselves, rather than a reflection of your skills. Sometimes, it might be possible to win a (bronze) medal by forking a public notebook or blindly ensembling other people’s solutions without understanding them (and getting lucky in the shake-up). That’s not what I want. Those medals don’t really count for anything, since they are disconnected from your actual abilities.&lt;/p&gt;
&lt;p&gt;I want to write my own code and understand (ideally) every line of it. Inspired by other people’s ideas, yes; but incorporating those inspirations purposefully and in an informed way into my own pipelines. Plan my experiments intelligently and choose the best progress based on the results of those experiments. Those medals will mean something. Most importantly, they will mean something to me. Setting this goal and putting it out there will create a bit more pressure to succeed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/QrcujnCu5qWVyea2Xc/giphy.gif&#34; title=&#34;Maybe it was a bit too much pressure.&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-50-blog-posts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Write 50 blog posts&lt;/h3&gt;
&lt;p&gt;Blogging was an aspect of my journey that suffered in 2021. Of the 12 posts that I had planned, I wrote only 2. But instead of lowering the bar more, I decided to raise it to 50 instead. This might not make a lot of intuitive sense, since a target of 50 looks way more intimidating than a mere dozen. However, I realised that it wasn’t really the number of posts that stopped me from writing. It was the process of writing a single posts, which I saw as a significant endeavour that needed preparation and research and comprehensive content to be worth it.&lt;/p&gt;
&lt;p&gt;I can be a bit of a perfectionist, which sometimes stops me from making public something that I had written but wasn’t happy with. And I would hate to turn into the kind of spammer who just churns out cookie-cutter posts in the search for engagement and followers. You know the ones. Although I don’t expect a large audience for those posts, I want to write something of value for those who stumble across my blog. There’s enough noise on the internet already.&lt;/p&gt;
&lt;p&gt;But here’s the crux: writing valuable content doesn’t have to mean writing long and/or polished content. Short snippets of concise code or brief reflections on recent learnings can have as much value as posts that have been months in the making. I simply need to get out of my comfort zone and write more.&lt;/p&gt;
&lt;p&gt;One benefit of writing is analogous to the idea that teaching a concept to someone is a great way of learning about this concept. If you can’t explain it, then do you really understand it? And if you can’t write about it clearly and concisely, then chances are you don’t actually understand it either. But understanding something is not necessarily a binary thing, and sharing even small progress in learning can be useful to people in similar situations.&lt;/p&gt;
&lt;p&gt;And after writing only 2 posts in 2021 (albeit more technical posts), this will be already the second post of 2022.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;run-a-sub-4h-marathon&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Run a sub-4h marathon&lt;/h3&gt;
&lt;p&gt;My first ever marathon in 2021 (not a race, just the distance) clocked in at 4 hours 25 minutes. Not too bad; now let’s see whether we can get under 4 hours. Shaving off 25 minutes of 42km might not look much at the outset, but it’s about 35s per km, which here is the difference between a 5:45 pace (4h-goal) and a 6:20 pace (last year), which is pretty noticeable. (Being European, I measure my pace in min/km.)&lt;/p&gt;
&lt;p&gt;My pace for shorter runs is significantly faster than that; and I’ve run a personal best 1h 40min half-marathon (i.e. 4:45 pace). However, a marathon becomes very different after the 2h mark, and most certainly during the final 10km. I will probably still need to take short stretches of walking with water and energy gels, which need to be factored into the overall running pace. I feel like being at 5:00 pace for the first half and then aiming for 5:30 - 6:30 pace for the remainder sounds realistic. In contrast, last year I dropped to 7:30 pace for the last 12km, albeit having done quite well until km 25. My plan is to work more on the stages between km 20 and 30 with long training runs of 2h to 3h durations during most weekends. Should be fun!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/cnipK4uGHhudr6tels/giphy.gif&#34; title=&#34;We might have different definitions of &amp;#39;fun&amp;#39;.&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;do-1-muscle-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Do 1 muscle up&lt;/h3&gt;
&lt;p&gt;This is a repetition of the 2021 goal, since I didn’t accomplish anything in that direction. For 2022, I will have to build a better plan to make at least some progress. I’m pretty sure that a muscle up is mostly a question of technique, given a certain level of strength. None of that silly cross-fit-style swingy stuff though; I’m talking about proper technique. There are different stages of the muscle up movement, and I hope to be able to train some of those in isolation.&lt;/p&gt;
&lt;p&gt;Instead of trying to build out the strength first (and my pull up continuity in 2021 wasn’t too bad), this year I will invest more time to research the technique and put together a plan to get me progressively closer to my goal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sleep-at-least-8-hnight-with-a-std-dev-0.5-h&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sleep at least 8 h/night with a std dev &amp;lt;= 0.5 h&lt;/h3&gt;
&lt;p&gt;This last goal might sound weirdly specific, but it’s all about continuity. As a student, then PhD student, then astronomer (of all things!) it’s easy to slip into weird sleeping habits. Even if you don’t actually have to spend your nights at a telescope. Working at a startup might sound like one of those “out of the frying pan and into the fire” type situations when it comes to being sleep deprived, but I’m happy to tell you that there are startups who recognise the value of a healthy work-life balance. Since more and more research is highlighting the importance of a good night’s sleep, I identified this goal as foundational for all the other goals in this post.&lt;/p&gt;
&lt;p&gt;It’s honestly also something that you should listen to your body for. From time to time it might be necessary to work late to meet a deadline, but the thought that your brain won’t be impacted by only 4h of shut-eye is pretty untenable. You pay for burning the midnight oil with reduced cognitive spark over the following days, so your average productivity still goes down even though you worked all those extra hours. And you might be able to get away with it for a while in your 20s, since that’s pretty much the kind of bad decisions that your 20s are for, but it’s still gonna catch up with you eventually. Anyway, no lecturing intended here. Sleep is super important, and I suggest to use it smartly to get where you want to get.&lt;/p&gt;
&lt;p&gt;So what about that goal? Well, last year I managed to exceed my planned average of 8h / night. But you can famously drown in a river that’s only 30cm deep on average. Variance matters, and there was too much of that in my sleep patterns. Once more, I was looking for a concrete goal to improve my habits. A standard deviation of less than 30 mins means that in 67% of nights I would be less than +/- 15 min away from my average goal. That sounds broadly doable. Last year’s standard deviation was 1h.&lt;/p&gt;
&lt;p&gt;I’m not suggesting to run around with a stop watch either, or try to plan your day down to the minute. That’s a recipe for a neurotic type of disaster worthy of a silly and slightly problematic 80s comedy. My approach is to pay a bit more attention to guide my habits towards being more sustainable and then let routine take over. You won’t always able to stick to those habits every single day, but if you’re generally maintaining them that’s already a big improvement. So don’t sleep on sleep.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/Qw4X3FJbFEXeglZ7s6A/giphy.gif&#34; title=&#34;Especially when travelling long distances.&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Now you know some of my plans for 2022. I’d be happy to read about yours, in whichever shape or form you’d like to share them.&lt;/p&gt;
&lt;p&gt;You might have noticed a pattern in most of my goals, and that is something I will write about more in detail in a future post: my overarching aim is for my 2022 self to be better than my 2021 self. Comparing yourself to other people can be a futile and demoralising exercise, especially if you, like me, tend to look to the most high-achieving people in your field or community for inspiration. Even more so in this age of social media highlight reels. In contrast, I believe that comparing yourself to a previous baseline of yourself can be a great way to learn from past mistakes and experiences. Look at what worked, slightly tweak those things that didn’t work to see if that improves the situation.&lt;/p&gt;
&lt;p&gt;It’s the basis for ML experiments, which comes from the basis of any science experiments, which comes from the scientific method, which comes from an evidence-based approach to trying to understand this often perplexing world in which we find ourselves. Reflecting on your choices and their consequences is a valuable tool in any context. Or as Socrates used to say: “The unexamined life is not worth living”.&lt;/p&gt;
&lt;p&gt;In the third and final part of this goals series I will write about measuring my goals and (finally!) bring in some code and visualisations. Stay tuned!&lt;/p&gt;
&lt;p&gt;This is the end of this post.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/mXuPwxly5V2xL1uAow/giphy.gif&#34; title=&#34;Go and be awesome!&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Yearly goals: 2021 experiences</title>
      <link>https://heads0rtai1s.github.io/2022/01/13/goals-2021-results/</link>
      <pubDate>Thu, 13 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2022/01/13/goals-2021-results/</guid>
      <description>


&lt;p&gt;At the beginning of last year, I got the motivation from other people’s Twitter posts to set and track my own goals for what would surely be the year we would emerge from the shadows of the pandemic. With an ever so slightly more jaded outlook to life, I’m doing the same this year. On a global level, many societies didn’t seem to have learned an aweful lot from the Covid years of 2020 and 2021. From a personal perspective, I hope that the experience of setting and sharing my goals has helped me to define better goals for 2022. This post is a brief retrospective on the 2021 part of my journey, and the lessons I hope to have learned for the road ahead.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://thumbs.gfycat.com/NauticalPastLadybird-size_restricted.gif&#34; title=&#34;Carry on ...&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;how-did-your-year-go&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How did your year go?&lt;/h3&gt;
&lt;p&gt;Those were my 2021 stretch goals and the tally I drew at the end of the year:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Final tally, not too bad:&lt;br&gt;&lt;br&gt;✅ Spend 500 h on &lt;a href=&#34;https://twitter.com/kaggle?ref_src=twsrc%5Etfw&#34;&gt;@Kaggle&lt;/a&gt;&lt;br&gt;&lt;br&gt;✅ Join 4 competitions and team up in 2 of them&lt;br&gt;&lt;br&gt;✅ Publish 52 episodes of &lt;a href=&#34;https://twitter.com/hashtag/KaggleHiddenGems?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#KaggleHiddenGems&lt;/a&gt;&lt;br&gt;&lt;br&gt;❌ Write 12 blog posts&lt;br&gt;&lt;br&gt;✅ Run my 1st marathon&lt;br&gt;&lt;br&gt;❌ Do 1 muscle up&lt;br&gt;&lt;br&gt;✅ Sleep 8 h/night&lt;/p&gt;&amp;mdash; Martin Henze (Heads or Tails) (@heads0rtai1s) &lt;a href=&#34;https://twitter.com/heads0rtai1s/status/1476293217779920901?ref_src=twsrc%5Etfw&#34;&gt;December 29, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Given that 2021 was year two of a global plague, it might be fair to add ‘having survived all that’ to the list of accomplishments. But I’m aware (and grateful) of my priviledged situation of being able to work from home on my computer, so let’s not cheapen the accomplishments of those who actually were out there working hard and who managed to stave off (or overcome) serious illness. A bit of perspective is necessary, from time to time.&lt;/p&gt;
&lt;p&gt;At the same time, I think it is important to highlight the degree of structure and control that can be provided by setting goals and trying to reach them; however much arbitrary or insignificant those goals might appear in the bigger picture. I believe that especially in uncertain times there is a lot of comfort in wrestling a modicum of control from the chaotic universe. As long as we do the best we can do to our abilities (and circumstances) then we’re doing rather well. But enough existentialism. Let’s get back to data science.&lt;/p&gt;
&lt;p&gt;On the goals themselves, I’d like to go into more detail than the Twitter thread allowed. I’d like to emphasise that those are stretch goals, and that I didn’t expect to reach all of them. Wouldn’t be fun it were too easy, after all.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spend-500-hours-on-kaggle&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Spend 500 hours on Kaggle&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; Now, hours themselves don’t create mastery; it’s about what you do during those hours that counts. A short time of focussed effort counts for more than a much longer period of distracted dabbling. Having said that: setting a specific goal that translates to 1-2 hours per day can help immensely in creating habits and budgeting time.&lt;/p&gt;
&lt;p&gt;For the latter, I would often spend about 1 hour per day on Kaggle problems during the week, and 2 or 3 hours on the weekend. Calling it a (Kaggle) day after that helped to avoid the temptation of working later at night, especially during the week. Kaggle challenges are more akin to a marathon (&lt;em&gt;foreshadowing …&lt;/em&gt;) than a sprint, which means that pacing yourself becomes important. In general, having a healthy work-life balance is necessary to avoid burn out and loss of motivation (or health).&lt;/p&gt;
&lt;p&gt;The habit-building effect of having a specific goal can hardly be overstated. Continuity is one of the main factors of successful learning and growing. If you’re spending (almost) every day on a certain project, or practicing a specific tool, then you are bound to make at least some progress in understanding. And learning is what Kaggle is mostly about, for me. (And the &lt;a href=&#34;https://heads0rtai1s.github.io/2019/04/19/great-community-kaggledays19/&#34;&gt;community&lt;/a&gt;, of course.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;join-4-competition-and-team-up-in-2-of-them&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Join 4 competition and team up in 2 of them&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; This continues the previous train of thought: competiting is less about the ranks and medals, but more a motivation for learning new skills and tools. I always found it most efficient to learn by doing - and to learn from examples and applications. Instead of merely reading books of theory, or even code, I prefered to focus on the skills that I needed to solve a specific problem. And what better challenge to spend 3 months at a time competing with the brightest applied-ML minds on Kaggle on a new and exiting problem.&lt;/p&gt;
&lt;p&gt;In 2021, I participated earnestly in 5 competitions. I also briefly dabbled in a few others; but honestly less is more in this kind of situation. You want to focus on one thing at a time, instead of scattering your attention across lots of projects and not really devoting enough time to any of them. I know, the temptation is great to jump into every new competition that launches. I’m trying myself to recover from that; so I recommend you do as I say, not do as I do. Having said that, 2021 for me was a notable improvement over previous year.&lt;/p&gt;
&lt;p&gt;A lot of the learning effect on Kaggle comes from reading other people’s contributions in form of Notebooks or Discussion posts. This kind of effect is applified by a factor of a lot when you team up with other competitors. I had only done this very occasionally in the past. The main reason was probably that I didn’t have enough confidence in my own skills to contribute meaningfully to a team. I don’t want to be carried. I want to make a significant difference. Having learnt a bit more in 2020 I decided to get outside of my comfort zone and challenge myself in a team.&lt;/p&gt;
&lt;p&gt;It turned out to be a very fun and rewarding experience. I joined forces with my Kaggle buddy &lt;a href=&#34;https://twitter.com/YassineAlouini&#34;&gt;Yassine&lt;/a&gt; on 2 imaging competitions and learned a lot from him on deep learning architectures and training strategies. On the way, I became much more confident in using the high-level &lt;a href=&#34;https://www.fast.ai/&#34;&gt;FastAI&lt;/a&gt; framework and also &lt;a href=&#34;https://colab.research.google.com&#34;&gt;Google Colab&lt;/a&gt;, which is a great cloud environment for GPU notebooks. Read my mid-2020 review on cloud GPUs &lt;a href=&#34;https://heads0rtai1s.github.io/2021/08/24/colab-plus-kaggle-cloud-gpu/&#34;&gt;right here&lt;/a&gt;. Bottom line: there are lots of great things to be found in the clouds these days.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media0.giphy.com/media/3oEdv6UTqzNk9Y5i36/giphy.gif?cid=790b76119299a8b65e0c47548fe0e8dbd2d4ca4fefe9698e&amp;amp;rid=giphy.gif&amp;amp;ct=g%20...&#34; title=&#34;Burgers not included in the Colab Pro Plus subscription ... yet&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;publish-52-episodes-of-kaggle-hidden-gems&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Publish 52 episodes of Kaggle Hidden Gems&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; The Hidden Gems series is my attempt to give back something to the Kaggle community by discovering and promoting underrated Notebooks published on the platform. The Kaggle community has grown enormously over the last years. So many bright people from many walks of life contribute to the community; which is great because you have the chance to learn from them. One difficulty, though, of this higher volume of contributions is that sometimes great content can have difficulties standing out and being recognised for its quality.&lt;/p&gt;
&lt;p&gt;Every week I pick 3 new, underrated Notebooks to highlight. Here’s the most recent example at the time of writing: &lt;a href=&#34;https://www.kaggle.com/general/300228&#34;&gt;episode number 88&lt;/a&gt;. As a result my week doesn’t feel complete now without reading cool
Kaggle Notebooks. In 2021 I succesfuly managed to establish a continuity of one episode every Tuesday, without fail. 52 episodes in 52 weeks. On the occasion of episode 50, I gathered all the Gems data into a &lt;a href=&#34;https://www.kaggle.com/headsortails/notebooks-of-the-week-hidden-gems&#34;&gt;Kaggle dataset&lt;/a&gt; and put together a &lt;a href=&#34;https://www.kaggle.com/headsortails/hidden-gems-a-collection-of-underrated-notebooks/&#34;&gt;Starter Notebook&lt;/a&gt;. This is Kaggle, after all, where even the data has data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-12-blog-posts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Write 12 blog posts&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Fail.&lt;/strong&gt; I only wrote 2 posts in total in 2021 (after writing 9 in 2019, and 5 in 2020). Establishing and keeping up a blogging habit is a question of time, certainly. But in my case, it might also be a challenge of perfectionism and preference for long posts. Like this post here, which is already shaping up to be longer than I had intended.&lt;/p&gt;
&lt;p&gt;Going out of my comfort zone when it comes to writing isn’t easy at all. I don’t expect a large audience for this blog. I’m writing mostly for myself, so that I can remember and digest some of the things that I picked up over time. At the same time, I want for anyone who stumbles across those posts to get some quality information out of reading them. And that holds me back from publishing more frequent and shorter posts. That is one lesson learnt from 2021.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;run-my-1st-marathon&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Run my 1st marathon&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; I gotta be honest: this is probably my proudest achievement. My first ever marathon. 42 km in one go. After failing a few attempts during the year, I finally got there on Dec 6th. Just in time to make it a 2021 achievement; but in time nonetheless. Running in colder weather was also more conducive to success, as opposed to my summer attempt in August. Who would have guessed.&lt;/p&gt;
&lt;p&gt;For me, exercise has always been a way to destress and detach from coding and data analysis work, which isn’t the most physically active of all occupations. As such, I didn’t come cold into this marathon challenge but have a certain level of fitness that I’ve built and maintained over the years. When the pandemic started, running outdoors was one of the few exercises that I felt comfortable doing. Having never ran more than 10 km at a time before (and that was quite a few years ago), I then gradually worked up to this marathon challenge. From 5k to 10k, and then running my first half-marathon (21 km) in 2020.&lt;/p&gt;
&lt;p&gt;I didn’t read a lot of running blogs or stuff, so some of the things I discovered for myself might have been easier to read up on, in hindsight. None of this is expert recommendation, obviously. One insight is that a full marathon is a very different beast from a half-marathon. The 21 km of the half-marathon are something that I could run without stopping or nutrition; even without water (although that might not be generally recommended). None of that worked for me for the marathon.&lt;/p&gt;
&lt;p&gt;Turns out that after 2-ish hours of continuous exercise you have burned through most of your carb reserves (i.e. glycogen). I genuinely didn’t know that. Coincidentally, my longest non-stop exercises thus far were just under the 2h limit (and years ago). So, in my first past 30-km attempt I had to stop with leg cramps and fatigue, and it took me a couple hours and a good meal to recover from that. I later found out about that glycogen limit and that I had literally ran out of energy to burn. Fun times. So for the next attempts I got some energy gels plus water with electrolytes and it made a huge difference. Almost no issues after the successful marathon. Just the fun of walking up and down stairs for the days after.&lt;/p&gt;
&lt;p&gt;In numbers: I ran a total of 1332 km in 2021, which translates to about 1/30th around the earth, in 116 hours. I also logged about 3.7 million steps, for an average of about 10k steps per day. As a data addict, I’m using a Garmin watch and app for all those measurements. No link, since they haven’t given me a sponsorship yet. Gotta keep running.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media4.giphy.com/media/9rRacglGbs68E/giphy.gif?cid=ecf05e472rkl4hg798m9nz6ikhrrr7nh9snqzkicv67yti1i&amp;amp;rid=giphy.gif&amp;amp;ct=g&#34; title=&#34;And running. And running.&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;do-1-muscle-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Do 1 muscle up&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Fail.&lt;/strong&gt; Another failure. I didn’t even get close. I did pull-ups frequently, after my shorter runs, but couldn’t get a rhythm going. Another excuse is that I lost access to a decent pull-up bar early in the year, so that I had to make do with the crossbars of small football goals or with playground structures, both of which always have bars that are at the very least slightly too thick to grab them comfortably. If they’re not weirdly shaped to begin with. I’m sure that everyone knows exactly what I’m talking about. Very frequent situation.&lt;/p&gt;
&lt;p&gt;In my eternal optimism, I had also hoped to finish the marathon by mid year. And then focus on the muscle up afterwards. As it turned out, that didn’t quite work either; what with me needing until December to run the marathon and all. So, the muscle up needed more effort due to the conditions not being as ideal, which was more than I could do at the time. Another reason for the failure was poor planning, as I didn’t look for specific muscle-up progressions that would take me there.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sleep-8-hnight&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sleep 8 h/night&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; This was likely the most important aspect. Sleep and rest are super important. We can only burn the midnight oil for so long before it has bad consequences for our mental and physical well-being. Balance is vital. I was very happy to reach and exceed my goal.&lt;/p&gt;
&lt;p&gt;But even though I got an average 8.4h over the year, there remained a notable variance of about 1 hour. I sometimes had to make up for shorter nights with sleeping longer at other times. I’m far from being an expert on sleep science, but from what I have read I believe that consistency is needed for building and maintaining healthy sleep patterns and restful recovery. Those patterns of mine still need some work, as the variations in sleep and wake times were also larger than I would have wanted.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;I’m going to end this post here. Originally, I was planning to put 2021 results and 2022 goals in the same post, but this one is pretty long already. See the lesson above on failing to write frequent and short posts.&lt;/p&gt;
&lt;p&gt;Coming up: how I’m building on these experiences for my 2022 journey.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Colab Pro&#43; Features, Kaggling on Colab, and Cloud GPU Platforms</title>
      <link>https://heads0rtai1s.github.io/2021/08/24/colab-plus-kaggle-cloud-gpu/</link>
      <pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2021/08/24/colab-plus-kaggle-cloud-gpu/</guid>
      <description>


&lt;p&gt;In the final, hectic days of a recent &lt;a href=&#34;https://www.kaggle.com/c/seti-breakthrough-listen&#34;&gt;Kaggle competition&lt;/a&gt; I found myself in want of more GPU power. As one often does in such an occasion. My own laptop, with its &lt;a href=&#34;https://heads0rtai1s.github.io/2021/02/25/gpu-setup-r-python-ubuntu/&#34;&gt;GPU setup&lt;/a&gt;, was doing a fine job with various small models and small images, but scaling up both aspects became necessary to climb the leaderboard further. My first choice options GCP and AWS quickly turned out to require quota increases which either didn’t happen or took too long. Thus, I decided to explore the paid options of &lt;a href=&#34;https://research.google.com/colaboratory/&#34;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I had only ever used the free version of Colab, and found 2 paid subscriptions: Colab Pro and Colab Pro+. The Plus version, at a not insignificant $50/month, was advertising “priority access to faster GPUs” compared to the 10 bucks/month Pro tier. While researching more about this and other vaguenesses in the feature descriptions that the website was offering up, I quickly realised that Plus had only be launched a day or two earlier. And nobody really seemed to know what the exact specs were. So I thought to myself “in for a penny, in for a pound”. Or about 36 pounds at the current exchange rate. Might as well get Plus and figure out what it’s about.&lt;/p&gt;
&lt;p&gt;This post describes the features I found in Colab Pro+, alongside some notes how to best use any version of Colab in a Kaggle competition. After sharing some findings &lt;a href=&#34;https://twitter.com/heads0rtai1s/status/1428833896330899463&#34;&gt;on Twitter&lt;/a&gt;, I received a number of very useful suggestions for Colab alternatives which are compiled in the final part of the post.&lt;/p&gt;
&lt;div id=&#34;colab-pro-features&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Colab Pro+ features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;GPU resources:&lt;/strong&gt; The Plus subscription gave me access to 1 V100 GPU in its “High-RAM” GPU runtime setting. I could only run a single of these sessions at a time. Alternatively, the “Standard” RAM runtime option allowed me to run 2 concurrent sessions with 1 P100 GPU each.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The “High-RAM” runtime did justify its name by providing 53GB of RAM, alongside 8 CPU cores. In my “Standard” RAM sessions I got 13GB RAM and 2 CPUs (which I think might be what’s included in the free Colab).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;runtime limit&lt;/strong&gt; for any session is 24 hours; which was consistent throughout my tests. The Plus subscription advertises that the notebook keeps running even after closing the browser, which can be a useful feature. But I didn’t test this. Be aware that even in Pro+ the runtime still disconnects after a certain time of inactivity (i.e. no cells are running).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With large data, &lt;strong&gt;storage&lt;/strong&gt; is important. Compared to the 100GB of the Pro subscription, Pro+ provided me with 150GB of disk space. This turned out to make a crucial difference in allowing me to copy all the train plus test data, in addition to pip installing updated libraries.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What I didn’t test: Colab’s TPU runtime as well as the number of concurrent CPU sessions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Are those features worth the $50/month? On the one hand, having 24h of V100 power is a notable step up from the free Colab and Kaggle resources. On the other hand, being restricted to 1 session at a time, or 2 sessions with slower P100s, can be a limiting factor in a time crunch.&lt;/p&gt;
&lt;p&gt;Also note, that the Colab FAQ states that “Resources in Colab Pro and Pro+ are prioritized for subscribers who have recently used less resources, in order to prevent the monopolization of limited resources by a small number of users.” Thus, it seems unlikely that one could use a V100 GPU 24/7 for an entire month. I intend to run more experiments and might encounter this limit sooner or later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kaggling-on-colab&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Kaggling on Colab&lt;/h3&gt;
&lt;p&gt;If you’ve exceeded your (considerable) Kaggle resources for the week or, like me, need a bit more horse power for a short time, then moving your Kaggle Notebook into Colab is a good option to keep training and experimenting. It can be non-trivial, though, and the 2 main challenges for me were getting the data and setting up the notebook environment. Well, that and Colab’s slightly infuriating choice to forego many of the standard Jupyter keyboard shortcuts (seriously: why?).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Get your data into Colab:&lt;/strong&gt; by far the best and fastest way here is to copy the data via their &lt;code&gt;GCS_DS_PATH&lt;/code&gt;; i.e. Google Cloud Storage path. Since Kaggle was acquired by Google in 2017, there has been significant integration of its framework into Google’s cloud environments. Kaggle datasets and competition data have cloud storage addresses and can be quickly moved to Colab from there.&lt;/p&gt;
&lt;p&gt;You can get the &lt;code&gt;GCS_DS_PATH&lt;/code&gt; by running the following code in a Kaggle Notebook. Substitute &lt;code&gt;seti-breakthrough-listen&lt;/code&gt; with the name of your competition or dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from kaggle_datasets import KaggleDatasets
GCS_DS_PATH = KaggleDatasets().get_gcs_path(&amp;quot;seti-breakthrough-listen&amp;quot;)
print(GCS_DS_PATH)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within Colab you can then use the &lt;code&gt;gsutil&lt;/code&gt; tool to copy the dataset, or even individual folders, like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;!gsutil -m cp -r {GCS_DS_PATH}/&amp;quot;train&amp;quot; .
!gsutil -m cp -r {GCS_DS_PATH}/&amp;quot;test&amp;quot; .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This retrieves the data significantly faster than by copying from Google Drive or downloading via the (otherwise great) &lt;a href=&#34;https://github.com/Kaggle/kaggle-api&#34;&gt;Kaggle API&lt;/a&gt;. And of course getting the data counts towards your 24 hour runtime limit. Keep in mind that the data is gone after your session gets disconnected, and you need to repeat the setup in a new session.&lt;/p&gt;
&lt;p&gt;The same is true for any files you create in your session (such as trained model weights or submission files) or for &lt;strong&gt;custom libraries&lt;/strong&gt; that you install. Colab has the usual Python and Deep Learning tools installed, but I found the versions to be rather old. You can update via &lt;code&gt;pip&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;!pip install --upgrade --force-reinstall fastai==2.4.1 -qq
!pip install --upgrade --force-reinstall timm==0.4.12 -qq
!pip install --upgrade --force-reinstall torch -qq&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two things to note: after installing you need to restart your runtime to be able to &lt;code&gt;import&lt;/code&gt; the new libraries. No worries: your data will still be there after the restart. Also make sure to leave enough disk space to install everything you need.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Save your outputs on Drive:&lt;/strong&gt; A final note to make sure to copy the results of your experiments - whether they’re trained weights, submission files, or EDA visuals - to your Google Drive account to make sure you don’t lose them when the runtime disconnects. Better to learn from my mistakes than by making them yourself. You can always download stuff manually, but I found that copying them automatically is more reliable.&lt;/p&gt;
&lt;p&gt;You can mount Drive in your Colab notebook like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from google.colab import drive
drive.mount(&amp;#39;/content/drive&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then copy files e.g. via Python’s &lt;code&gt;os.system&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-cloud-gpu-options&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other cloud GPU options&lt;/h3&gt;
&lt;p&gt;While Colab and its Pro versions, as outlined above, have several strong points in their favour, you might want explore other cloud GPU alternatives that either offer more power (A100s!) or are cheaper or more flexible to use. The options here were contributed by other ML practitioners &lt;a href=&#34;https://twitter.com/heads0rtai1s/status/1428833896330899463&#34;&gt;via Twitter&lt;/a&gt; and are listed in no particular order. I’m not including the well-known GCP or AWS here, although someone recommended preemptible (aka “spot”) instances on these platforms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://gradient.paperspace.com/pricing&#34;&gt;Paperspace Gradient&lt;/a&gt;: the G1 subscription costs $8/month and gives free instances with smaller GPUs and a 6h runtime limit. Beyond that, pay $2.30/h to run &lt;a href=&#34;https://docs.paperspace.com/gradient/more/instance-types&#34;&gt;an V100 instance&lt;/a&gt;. 200GB storage included, and 5 concurrent running notebooks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.jarvislabs.ai/&#34;&gt;JarvisCloud&lt;/a&gt;: great landing page, plus A100 GPUs at $2.4/h are being attractive features here. They also offer up-to-date Pytorch, FastAI, Tensorflow as pre-installed frameworks. Storage up to 500GB at max 7 cents per hour.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vast.ai/console/create/&#34;&gt;Vast.ai&lt;/a&gt;: is a marketplace for people to rent out their GPUs. You can also access GCP, AWS, and Paperspace resources here. Prices vary quite a bit, but some look significantly cheaper than those of the big players at a similar level of reliability.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.oracle.com/cloud/compute/pricing.html&#34;&gt;OracleCloud&lt;/a&gt;: seems to be at about $3/h for V100, which is comparable to AWS. A100s “available soon”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.ovhcloud.com/en/public-cloud/prices/&#34;&gt;OHVcloud&lt;/a&gt;: a French provider known for being not very expensive. They have 1 V100 with 400GB storage starting at $1.7/h; which is not bad at all.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Plenty of options available to explore further. Maybe we’ll see prices drop a bit more amidst this healthy competition.&lt;/p&gt;
&lt;p&gt;Hope those are useful for your projects on Kaggle or otherwise. Have fun!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Animations in the time of Coronavirus</title>
      <link>https://heads0rtai1s.github.io/2020/04/30/animate-map-covid/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2020/04/30/animate-map-covid/</guid>
      <description>
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The first four months of 2020 have been dominated by the Coronavirus pandemic (aka COVID-19), which has transformed global life in an unprecedented way. Societies and economies struggle to adapt to the new conditions and necessary contraints. A reassuringly large fraction of governments around the world continue to take evidence-based approaches to this crisis that are grounded in large scale data collection efforts. Most of this data is being made publicly available and can be studied in real time. This post will describe how to extract and prepare the necessary data to animate the spread of the virus over time within my native country of Germany.&lt;/p&gt;
&lt;p&gt;I have published a pre-processed version of the relevant data for this project as a &lt;a href=&#34;https://www.kaggle.com/headsortails/covid19-tracking-germany&#34;&gt;Kaggle dataset&lt;/a&gt;, together with the geospatial shape files you need to plot the resulting map. This post outlines how to build that dataset from the original source data using a set of &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; tools. Then we will use the &lt;a href=&#34;https://github.com/thomasp85/gganimate&#34;&gt;gganimate&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/sf/index.html&#34;&gt;sf&lt;/a&gt; packages to create animated map visuals.&lt;/p&gt;
&lt;p&gt;Those are the packages we need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs &amp;lt;- c(&amp;#39;dplyr&amp;#39;, &amp;#39;tibble&amp;#39;,      # wrangling
          &amp;#39;stringr&amp;#39;, &amp;#39;readr&amp;#39;,     # strings, input
          &amp;#39;lubridate&amp;#39;, &amp;#39;tidyr&amp;#39;,   # time, wrangling
          &amp;#39;knitr&amp;#39;, &amp;#39;kableExtra&amp;#39;,  # table styling
          &amp;#39;ggplot2&amp;#39;, &amp;#39;viridis&amp;#39;,   # visuals
          &amp;#39;gganimate&amp;#39;, &amp;#39;sf&amp;#39;,      # animations, maps
          &amp;#39;ggthemes&amp;#39;)             # visuals
invisible(lapply(libs, library, character.only = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The COVID-19 data for Germany are being collected by the &lt;a href=&#34;https://www.rki.de/EN/Home/homepage_node.html&#34;&gt;Robert Koch Institute&lt;/a&gt; and can be download through the &lt;a href=&#34;https://npgeo-corona-npgeo-de.hub.arcgis.com/&#34;&gt;National Platform for Geographic Data&lt;/a&gt; (which also hosts an interactive dashboard). The earliest recorded cases are from 2020-01-24. Here we define the corresponding link and read the dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;infile &amp;lt;- &amp;quot;https://opendata.arcgis.com/datasets/dd4580c810204019a7b8eb3e0b329dd6_0.csv&amp;quot;
covid_de &amp;lt;- read_csv(infile, col_types = cols())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This data contains a number of columns which are, unsurprisingly, named in German:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covid_de %&amp;gt;% 
  head(5) %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 5
## Variables: 18
## $ FID                  &amp;lt;dbl&amp;gt; 4281356, 4281357, 4281358, 4281359, 4281360
## $ IdBundesland         &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1
## $ Bundesland           &amp;lt;chr&amp;gt; &amp;quot;Schleswig-Holstein&amp;quot;, &amp;quot;Schleswig-Holstein&amp;quot;,…
## $ Landkreis            &amp;lt;chr&amp;gt; &amp;quot;SK Flensburg&amp;quot;, &amp;quot;SK Flensburg&amp;quot;, &amp;quot;SK Flensbu…
## $ Altersgruppe         &amp;lt;chr&amp;gt; &amp;quot;A15-A34&amp;quot;, &amp;quot;A15-A34&amp;quot;, &amp;quot;A15-A34&amp;quot;, &amp;quot;A15-A34&amp;quot;,…
## $ Geschlecht           &amp;lt;chr&amp;gt; &amp;quot;M&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;M&amp;quot;
## $ AnzahlFall           &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1
## $ AnzahlTodesfall      &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0
## $ Meldedatum           &amp;lt;chr&amp;gt; &amp;quot;2020/03/14 00:00:00&amp;quot;, &amp;quot;2020/03/19 00:00:00…
## $ IdLandkreis          &amp;lt;chr&amp;gt; &amp;quot;01001&amp;quot;, &amp;quot;01001&amp;quot;, &amp;quot;01001&amp;quot;, &amp;quot;01001&amp;quot;, &amp;quot;01001&amp;quot;
## $ Datenstand           &amp;lt;chr&amp;gt; &amp;quot;30.04.2020, 00:00 Uhr&amp;quot;, &amp;quot;30.04.2020, 00:00…
## $ NeuerFall            &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0
## $ NeuerTodesfall       &amp;lt;dbl&amp;gt; -9, -9, -9, -9, -9
## $ Refdatum             &amp;lt;chr&amp;gt; &amp;quot;2020/03/16 00:00:00&amp;quot;, &amp;quot;2020/03/13 00:00:00…
## $ NeuGenesen           &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0
## $ AnzahlGenesen        &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1
## $ IstErkrankungsbeginn &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1
## $ Altersgruppe2        &amp;lt;chr&amp;gt; &amp;quot;nicht übermittelt&amp;quot;, &amp;quot;nicht übermittelt&amp;quot;, &amp;quot;…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following code block reshapes and translates the data to make it better accessible. This includes replacing our beloved German umlauts with simplified diphthongs, creating age groups, and aggregating COVID-19 numbers by county, age group, gender, and date:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covid_de &amp;lt;- covid_de %&amp;gt;% 
  select(state = Bundesland,
         county = Landkreis,
         age_group = Altersgruppe,
         gender = Geschlecht,
         cases = AnzahlFall,
         deaths = AnzahlTodesfall,
         recovered = AnzahlGenesen,
         date = Meldedatum) %&amp;gt;% 
  mutate(date = date(date)) %&amp;gt;% 
  mutate(age_group = str_remove_all(age_group, &amp;quot;A&amp;quot;)) %&amp;gt;% 
  mutate(age_group = case_when(
    age_group == &amp;quot;unbekannt&amp;quot; ~ NA_character_,
    age_group == &amp;quot;80+&amp;quot; ~ &amp;quot;80-99&amp;quot;,
    TRUE ~ age_group
  )) %&amp;gt;% 
  mutate(gender = case_when(
    gender == &amp;quot;W&amp;quot; ~ &amp;quot;F&amp;quot;,
    gender == &amp;quot;unbekannt&amp;quot; ~ NA_character_,
    TRUE ~ gender
  )) %&amp;gt;% 
  group_by(state, county, age_group, gender, date) %&amp;gt;% 
  summarise(cases = sum(cases),
            deaths = sum(deaths),
            recovered = sum(recovered)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  filter(cases &amp;gt;= 0 &amp;amp; deaths &amp;gt;= 0) %&amp;gt;%
  filter(date &amp;lt; today()) %&amp;gt;% 
  mutate(state = str_replace_all(state, &amp;quot;ü&amp;quot;, &amp;quot;ue&amp;quot;)) %&amp;gt;% 
  mutate(state = str_replace_all(state, &amp;quot;ä&amp;quot;, &amp;quot;ae&amp;quot;)) %&amp;gt;% 
  mutate(state = str_replace_all(state, &amp;quot;ö&amp;quot;, &amp;quot;oe&amp;quot;)) %&amp;gt;% 
  mutate(state = str_replace_all(state, &amp;quot;ß&amp;quot;, &amp;quot;ss&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ü&amp;quot;, &amp;quot;ue&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ä&amp;quot;, &amp;quot;ae&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ö&amp;quot;, &amp;quot;oe&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ß&amp;quot;, &amp;quot;ss&amp;quot;)) %&amp;gt;% 
  mutate(county = str_remove(county, &amp;quot;\\(.+\\)&amp;quot;)) %&amp;gt;% 
  mutate(county = str_trim(county)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is a dataset that lists daily (&lt;em&gt;not cumulative!&lt;/em&gt;) cases, deaths, and recovered cases for 6 age groups, gender, and the German counties and their corresponding federal states. Similar to the US, Germany has a federal system in which the 16 federal states have a large amout of legislative power. The German equivalent of the US county is the “Kreis”, which can either be associated with a city (“Stadtkreis” = “SK”) or the country side (“Landkreis” = “LK”). Here only a subset of columns are shown for reasons of clarity:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covid_de %&amp;gt;%
  filter(state == &amp;quot;Sachsen&amp;quot;) %&amp;gt;% 
  select(-deaths, -recovered) %&amp;gt;% 
  head(5) %&amp;gt;% 
  kable() %&amp;gt;% 
  column_spec(1:6, width = c(&amp;quot;15%&amp;quot;, &amp;quot;25%&amp;quot;, &amp;quot;15%&amp;quot;, &amp;quot;10%&amp;quot;, &amp;quot;25%&amp;quot;, &amp;quot;10%&amp;quot;)) %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
state
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
county
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
age_group
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
gender
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
date
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
cases
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
Sachsen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
LK Bautzen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
00-04
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
F
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
2020-03-20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 15%; &#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
Sachsen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
LK Bautzen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
00-04
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
F
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
2020-04-07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 15%; &#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
Sachsen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
LK Bautzen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
00-04
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
M
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
2020-03-21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 15%; &#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
Sachsen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
LK Bautzen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
05-14
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
F
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
2020-03-20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 15%; &#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
Sachsen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
LK Bautzen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
05-14
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
F
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
2020-03-21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 15%; &#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is the cleaned dataset which is available on &lt;a href=&#34;https://www.kaggle.com/headsortails/covid19-tracking-germany&#34;&gt;Kaggle&lt;/a&gt; as &lt;code&gt;covid_de.csv&lt;/code&gt;. With this data, you can already already slice and analyse Germany’s COVID-19 characteristics by various demographic and geographical features.&lt;/p&gt;
&lt;p&gt;However, for the maps that we’re interested in one more input is missing: shapefiles. A &lt;a href=&#34;https://en.wikipedia.org/wiki/Shapefile&#34;&gt;shapefile&lt;/a&gt; uses a standard vector format for specifying spatial geometries. It packages the map boundary data of the required entities (like countries, federal states) into a small set of related files. For this project, I found publicly available shapefiles on the state and county level provided by Germany’s &lt;a href=&#34;https://www.bkg.bund.de/EN/Home/home.html&#34;&gt;Federal Agency for Cartography and Geodesy&lt;/a&gt;. Both levels are available in the Kaggle dataset. Here I put the county level files (&lt;code&gt;de_county.*&lt;/code&gt;) into a local, static directory.&lt;/p&gt;
&lt;p&gt;Shapefiles can be read into R using the &lt;code&gt;sf&lt;/code&gt; package tool &lt;code&gt;st_read&lt;/code&gt;. In order to soon join them to our COVID-19 data, we need to do a bit of string translating and wrangling again. The &lt;code&gt;tidyr&lt;/code&gt; tool &lt;code&gt;unite&lt;/code&gt; is being used to combine the county type (&lt;code&gt;BEZ in c(&amp;quot;LK&amp;quot;, &amp;quot;SK&amp;quot;)&lt;/code&gt;) and county name into the format we have in our COVID-19 data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shape_county &amp;lt;- st_read(str_c(&amp;quot;../../static/files/&amp;quot;, &amp;quot;de_county.shp&amp;quot;), quiet = TRUE) %&amp;gt;% 
  rename(county = GEN) %&amp;gt;% 
  select(county, BEZ, geometry) %&amp;gt;% 
  mutate(county = as.character(county)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ü&amp;quot;, &amp;quot;ue&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ä&amp;quot;, &amp;quot;ae&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ö&amp;quot;, &amp;quot;oe&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ß&amp;quot;, &amp;quot;ss&amp;quot;)) %&amp;gt;% 
  mutate(county = str_remove(county, &amp;quot;\\(.+\\)&amp;quot;)) %&amp;gt;% 
  mutate(county = str_trim(county)) %&amp;gt;% 
  mutate(BEZ = case_when(
    BEZ == &amp;quot;Kreis&amp;quot; ~ &amp;quot;LK&amp;quot;,
    BEZ == &amp;quot;Landkreis&amp;quot; ~ &amp;quot;LK&amp;quot;,
    BEZ == &amp;quot;Stadtkreis&amp;quot; ~ &amp;quot;SK&amp;quot;,
    BEZ == &amp;quot;Kreisfreie Stadt&amp;quot; ~ &amp;quot;SK&amp;quot;
  )) %&amp;gt;% 
  unite(county, BEZ, county, sep = &amp;quot; &amp;quot;, remove = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this stage, there are still some county names that don’t match precisely. It would have been too easy, otherwise. These cases mostly come down to different styles of abbreviations being used for counties with longer names. A scalable way to deal with these wonders of the German language would be &lt;a href=&#34;https://cran.r-project.org/web/packages/fuzzyjoin/&#34;&gt;fuzzy matching&lt;/a&gt; by &lt;a href=&#34;https://cran.r-project.org/web/packages/stringdist/&#34;&gt;string distance&lt;/a&gt; similarities. Here, the number of mismatches is small and I decided to adjust them manually.&lt;/p&gt;
&lt;p&gt;Then, I group everything by &lt;code&gt;county&lt;/code&gt; and &lt;code&gt;date&lt;/code&gt; and sum over the remaining features. One major issue here is that not all counties will report numbers for all days. Those are small areas, after all. In this dataset, these cases are implicitely missing; i.e. the corresponding rows are just not present. It is important to convert those cases into explicitely missing entries: rows that have a count of zero. Otherwise, our eventual map will have “holes” in it for specific days and specific counties. The elegant solution in the code is made possible by the &lt;code&gt;tidyr&lt;/code&gt; function &lt;code&gt;complete&lt;/code&gt;: simply name all the columns for which we want to have all the combinations and specify how they should be filled. This approach applies to any situation where we have a set of features and need a complete grid of all possible combinations.&lt;/p&gt;
&lt;p&gt;Finally, we sum up the cumulative cases and deaths. Here, I also applied a &lt;code&gt;filter&lt;/code&gt; to extract data from March 1st - 31st only, to prevent the animation file from becoming too large. Feel free to expand this to a longer time frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;foo &amp;lt;- covid_de %&amp;gt;% 
  mutate(county = case_when(
    county == &amp;quot;Region Hannover&amp;quot; ~ &amp;quot;LK Region Hannover&amp;quot;,
    county == &amp;quot;SK Muelheim a.d.Ruhr&amp;quot; ~ &amp;quot;SK Muelheim an der Ruhr&amp;quot;,
    county == &amp;quot;StadtRegion Aachen&amp;quot; ~ &amp;quot;LK Staedteregion Aachen&amp;quot;,
    county == &amp;quot;SK Offenbach&amp;quot; ~ &amp;quot;SK Offenbach am Main&amp;quot;,
    county == &amp;quot;LK Bitburg-Pruem&amp;quot; ~ &amp;quot;LK Eifelkreis Bitburg-Pruem&amp;quot;,
    county == &amp;quot;SK Landau i.d.Pfalz&amp;quot; ~ &amp;quot;SK Landau in der Pfalz&amp;quot;,
    county == &amp;quot;SK Ludwigshafen&amp;quot; ~ &amp;quot;SK Ludwigshafen am Rhein&amp;quot;,
    county == &amp;quot;SK Neustadt a.d.Weinstrasse&amp;quot; ~ &amp;quot;SK Neustadt an der Weinstrasse&amp;quot;,
    county == &amp;quot;SK Freiburg i.Breisgau&amp;quot; ~ &amp;quot;SK Freiburg im Breisgau&amp;quot;,
    county == &amp;quot;LK Landsberg a.Lech&amp;quot; ~ &amp;quot;LK Landsberg am Lech&amp;quot;,
    county == &amp;quot;LK Muehldorf a.Inn&amp;quot; ~ &amp;quot;LK Muehldorf a. Inn&amp;quot;,
    county == &amp;quot;LK Pfaffenhofen a.d.Ilm&amp;quot; ~ &amp;quot;LK Pfaffenhofen a.d. Ilm&amp;quot;,
    county == &amp;quot;SK Weiden i.d.OPf.&amp;quot; ~ &amp;quot;SK Weiden i.d. OPf.&amp;quot;,
    county == &amp;quot;LK Neumarkt i.d.OPf.&amp;quot; ~ &amp;quot;LK Neumarkt i.d. OPf.&amp;quot;,
    county == &amp;quot;LK Neustadt a.d.Waldnaab&amp;quot; ~ &amp;quot;LK Neustadt a.d. Waldnaab&amp;quot;,
    county == &amp;quot;LK Wunsiedel i.Fichtelgebirge&amp;quot; ~ &amp;quot;LK Wunsiedel i. Fichtelgebirge&amp;quot;,
    county == &amp;quot;LK Neustadt a.d.Aisch-Bad Windsheim&amp;quot; ~ &amp;quot;LK Neustadt a.d. Aisch-Bad Windsheim&amp;quot;,
    county == &amp;quot;LK Dillingen a.d.Donau&amp;quot; ~ &amp;quot;LK Dillingen a.d. Donau&amp;quot;,
    county == &amp;quot;LK Stadtverband Saarbruecken&amp;quot; ~ &amp;quot;LK Regionalverband Saarbruecken&amp;quot;,
    county == &amp;quot;LK Saar-Pfalz-Kreis&amp;quot; ~ &amp;quot;LK Saarpfalz-Kreis&amp;quot;,
    county == &amp;quot;LK Sankt Wendel&amp;quot; ~ &amp;quot;LK St. Wendel&amp;quot;,
    county == &amp;quot;SK Brandenburg a.d.Havel&amp;quot; ~ &amp;quot;SK Brandenburg an der Havel&amp;quot;,
    str_detect(county, &amp;quot;Berlin&amp;quot;) ~ &amp;quot;SK Berlin&amp;quot;,
    TRUE ~ county
  )) %&amp;gt;% 
  group_by(county, date) %&amp;gt;% 
  summarise(cases = sum(cases),
            deaths = sum(deaths)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  complete(county, date, fill = list(cases = 0, deaths = 0)) %&amp;gt;% 
  group_by(county) %&amp;gt;% 
  mutate(cumul_cases = cumsum(cases),
         cumul_deaths = cumsum(deaths)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  filter(between(date, date(&amp;quot;2020-03-01&amp;quot;), date(&amp;quot;2020-03-31&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have all the ingredients for animating a county-level map of cumulative cases. Here we first define the animation object by specifying &lt;code&gt;geom_sf()&lt;/code&gt; and &lt;code&gt;theme_map()&lt;/code&gt; for the map style, then providing the animation steps column &lt;code&gt;date&lt;/code&gt; to the &lt;code&gt;transition_time()&lt;/code&gt; method. The advantage of &lt;a href=&#34;https://rdrr.io/github/thomasp85/gganimate/man/transition_time.html&#34;&gt;transition_time&lt;/a&gt; is that the length of transitions between steps takes is proportional to the intrinsic time differences. Here, we have a very well behaved dataset and all our steps are of length 1 day. Thus, we could also use &lt;code&gt;transition_states()&lt;/code&gt; directly. However, I consider it good practice to use &lt;code&gt;transition_time&lt;/code&gt; whenever actual time steps are involved; to be prepared for unequal time intervals.&lt;/p&gt;
&lt;p&gt;The animation parameters are provided in the &lt;code&gt;animate&lt;/code&gt; function, such as the transition style from one day to the next (&lt;code&gt;cubic-in-out&lt;/code&gt;), the animation speed (10 frames per s), or the size of the plot. For cumulative animations like this, it’s always a good idea to include an &lt;code&gt;end_pause&lt;/code&gt; freeze-frame, so that the reader can have a closer look at the final state before the loop begins anew:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- shape_county %&amp;gt;% 
  right_join(foo, by = &amp;quot;county&amp;quot;) %&amp;gt;% 
  ggplot(aes(fill = cumul_cases)) +
  geom_sf() +
  scale_fill_viridis(trans = &amp;quot;log1p&amp;quot;, breaks = c(0, 10, 100, 1000)) +
  theme_map() +
  theme(title = element_text(size = 15), legend.text = element_text(size = 12),
        legend.title = element_text(size = 15)) +
  labs(title = &amp;quot;Total COVID-19 cases in Germany: {frame_time}&amp;quot;, fill = &amp;quot;Cases&amp;quot;) +
  transition_time(date)

animate(gg + ease_aes(&amp;#39;cubic-in-out&amp;#39;), fps = 10, end_pause = 25, height = 800, width = round(800/1.61803398875))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2020-04-30-animate-map-covid_files/figure-html/unnamed-chunk-8-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Our final map shows how the number of COVID-19 cases in Germany first started to rise in the South and West, and how they spread to other parts of the country. The geographical middle of Germany appears to be lagging behind in case counts even at later times. Note the logarithmic colour scale.&lt;/p&gt;
&lt;p&gt;More info:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;One caveat: This view does not take into account population density, which makes large cities like Berlin (north-east) stand out more towards the end. My Kaggle dataset currently includes population counts for the state-level only, but I’m planning to add county data in the near future.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you’re looking for further inspiration on how to analyse this dataset then check out the various &lt;a href=&#34;https://www.kaggle.com/headsortails/covid19-tracking-germany/kernels&#34;&gt;Notebooks&lt;/a&gt; (aka “Kernels”) which are associated with it on Kaggle. Kaggle has the big advantage that you can run R or Python scripts and notebooks in a pretty powerful cloud environment; and present your work alongside datasets and competitions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Another Kaggle dataset of mine with daily COVID-19 cases, deaths, and recoveries in the US can be found &lt;a href=&#34;https://www.kaggle.com/headsortails/covid19-us-county-jhu-data-demographics&#34;&gt;here&lt;/a&gt;. This data also has a county-level resolution. It is based on &lt;a href=&#34;https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series&#34;&gt;Johns Hopkins University data&lt;/a&gt; and I’m updating it on a daily basis.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>What makes a community great?</title>
      <link>https://heads0rtai1s.github.io/2019/04/19/great-community-kaggledays19/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2019/04/19/great-community-kaggledays19/</guid>
      <description>


&lt;p&gt;The easy answer to this question is: the people. Great people build great communities. Case not quite closed yet, though, because there is more to it. Even the most promising group of individuals needs certain conditions in order to grow into a strong and thriving community. The kind of community that lifts up its members beyond their individual capabilities and becomes more than the sum of their proverbial skills and contributions. I believe that such communities are the cornerstones of all scientific fields, including data science, and that those fields succeed or fail depending on their communities.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://heads0rtai1s.github.io/pics/kdays19_1.jpg&#34; alt=&#34;Kaggle Days audience for welcome talks&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;em&gt;Kaggle Days audience for welcome talks&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Here is the case study that prompted this post: last week I took part in the &lt;a href=&#34;https://kaggledays.com/sanfrancisco/&#34;&gt;Kaggle Days meeting in San Francisco&lt;/a&gt;. Kaggle Days are a new series of local meetings which aim to bring together members of the international, virtual &lt;a href=&#34;https://www.kaggle.com/&#34;&gt;Kaggle&lt;/a&gt; community for in-person workshops, presentations, and competitions. Over the last few years, Kaggle itself has transformed from being “merely” the go-to place for sophisticated machine learning competitions to a multi-faceted online community. The range and depth of user-hosted datasets continues to grow rapidly from month to month, as does a unique repository of machine learning and data science code templates in the form of Kaggle &lt;a href=&#34;https://www.kaggle.com/kernels&#34;&gt;Kernels&lt;/a&gt;: reproducible R/Python notebooks or scripts in a self-contained, cloud-based environment. There really is something for everyone.&lt;/p&gt;
&lt;p&gt;It is fair to say that Kaggle has been a main catalyst for my career change. Joining the platform in early 2017, two years ago, opened my eyes to the multitude of fascinating challenges and problem-solving strategies beyond my narrow academic field. One year later, I had become the first ever &lt;a href=&#34;http://blog.kaggle.com/2018/06/19/tales-from-my-first-year-inside-the-head-of-a-recent-kaggle-addict/&#34;&gt;Kaggle Kernels Grandmaster&lt;/a&gt; - a journey that I plan to revisit in a future post. What drew me into Kaggle, beyond the fun competitions, was a remarkably friendly and supportive community. It’s a rare occurrence to find people who are both extremely smart and happy to help a newcomer in an approachable and relaxed way. Machine Learning can be intimidating, but the people on Kaggle made it fun. Sooner than expected, I started to feel that I had become part of a community which, despite being very competitive, was remarkably efficient at working together to solve hard problems. Especially considering that we all worked on these challenges in our free-time and in different corners of the world.&lt;/p&gt;
&lt;p&gt;All this back story might help to illustrate why I felt rather excited to finally encounter many of my fellow Kagglers in person at the Kaggle Days meetup. Excited and a bit nervous as to how the virtual collaboration would translate to the real world. I had high expectations - which were exceeded spectacularly. Kaggle Days was a blast! Almost the entire &lt;a href=&#34;https://www.kaggle.com/about/team&#34;&gt;Kaggle Team&lt;/a&gt; was present, including CEO &lt;a href=&#34;https://twitter.com/antgoldbloom&#34;&gt;Anthony Goldblum&lt;/a&gt; and Co-Founder &lt;a href=&#34;https://twitter.com/benhamner&#34;&gt;Ben Hamner&lt;/a&gt;. Top Kagglers such as &lt;a href=&#34;https://twitter.com/tunguz&#34;&gt;Bojan Tunguz&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/DmitryLarko&#34;&gt;Dmitry Larko&lt;/a&gt; gave presentations and workshops alongside Machine Learning gurus like &lt;a href=&#34;https://twitter.com/fchollet&#34;&gt;Francois Chollet&lt;/a&gt; (Keras) or &lt;a href=&#34;https://twitter.com/quocleix&#34;&gt;Quoc Le&lt;/a&gt; (Google Brain / AutoML). So many super smart people to listen and talk to! It was great fun.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://heads0rtai1s.github.io/pics/kdays19_2.jpg&#34; alt=&#34;Kaggle Days brainstorming sessions&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;em&gt;Kaggle Days brainstorming sessions&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, as a side effect of my initial Kaggle anonymity (I did not use my real name at all during my first year) I quickly found it more useful to introduce myself as “Heads or Tails”, even though my badge had my actual name on it. I only forgot this when I first met almost the entire Kaggle team at once and needed a second take for a more useful introduction. As this blog shows, I still prefer the “Heads or Tails” moniker for my Data Science personality. Let’s hope that no psychologists read this.&lt;/p&gt;
&lt;p&gt;Ever the hands-on community, the second (and final) day of the Kaggle Days meetup gave us the opportunity to form small teams to participate in an on-site competition. This was particularly interesting for me, since I had never teamed up with others to tackle a Kaggle problem. In a team, there is much more code sharing and discussion than in the open Kaggle forum. And even though there were a few small hiccups in this particular competition (Want a change in metric plus additional data halfway through? Say no more!) working with my team mates was a lot of fun. Shout-out to &lt;a href=&#34;https://www.kaggle.com/thawatt&#34;&gt;Michael&lt;/a&gt; and &lt;a href=&#34;https://www.kaggle.com/gtoubassi&#34;&gt;Garrick&lt;/a&gt; - you guys rock! As a result of Kaggle Days I’m definitely more motivated to team up with others in future competitions. Not just that: I’m more motivated in general to spend time on Kaggle.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://heads0rtai1s.github.io/pics/kdays19_3.jpg&#34; alt=&#34;Kaggle Days competition winners&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;em&gt;Kaggle Days competition winners&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now, why is that exactly? What makes the Kaggle community such a fun place? For me, there are several different factors that all enhance each other when combined:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Kagglers are smart yet down to earth.&lt;/em&gt; &lt;strong&gt;Not only are they happy to share their insights, they really make an effort to do so in an accessible way.&lt;/strong&gt; I recommend anyone who joins a competition not to immediately abandon it after the results are in, but to read the write ups of the top teams which are usually of high detail and quality. There is lots to learn from such a post mortem.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;No big egos in the community.&lt;/em&gt; This is related to the previous point but touches on a different aspect. Even though our community has its own big names who’s opinions (deservedly) have weight in discussions, nobody thinks they are more important than others. This is crucial because it lowers the threshold for beginners (and anyone) to ask questions. Asking questions is what drives the improvement of individuals and the community. As a side note: In my time in academia I have come across some really big egos, although luckily never in immediate collaboration. Although these people are very smart you really don’t want to be around them for longer than absolutely necessary. No gossip here - moving on:&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;A common goal.&lt;/em&gt; As soon as you meet other Kagglers you have something to talk about; be it an ongoing competition, the new Kernels interface, or the most recent Machine Learning tools. But it goes beyond that: competitions are the best example for creating a specific goal that everyone can focus on and contribute to, to the best of their abilities. And while sharing is encouraged, there is plenty of competition at the highest level. The last days of a competition are one of the most intense examples of a singular focus in an online community of hundreds to ten thousands of people from all over the world.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Diversity.&lt;/em&gt; Speaking of ‘all over the world’. Kagglers come from a large number of countries and have many different backgrounds. It is true that we are still predominantly male and STEM based, and we are working on becoming more inclusive towards many other groups. You can think about it this way: When doing Machine Learning, diversity is a big advantage. If you average over several models then your results will be better the less similarity these models have (i.e. the less collinearity there is). An insight that is missed by one model might be picked up by a different method. The likelihood that all models will overfit in the same direction is smaller. And the same is true for communities. Different points of view help us to challenge pre-conceived beliefs and broaden our horizons. For deeper insights into the diversity of Kagglers you can check out my analysis, and those of many others, of the latest &lt;a href=&#34;https://www.kaggle.com/headsortails/what-we-do-in-the-kernels-a-kaggle-survey-story&#34;&gt;Kaggle Survey&lt;/a&gt; which, true to form, is a detailed annual assessment of the state of the community.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;People care about the community.&lt;/em&gt; This is one of the most important factors. It might somewhat derive from the points above but it’s by no means a given. I have been part of (and witness to) passionate discussions in the Kaggle forums about difficult issues in the community. Often Kagglers themselves have suggested solutions to problems that the administrative team might not have been aware of. And even if tempers flare up, which is more understandable in a competitive context, there is mutual respect and usually a (virtual) handshake once the dust has settled. Kaggle is &lt;em&gt;our&lt;/em&gt; community and we care about keeping it friendly and welcoming.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Infrastructure for collaboration and communication.&lt;/em&gt; Last but not least, for a community to function well there need to be tools and environments in place that allow for efficient communication. The lower the thresholds are for exchanging information the better it will work. Ideally, the infrastructure should be designed in a way that encourages different ways of interaction for the community members. This further promotes a welcoming and inclusive atmosphere. Kaggle provides all this through discussion forums (general ones and those specific to each competition or data set). In addition, the aforementioned Kernel notebooks have comments enabled, which is a great way to show appreciation to an author’s work or ask for clarification. From my point of view, commenting on Kernels is great, low-threshold way of starting to actively participate in the community. And I can guarantee that Kernel authors appreciated feedback. My own Kernels have frequently been improved by people kind enough to post their ideas and suggestions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A special kind of infrastructure is an in-person meetup like Kaggle Days. Remote interaction works fine, but from my experience there is a certain extra factor in face-to-face meetings. During my time in academia I had the privilege to be part of many successful teams and to work with many smart people. The highlights of these collaborations were always our team meetings in which we could brainstorm new ideas and strategies. Sometimes at a hotel pool, sometimes late at night over drinks or pizza; but always with extra energy and creativity. And when your creativity goes into overdrive then you have found a great community.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Finally, this post doesn’t feel complete without highlighting the remarkable way in which the Data Science community (and especially the R community) is responding to the disgraceful case of sexual harassment and the botched attempt at a cover up at &lt;a href=&#34;https://dhavide.github.io/a-note-to-our-commuity-on-building-trust.html&#34;&gt;DataCamp&lt;/a&gt;. The community is supporting the victim and former employees who spoke out and were fired. Many content creators are pulling their courses from DataCamp to push for necessary change. Here is a community actively working to transform bad practices that those who are primarily responsible are repeatedly failing to address. Because I’m an optimist at heart, I want to close by pointing to one of the most remarkable products of this sorry situation: a free natural language programming (NLP) course plus a great interactive app template build by &lt;a href=&#34;https://twitter.com/_inesmontani&#34;&gt;Ines Montani&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Like many of you, I&amp;#39;m incredibly disappointed by DataCamp. I wanted to make a free version of my spaCy course so you don&amp;#39;t have to sign up for their service – and ended up building my own interactive app. Powered by the awesome &lt;a href=&#34;https://twitter.com/mybinderteam?ref_src=twsrc%5Etfw&#34;&gt;@mybinderteam&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/GatsbyJS?ref_src=twsrc%5Etfw&#34;&gt;@gatsbyjs&lt;/a&gt; 💖 &lt;a href=&#34;https://t.co/2QOuDPoZEX&#34;&gt;https://t.co/2QOuDPoZEX&lt;/a&gt;&lt;/p&gt;&amp;mdash; Ines Montani 〰️ (@_inesmontani) &lt;a href=&#34;https://twitter.com/_inesmontani/status/1118508634357604353?ref_src=twsrc%5Etfw&#34;&gt;April 17, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</description>
    </item>
    
  </channel>
</rss>
