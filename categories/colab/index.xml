<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Colab on head spin - the Heads or Tails blog</title>
    <link>https://heads0rtai1s.github.io/categories/colab/</link>
    <description>Recent content in Colab on head spin - the Heads or Tails blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Jan 2022 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://heads0rtai1s.github.io/categories/colab/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Yearly goals: 2021 experiences</title>
      <link>https://heads0rtai1s.github.io/2022/01/13/goals-2021-results/</link>
      <pubDate>Thu, 13 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2022/01/13/goals-2021-results/</guid>
      <description>


&lt;p&gt;At the beginning of last year, I got the motivation from other people’s Twitter posts to set and track my own goals for what would surely be the year we would emerge from the shadows of the pandemic. With an ever so slightly more jaded outlook to life, I’m doing the same this year. On a global level, many societies didn’t seem to have learned an aweful lot from the Covid years of 2020 and 2021. From a personal perspective, I hope that the experience of setting and sharing my goals has helped me to define better goals for 2022. This post is a brief retrospective on the 2021 part of my journey, and the lessons I hope to have learned for the road ahead.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://thumbs.gfycat.com/NauticalPastLadybird-size_restricted.gif&#34; title=&#34;Carry on ...&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;how-did-your-year-go&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How did your year go?&lt;/h3&gt;
&lt;p&gt;Those were my 2021 stretch goals and the tally I drew at the end of the year:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Final tally, not too bad:&lt;br&gt;&lt;br&gt;✅ Spend 500 h on &lt;a href=&#34;https://twitter.com/kaggle?ref_src=twsrc%5Etfw&#34;&gt;@Kaggle&lt;/a&gt;&lt;br&gt;&lt;br&gt;✅ Join 4 competitions and team up in 2 of them&lt;br&gt;&lt;br&gt;✅ Publish 52 episodes of &lt;a href=&#34;https://twitter.com/hashtag/KaggleHiddenGems?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#KaggleHiddenGems&lt;/a&gt;&lt;br&gt;&lt;br&gt;❌ Write 12 blog posts&lt;br&gt;&lt;br&gt;✅ Run my 1st marathon&lt;br&gt;&lt;br&gt;❌ Do 1 muscle up&lt;br&gt;&lt;br&gt;✅ Sleep 8 h/night&lt;/p&gt;&amp;mdash; Martin Henze (Heads or Tails) (@heads0rtai1s) &lt;a href=&#34;https://twitter.com/heads0rtai1s/status/1476293217779920901?ref_src=twsrc%5Etfw&#34;&gt;December 29, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Given that 2021 was year two of a global plague, it might be fair to add ‘having survived all that’ to the list of accomplishments. But I’m aware (and grateful) of my priviledged situation of being able to work from home on my computer, so let’s not cheapen the accomplishments of those who actually were out there working hard and who managed to stave off (or overcome) serious illness. A bit of perspective is necessary, from time to time.&lt;/p&gt;
&lt;p&gt;At the same time, I think it is important to highlight the degree of structure and control that can be provided by setting goals and trying to reach them; however much arbitrary or insignificant those goals might appear in the bigger picture. I believe that especially in uncertain times there is a lot of comfort in wrestling a modicum of control from the chaotic universe. As long as we do the best we can do to our abilities (and circumstances) then we’re doing rather well. But enough existentialism. Let’s get back to data science.&lt;/p&gt;
&lt;p&gt;On the goals themselves, I’d like to go into more detail than the Twitter thread allowed. I’d like to emphasise that those are stretch goals, and that I didn’t expect to reach all of them. Wouldn’t be fun it were too easy, after all.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spend-500-hours-on-kaggle&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Spend 500 hours on Kaggle&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; Now, hours themselves don’t create mastery; it’s about what you do during those hours that counts. A short time of focussed effort counts for more than a much longer period of distracted dabbling. Having said that: setting a specific goal that translates to 1-2 hours per day can help immensely in creating habits and budgeting time.&lt;/p&gt;
&lt;p&gt;For the latter, I would often spend about 1 hour per day on Kaggle problems during the week, and 2 or 3 hours on the weekend. Calling it a (Kaggle) day after that helped to avoid the temptation of working later at night, especially during the week. Kaggle challenges are more akin to a marathon (&lt;em&gt;foreshadowing …&lt;/em&gt;) than a sprint, which means that pacing yourself becomes important. In general, having a healthy work-life balance is necessary to avoid burn out and loss of motivation (or health).&lt;/p&gt;
&lt;p&gt;The habit-building effect of having a specific goal can hardly be overstated. Continuity is one of the main factors of successful learning and growing. If you’re spending (almost) every day on a certain project, or practicing a specific tool, then you are bound to make at least some progress in understanding. And learning is what Kaggle is mostly about, for me. (And the &lt;a href=&#34;https://heads0rtai1s.github.io/2019/04/19/great-community-kaggledays19/&#34;&gt;community&lt;/a&gt;, of course.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;join-4-competition-and-team-up-in-2-of-them&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Join 4 competition and team up in 2 of them&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; This continues the previous train of thought: competiting is less about the ranks and medals, but more a motivation for learning new skills and tools. I always found it most efficient to learn by doing - and to learn from examples and applications. Instead of merely reading books of theory, or even code, I prefered to focus on the skills that I needed to solve a specific problem. And what better challenge to spend 3 months at a time competing with the brightest applied-ML minds on Kaggle on a new and exiting problem.&lt;/p&gt;
&lt;p&gt;In 2021, I participated earnestly in 5 competitions. I also briefly dabbled in a few others; but honestly less is more in this kind of situation. You want to focus on one thing at a time, instead of scattering your attention across lots of projects and not really devoting enough time to any of them. I know, the temptation is great to jump into every new competition that launches. I’m trying myself to recover from that; so I recommend you do as I say, not do as I do. Having said that, 2021 for me was a notable improvement over previous year.&lt;/p&gt;
&lt;p&gt;A lot of the learning effect on Kaggle comes from reading other people’s contributions in form of Notebooks or Discussion posts. This kind of effect is applified by a factor of a lot when you team up with other competitors. I had only done this very occasionally in the past. The main reason was probably that I didn’t have enough confidence in my own skills to contribute meaningfully to a team. I don’t want to be carried. I want to make a significant difference. Having learnt a bit more in 2020 I decided to get outside of my comfort zone and challenge myself in a team.&lt;/p&gt;
&lt;p&gt;It turned out to be a very fun and rewarding experience. I joined forces with my Kaggle buddy &lt;a href=&#34;https://twitter.com/YassineAlouini&#34;&gt;Yassine&lt;/a&gt; on 2 imaging competitions and learned a lot from him on deep learning architectures and training strategies. On the way, I became much more confident in using the high-level &lt;a href=&#34;https://www.fast.ai/&#34;&gt;FastAI&lt;/a&gt; framework and also &lt;a href=&#34;https://colab.research.google.com&#34;&gt;Google Colab&lt;/a&gt;, which is a great cloud environment for GPU notebooks. Read my mid-2020 review on cloud GPUs &lt;a href=&#34;https://heads0rtai1s.github.io/2021/08/24/colab-plus-kaggle-cloud-gpu/&#34;&gt;right here&lt;/a&gt;. Bottom line: there are lots of great things to be found in the clouds these days.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media0.giphy.com/media/3oEdv6UTqzNk9Y5i36/giphy.gif?cid=790b76119299a8b65e0c47548fe0e8dbd2d4ca4fefe9698e&amp;amp;rid=giphy.gif&amp;amp;ct=g%20...&#34; title=&#34;Burgers not included in the Colab Pro Plus subscription ... yet&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;publish-52-episodes-of-kaggle-hidden-gems&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Publish 52 episodes of Kaggle Hidden Gems&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; The Hidden Gems series is my attempt to give back something to the Kaggle community by discovering and promoting underrated Notebooks published on the platform. The Kaggle community has grown enormously over the last years. So many bright people from many walks of life contribute to the community; which is great because you have the chance to learn from them. One difficulty, though, of this higher volume of contributions is that sometimes great content can have difficulties standing out and being recognised for its quality.&lt;/p&gt;
&lt;p&gt;Every week I pick 3 new, underrated Notebooks to highlight. Here’s the most recent example at the time of writing: &lt;a href=&#34;https://www.kaggle.com/general/300228&#34;&gt;episode number 88&lt;/a&gt;. As a result my week doesn’t feel complete now without reading cool
Kaggle Notebooks. In 2021 I succesfuly managed to establish a continuity of one episode every Tuesday, without fail. 52 episodes in 52 weeks. On the occasion of episode 50, I gathered all the Gems data into a &lt;a href=&#34;https://www.kaggle.com/headsortails/notebooks-of-the-week-hidden-gems&#34;&gt;Kaggle dataset&lt;/a&gt; and put together a &lt;a href=&#34;https://www.kaggle.com/headsortails/hidden-gems-a-collection-of-underrated-notebooks/&#34;&gt;Starter Notebook&lt;/a&gt;. This is Kaggle, after all, where even the data has data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-12-blog-posts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Write 12 blog posts&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Fail.&lt;/strong&gt; I only wrote 2 posts in total in 2021 (after writing 9 in 2019, and 5 in 2020). Establishing and keeping up a blogging habit is a question of time, certainly. But in my case, it might also be a challenge of perfectionism and preference for long posts. Like this post here, which is already shaping up to be longer than I had intended.&lt;/p&gt;
&lt;p&gt;Going out of my comfort zone when it comes to writing isn’t easy at all. I don’t expect a large audience for this blog. I’m writing mostly for myself, so that I can remember and digest some of the things that I picked up over time. At the same time, I want for anyone who stumbles across those posts to get some quality information out of reading them. And that holds me back from publishing more frequent and shorter posts. That is one lesson learnt from 2021.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;run-my-1st-marathon&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Run my 1st marathon&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; I gotta be honest: this is probably my proudest achievement. My first ever marathon. 42 km in one go. After failing a few attempts during the year, I finally got there on Dec 6th. Just in time to make it a 2021 achievement; but in time nonetheless. Running in colder weather was also more conducive to success, as opposed to my summer attempt in August. Who would have guessed.&lt;/p&gt;
&lt;p&gt;For me, exercise has always been a way to destress and detach from coding and data analysis work, which isn’t the most physically active of all occupations. As such, I didn’t come cold into this marathon challenge but have a certain level of fitness that I’ve built and maintained over the years. When the pandemic started, running outdoors was one of the few exercises that I felt comfortable doing. Having never ran more than 10 km at a time before (and that was quite a few years ago), I then gradually worked up to this marathon challenge. From 5k to 10k, and then running my first half-marathon (21 km) in 2020.&lt;/p&gt;
&lt;p&gt;I didn’t read a lot of running blogs or stuff, so some of the things I discovered for myself might have been easier to read up on, in hindsight. None of this is expert recommendation, obviously. One insight is that a full marathon is a very different beast from a half-marathon. The 21 km of the half-marathon are something that I could run without stopping or nutrition; even without water (although that might not be generally recommended). None of that worked for me for the marathon.&lt;/p&gt;
&lt;p&gt;Turns out that after 2-ish hours of continuous exercise you have burned through most of your carb reserves (i.e. glycogen). I genuinely didn’t know that. Coincidentally, my longest non-stop exercises thus far were just under the 2h limit (and years ago). So, in my first past 30-km attempt I had to stop with leg cramps and fatigue, and it took me a couple hours and a good meal to recover from that. I later found out about that glycogen limit and that I had literally ran out of energy to burn. Fun times. So for the next attempts I got some energy gels plus water with electrolytes and it made a huge difference. Almost no issues after the successful marathon. Just the fun of walking up and down stairs for the days after.&lt;/p&gt;
&lt;p&gt;In numbers: I ran a total of 1332 km in 2021, which translates to about 1/30th around the earth, in 116 hours. I also logged about 3.7 million steps, for an average of about 10k steps per day. As a data addict, I’m using a Garmin watch and app for all those measurements. No link, since they haven’t given me a sponsorship yet. Gotta keep running.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media4.giphy.com/media/9rRacglGbs68E/giphy.gif?cid=ecf05e472rkl4hg798m9nz6ikhrrr7nh9snqzkicv67yti1i&amp;amp;rid=giphy.gif&amp;amp;ct=g&#34; title=&#34;And running. And running.&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;do-1-muscle-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Do 1 muscle up&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Fail.&lt;/strong&gt; Another failure. I didn’t even get close. I did pull-ups frequently, after my shorter runs, but couldn’t get a rhythm going. Another excuse is that I lost access to a decent pull-up bar early in the year, so that I had to make do with the crossbars of small football goals or with playground structures, both of which always have bars that are at the very least slightly too thick to grab them comfortably. If they’re not weirdly shaped to begin with. I’m sure that everyone knows exactly what I’m talking about. Very frequent situation.&lt;/p&gt;
&lt;p&gt;In my eternal optimism, I had also hoped to finish the marathon by mid year. And then focus on the muscle up afterwards. As it turned out, that didn’t quite work either; what with me needing until December to run the marathon and all. So, the muscle up needed more effort due to the conditions not being as ideal, which was more than I could do at the time. Another reason for the failure was poor planning, as I didn’t look for specific muscle-up progressions that would take me there.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sleep-8-hnight&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sleep 8 h/night&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; This was likely the most important aspect. Sleep and rest are super important. We can only burn the midnight oil for so long before it has bad consequences for our mental and physical well-being. Balance is vital. I was very happy to reach and exceed my goal.&lt;/p&gt;
&lt;p&gt;But even though I got an average 8.4h over the year, there remained a notable variance of about 1 hour. I sometimes had to make up for shorter nights with sleeping longer at other times. I’m far from being an expert on sleep science, but from what I have read I believe that consistency is needed for building and maintaining healthy sleep patterns and restful recovery. Those patterns of mine still need some work, as the variations in sleep and wake times were also larger than I would have wanted.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;I’m going to end this post here. Originally, I was planning to put 2021 results and 2022 goals in the same post, but this one is pretty long already. See the lesson above on failing to write frequent and short posts.&lt;/p&gt;
&lt;p&gt;Coming up: how I’m building on these experiences for my 2022 journey.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Colab Pro&#43; Features, Kaggling on Colab, and Cloud GPU Platforms</title>
      <link>https://heads0rtai1s.github.io/2021/08/24/colab-plus-kaggle-cloud-gpu/</link>
      <pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2021/08/24/colab-plus-kaggle-cloud-gpu/</guid>
      <description>


&lt;p&gt;In the final, hectic days of a recent &lt;a href=&#34;https://www.kaggle.com/c/seti-breakthrough-listen&#34;&gt;Kaggle competition&lt;/a&gt; I found myself in want of more GPU power. As one often does in such an occasion. My own laptop, with its &lt;a href=&#34;https://heads0rtai1s.github.io/2021/02/25/gpu-setup-r-python-ubuntu/&#34;&gt;GPU setup&lt;/a&gt;, was doing a fine job with various small models and small images, but scaling up both aspects became necessary to climb the leaderboard further. My first choice options GCP and AWS quickly turned out to require quota increases which either didn’t happen or took too long. Thus, I decided to explore the paid options of &lt;a href=&#34;https://research.google.com/colaboratory/&#34;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I had only ever used the free version of Colab, and found 2 paid subscriptions: Colab Pro and Colab Pro+. The Plus version, at a not insignificant $50/month, was advertising “priority access to faster GPUs” compared to the 10 bucks/month Pro tier. While researching more about this and other vaguenesses in the feature descriptions that the website was offering up, I quickly realised that Plus had only be launched a day or two earlier. And nobody really seemed to know what the exact specs were. So I thought to myself “in for a penny, in for a pound”. Or about 36 pounds at the current exchange rate. Might as well get Plus and figure out what it’s about.&lt;/p&gt;
&lt;p&gt;This post describes the features I found in Colab Pro+, alongside some notes how to best use any version of Colab in a Kaggle competition. After sharing some findings &lt;a href=&#34;https://twitter.com/heads0rtai1s/status/1428833896330899463&#34;&gt;on Twitter&lt;/a&gt;, I received a number of very useful suggestions for Colab alternatives which are compiled in the final part of the post.&lt;/p&gt;
&lt;div id=&#34;colab-pro-features&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Colab Pro+ features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;GPU resources:&lt;/strong&gt; The Plus subscription gave me access to 1 V100 GPU in its “High-RAM” GPU runtime setting. I could only run a single of these sessions at a time. Alternatively, the “Standard” RAM runtime option allowed me to run 2 concurrent sessions with 1 P100 GPU each.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The “High-RAM” runtime did justify its name by providing 53GB of RAM, alongside 8 CPU cores. In my “Standard” RAM sessions I got 13GB RAM and 2 CPUs (which I think might be what’s included in the free Colab).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;runtime limit&lt;/strong&gt; for any session is 24 hours; which was consistent throughout my tests. The Plus subscription advertises that the notebook keeps running even after closing the browser, which can be a useful feature. But I didn’t test this. Be aware that even in Pro+ the runtime still disconnects after a certain time of inactivity (i.e. no cells are running).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With large data, &lt;strong&gt;storage&lt;/strong&gt; is important. Compared to the 100GB of the Pro subscription, Pro+ provided me with 150GB of disk space. This turned out to make a crucial difference in allowing me to copy all the train plus test data, in addition to pip installing updated libraries.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What I didn’t test: Colab’s TPU runtime as well as the number of concurrent CPU sessions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Are those features worth the $50/month? On the one hand, having 24h of V100 power is a notable step up from the free Colab and Kaggle resources. On the other hand, being restricted to 1 session at a time, or 2 sessions with slower P100s, can be a limiting factor in a time crunch.&lt;/p&gt;
&lt;p&gt;Also note, that the Colab FAQ states that “Resources in Colab Pro and Pro+ are prioritized for subscribers who have recently used less resources, in order to prevent the monopolization of limited resources by a small number of users.” Thus, it seems unlikely that one could use a V100 GPU 24/7 for an entire month. I intend to run more experiments and might encounter this limit sooner or later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kaggling-on-colab&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Kaggling on Colab&lt;/h3&gt;
&lt;p&gt;If you’ve exceeded your (considerable) Kaggle resources for the week or, like me, need a bit more horse power for a short time, then moving your Kaggle Notebook into Colab is a good option to keep training and experimenting. It can be non-trivial, though, and the 2 main challenges for me were getting the data and setting up the notebook environment. Well, that and Colab’s slightly infuriating choice to forego many of the standard Jupyter keyboard shortcuts (seriously: why?).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Get your data into Colab:&lt;/strong&gt; by far the best and fastest way here is to copy the data via their &lt;code&gt;GCS_DS_PATH&lt;/code&gt;; i.e. Google Cloud Storage path. Since Kaggle was acquired by Google in 2017, there has been significant integration of its framework into Google’s cloud environments. Kaggle datasets and competition data have cloud storage addresses and can be quickly moved to Colab from there.&lt;/p&gt;
&lt;p&gt;You can get the &lt;code&gt;GCS_DS_PATH&lt;/code&gt; by running the following code in a Kaggle Notebook. Substitute &lt;code&gt;seti-breakthrough-listen&lt;/code&gt; with the name of your competition or dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from kaggle_datasets import KaggleDatasets
GCS_DS_PATH = KaggleDatasets().get_gcs_path(&amp;quot;seti-breakthrough-listen&amp;quot;)
print(GCS_DS_PATH)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within Colab you can then use the &lt;code&gt;gsutil&lt;/code&gt; tool to copy the dataset, or even individual folders, like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;!gsutil -m cp -r {GCS_DS_PATH}/&amp;quot;train&amp;quot; .
!gsutil -m cp -r {GCS_DS_PATH}/&amp;quot;test&amp;quot; .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This retrieves the data significantly faster than by copying from Google Drive or downloading via the (otherwise great) &lt;a href=&#34;https://github.com/Kaggle/kaggle-api&#34;&gt;Kaggle API&lt;/a&gt;. And of course getting the data counts towards your 24 hour runtime limit. Keep in mind that the data is gone after your session gets disconnected, and you need to repeat the setup in a new session.&lt;/p&gt;
&lt;p&gt;The same is true for any files you create in your session (such as trained model weights or submission files) or for &lt;strong&gt;custom libraries&lt;/strong&gt; that you install. Colab has the usual Python and Deep Learning tools installed, but I found the versions to be rather old. You can update via &lt;code&gt;pip&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;!pip install --upgrade --force-reinstall fastai==2.4.1 -qq
!pip install --upgrade --force-reinstall timm==0.4.12 -qq
!pip install --upgrade --force-reinstall torch -qq&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two things to note: after installing you need to restart your runtime to be able to &lt;code&gt;import&lt;/code&gt; the new libraries. No worries: your data will still be there after the restart. Also make sure to leave enough disk space to install everything you need.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Save your outputs on Drive:&lt;/strong&gt; A final note to make sure to copy the results of your experiments - whether they’re trained weights, submission files, or EDA visuals - to your Google Drive account to make sure you don’t lose them when the runtime disconnects. Better to learn from my mistakes than by making them yourself. You can always download stuff manually, but I found that copying them automatically is more reliable.&lt;/p&gt;
&lt;p&gt;You can mount Drive in your Colab notebook like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from google.colab import drive
drive.mount(&amp;#39;/content/drive&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then copy files e.g. via Python’s &lt;code&gt;os.system&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-cloud-gpu-options&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other cloud GPU options&lt;/h3&gt;
&lt;p&gt;While Colab and its Pro versions, as outlined above, have several strong points in their favour, you might want explore other cloud GPU alternatives that either offer more power (A100s!) or are cheaper or more flexible to use. The options here were contributed by other ML practitioners &lt;a href=&#34;https://twitter.com/heads0rtai1s/status/1428833896330899463&#34;&gt;via Twitter&lt;/a&gt; and are listed in no particular order. I’m not including the well-known GCP or AWS here, although someone recommended preemptible (aka “spot”) instances on these platforms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://gradient.paperspace.com/pricing&#34;&gt;Paperspace Gradient&lt;/a&gt;: the G1 subscription costs $8/month and gives free instances with smaller GPUs and a 6h runtime limit. Beyond that, pay $2.30/h to run &lt;a href=&#34;https://docs.paperspace.com/gradient/more/instance-types&#34;&gt;an V100 instance&lt;/a&gt;. 200GB storage included, and 5 concurrent running notebooks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.jarvislabs.ai/&#34;&gt;JarvisCloud&lt;/a&gt;: great landing page, plus A100 GPUs at $2.4/h are being attractive features here. They also offer up-to-date Pytorch, FastAI, Tensorflow as pre-installed frameworks. Storage up to 500GB at max 7 cents per hour.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vast.ai/console/create/&#34;&gt;Vast.ai&lt;/a&gt;: is a marketplace for people to rent out their GPUs. You can also access GCP, AWS, and Paperspace resources here. Prices vary quite a bit, but some look significantly cheaper than those of the big players at a similar level of reliability.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.oracle.com/cloud/compute/pricing.html&#34;&gt;OracleCloud&lt;/a&gt;: seems to be at about $3/h for V100, which is comparable to AWS. A100s “available soon”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.ovhcloud.com/en/public-cloud/prices/&#34;&gt;OHVcloud&lt;/a&gt;: a French provider known for being not very expensive. They have 1 V100 with 400GB storage starting at $1.7/h; which is not bad at all.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Plenty of options available to explore further. Maybe we’ll see prices drop a bit more amidst this healthy competition.&lt;/p&gt;
&lt;p&gt;Hope those are useful for your projects on Kaggle or otherwise. Have fun!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
