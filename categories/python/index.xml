<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on head spin - the Heads or Tails blog</title>
    <link>https://heads0rtai1s.github.io/categories/python/</link>
    <description>Recent content in Python on head spin - the Heads or Tails blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Feb 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://heads0rtai1s.github.io/categories/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Installing CUDA, tensorflow, torch for R &amp; Python on Ubuntu 20.04</title>
      <link>https://heads0rtai1s.github.io/2021/02/25/gpu-setup-r-python-ubuntu/</link>
      <pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2021/02/25/gpu-setup-r-python-ubuntu/</guid>
      <description>


&lt;p&gt;Last weekend, I finally managed to get round to upgrading Ubuntu from version 19.10 to the &lt;a href=&#34;https://releases.ubuntu.com/20.04/&#34;&gt;long-term support release 20.04&lt;/a&gt; on my workhorse laptop. To be precise, I’m using the &lt;a href=&#34;https://kubuntu.org/&#34;&gt;Kubuntu flavour&lt;/a&gt; since &lt;a href=&#34;https://www.youtube.com/watch?v=9QxJQRVQqao&#34;&gt;I’m more of a KDE guy myself&lt;/a&gt;. I usually do a fresh install on those occasions, instead of a &lt;code&gt;dist_upgrade&lt;/code&gt;, because it’s a good opportunity to remove clutter and update software that I might otherwise just keep at an older version, out of convenience.&lt;/p&gt;
&lt;p&gt;One of my main goals this year is to get better at deep learning (DL) in R and Python - and there’s no way around using GPUs for those purposes. My laptop, a Dell G3 15, has a Nvidia GeForce GTX 1660, which at the time of writing does a decent job at playing with smaller neural networks which can then be scaled up on cloud platforms such as &lt;a href=&#34;https://www.kaggle.com/code&#34;&gt;Kaggle Notebooks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Setting up GPU-powered DL libraries on your local machine can still be a somewhat daunting task. Having just done it successfully (crossing fingers; nothing broke yet) I decided to write down my notes and experiences while they are still fresh. At a minimum, this will help me the next time I set up at DL machine. And maybe my experience can even be helpful to others in a similar situation.&lt;/p&gt;
&lt;p&gt;Note: before starting you want to be sure that your machine has a Nvidia GPU that’s recent enough to run DL software. If in doubt, read up on compute capability (and consult &lt;a href=&#34;https://developer.nvidia.com/cuda-gpus#compute&#34;&gt;those tables&lt;/a&gt;).&lt;/p&gt;
&lt;div id=&#34;high-level-overview-main-challenge&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;High-level overview &amp;amp; main challenge&lt;/h1&gt;
&lt;p&gt;These are the main ingredients you need to enable your R &amp;amp; Python DL packages:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;CUDA drivers to access your GPU.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The cuDNN library which provides GPU acceleration.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For Python, the DL framework of your choice: Tensorflow or Pytorch.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For R, the &lt;code&gt;reticulate&lt;/code&gt; package for &lt;code&gt;keras&lt;/code&gt; and/or the new &lt;code&gt;torch&lt;/code&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These steps by themselves are not that hard, and there is a reasonable amount of documentation available online. &lt;strong&gt;The main challenge lies in finding the right library versions that play nicely together.&lt;/strong&gt; This difficulty stems primarily from the breakneck speed at which all the parts of the DL ecosystem continue to evolve. New features are constantly being implemented, and older versions might no longer be supported. For instance, Tensorflow version 2 is significantly re-imagined (and considerably more beginner friendly) than version 1.&lt;/p&gt;
&lt;p&gt;As a result, the latest GPU driver library versions might not always be supported by the latest DL package version. I ran into this problem at the very end of my first installation attempt (when installing Pytorch) and decided that it would be easier to redo everything from scratch. And indeed, the second installation went much smoother and faster. I hope that my lost hours are your gain, dear reader, and that my repeated experience will prove useful in one way or another.&lt;/p&gt;
&lt;p&gt;Below I outline the necessary installation steps.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-by-step-installation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step-by-step installation&lt;/h1&gt;
&lt;div id=&#34;prerequisites-a-clean-system&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prerequisites: a clean system&lt;/h2&gt;
&lt;p&gt;It’s possible that you already have some CUDA or Nvidia libraries installed. But honestly, the best way is to remove everything and start with a clean install. Otherwise there’s just too much danger of version clashes or duplicated paths. The following steps accomplish this. This is also the way in which you can clean up a botched or wrong CUDA installation (like I did) and start afresh. The following is copied from the &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html&#34;&gt;CUDA installation manual&lt;/a&gt; (more on this in the next step):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Remove CUDA Toolkit:
sudo apt-get --purge remove &amp;quot;*cublas*&amp;quot; &amp;quot;*cufft*&amp;quot; &amp;quot;*curand*&amp;quot; &amp;quot;*cusolver*&amp;quot; &amp;quot;*cusparse*&amp;quot; &amp;quot;*npp*&amp;quot; &amp;quot;*nvjpeg*&amp;quot; &amp;quot;cuda*&amp;quot; &amp;quot;nsight*&amp;quot; &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Remove Nvidia Drivers:
sudo apt-get --purge remove &amp;quot;*nvidia*&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Clean up the uninstall:
sudo apt-get autoremove&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some more clean-up tips are given in &lt;a href=&#34;cleanup:%20https://medium.com/@stephengregory_69986/installing-cuda-10-1-on-ubuntu-20-04-e562a5e724a0&#34;&gt;this article&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cuda-drivers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CUDA drivers&lt;/h2&gt;
&lt;p&gt;Let’s get the CUDA GPU drivers (aka CUDA toolkit). Note, that there are instructions for this on software-specific websites, such as &lt;a href=&#34;https://www.tensorflow.org/install/gpu&#34;&gt;for Tensorflow&lt;/a&gt;. However, those aren’t always up to date, and I recommend instead to follow the &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html&#34;&gt;official CUDA installation manual&lt;/a&gt; which is really good and detailed.&lt;/p&gt;
&lt;p&gt;So detailed, in fact, that in can be a little overwhelming at first contact. Here I break down the essential steps:&lt;/p&gt;
&lt;div id=&#34;choose-and-install-the-appropriate-cuda-version&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Choose and install the appropriate CUDA version&lt;/h3&gt;
&lt;p&gt;There’s a nice little platform selector linked in the manual, but &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;do not use this version&lt;/a&gt;. Or at least double check if you want this version. Because this link always chooses the most recent CUDA version, which is 11.2 as I’m writing these lines. Now, also at the time of writing, Pytorch &amp;amp; torchlib only support CUDA 11.0 (not the latest 11.2) and Tensorflow 2.4 is also build against the same version. Therefore, we want to install CUDA 11.0.&lt;/p&gt;
&lt;p&gt;(If you decide to install the latest CUDA version instead, there are some troubleshooting notes at the very bottom of this article that might help you out in a pinch.)&lt;/p&gt;
&lt;p&gt;You can &lt;a href=&#34;https://developer.nvidia.com/cuda-11.0-update1-download-archive&#34;&gt;get the CUDA 11.0 toolkit here&lt;/a&gt;. This gives you the exact same platform selection steps. &lt;a href=&#34;https://developer.nvidia.com/cuda-11.0-update1-download-archive?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;target_distro=Ubuntu&amp;amp;target_version=2004&amp;amp;target_type=deblocal&#34;&gt;This is my configuration&lt;/a&gt;, which gives me the following install commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin
sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda-repo-ubuntu2004-11-0-local_11.0.3-450.51.06-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2004-11-0-local_11.0.3-450.51.06-1_amd64.deb
sudo apt-key add /var/cuda-repo-ubuntu2004-11-0-local/7fa2af80.pub
sudo apt-get update
sudo apt-get install cuda&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Essentially, you download the CUDA toolkit as a &lt;code&gt;.deb&lt;/code&gt; package, add the CUDA repository for Ubuntu 20.04, and install. The &lt;code&gt;pin&lt;/code&gt; stuff makes sure that you continue to pull CUDA stuff from the right repository in the future (&lt;a href=&#34;https://help.ubuntu.com/community/PinningHowto&#34;&gt;see e.g. here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;.deb&lt;/code&gt; file is about 2.2 GB, so you might want to get a cup of coffee or tea while downloading.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-the-correct-library-paths&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Set the correct library paths&lt;/h3&gt;
&lt;p&gt;The easiest way is to copy those three lines into your &lt;code&gt;.bashrc&lt;/code&gt;. (&lt;a href=&#34;https://unix.stackexchange.com/questions/129143/what-is-the-purpose-of-bashrc-and-how-does-it-work&#34;&gt;What is bashrc?&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export PATH=/usr/local/cuda-11.0/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=/usr/local/cuda-11.0/include:$LD_LIBRARY_PATH&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;confirm-the-install&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confirm the install&lt;/h3&gt;
&lt;p&gt;To make sure that everything is working, run those commands. None of them should throw an error:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat /proc/driver/nvidia/version&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;nvcc -V&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This last tool, SMI, is very useful to see your driver versions and also the GPU memory usage during training.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;optional-libraries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Optional libraries&lt;/h3&gt;
&lt;p&gt;Not strictly necessary, but probably useful in one way or another. In my case, I had most of those already installed anyway:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install g++ freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libglu1-mesa libglu1-mesa-dev&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;cudnn-libraries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;cuDNN libraries&lt;/h2&gt;
&lt;p&gt;You also need Nvidia’s &lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN&lt;/a&gt;, the CUDA Deep Neural Network library. Those tools provide GPU-optimised implementation for neural network fundamentals.&lt;/p&gt;
&lt;p&gt;Getting the appropriate cuDNN libraries is easier than the previous step. You can download them from the &lt;a href=&#34;https://developer.nvidia.com/rdp/cudnn-download&#34;&gt;Nvidia developer portal&lt;/a&gt;. That website requires you to make a free account, which is just a formality. When choosing the cuDNN version you will see the options with their matching CUDA versions, e.g.: &lt;code&gt;Download cuDNN v8.1.0 (January 26th, 2021), for CUDA 11.0,11.1 and 11.2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We just installed CUDA 11.0, so we’ll click on the above option which provides a list of download links for different operating systems and architectures. There is a generic version &lt;code&gt;cuDNN Library for Linux (x86_64)&lt;/code&gt; which provides a &lt;code&gt;.tgz&lt;/code&gt; file we could use. But as (K)Ubuntu users we can also download tailored &lt;code&gt;.deb&lt;/code&gt; packages instead. There is a “Developer Version”, a “Runtime Version”, and “Code Samples and User Guide” - all for “Ubuntu20.04 x86_64 (Deb)”. Perfect! Just download everything.&lt;/p&gt;
&lt;p&gt;Once you’ve got the packages, there is a pretty nice &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html&#34;&gt;cuDNN installation guide&lt;/a&gt; which boils down to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo dpkg -i libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb 
sudo dpkg -i libcudnn8-dev_8.1.0.77-1+cuda11.2_amd64.deb 
sudo dpkg -i libcudnn8-samples_8.1.0.77-1+cuda11.2_amd64.deb &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The guide also includes some troubleshooting and verification steps, but this part rarely goes wrong.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tensorflow-pytorch-for-python&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tensorflow &amp;amp; Pytorch for Python&lt;/h2&gt;
&lt;p&gt;The drivers are the main challenge; from here everything should be straightforward. There are 2 main deep learning packages in 2020: Tensorflow and Pytorch. If you’re just starting out with deep learning, then in my view it doesn’t matter much which one you pick. They are both pretty user friendly by now, and the fundamentals are similar enough so that familiarity with one package will help you to get started quickly with the other.&lt;/p&gt;
&lt;p&gt;For &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Tensorflow&lt;/a&gt;, not long ago there were two different Python packages for GPU and CPU, respectively. But now you get everything via:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install tensorflow keras&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://keras.io/&#34;&gt;Keras&lt;/a&gt; is a well-designed high-level API for Tensorflow. These other 2 packages are useful additions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install tensorflow_datasets tensorflow_addons&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For &lt;a href=&#34;https://pytorch.org/&#34;&gt;Pytorch&lt;/a&gt;, I have a penchant for &lt;a href=&#34;https://www.fast.ai/&#34;&gt;FastAI&lt;/a&gt; as a higher-level gateway. Using my preferred &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;miniconda&lt;/a&gt; environment, you can get both from their respective channels like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda create -n fastai -c fastai -c pytorch fastai&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll need some kind of environment manager for the next R step anyway, and it’s easier to keep up with the rapidly evolving libraries if you use some version of &lt;a href=&#34;https://docs.anaconda.com/&#34;&gt;anaconda&lt;/a&gt;. This &lt;code&gt;conda&lt;/code&gt; install will also get you stuff like &lt;code&gt;torchvision&lt;/code&gt; for image models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tensorflow-torch-for-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tensorflow &amp;amp; Torch for R&lt;/h2&gt;
&lt;p&gt;In R, Tensorflow and Keras are best installed via the &lt;a href=&#34;https://keras.rstudio.com/&#34;&gt;keras package&lt;/a&gt;. This uses the fantastic &lt;a href=&#34;https://rstudio.github.io/reticulate/&#34;&gt;reticulate package&lt;/a&gt; as a wrapper around Python’s Tensorflow/Keras, so make sure you got it installed. For an introduction to reticulate check out my &lt;a href=&#34;https://heads0rtai1s.github.io/2019/10/03/reticulate-intro/&#34;&gt;earlier blogpost&lt;/a&gt;. Install as such:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;keras&amp;quot;, repos=&amp;quot;http://cran.r-project.org&amp;quot;, dependencies=TRUE)
keras::install_keras(tensorflow = &amp;quot;gpu&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you want to check your Python configuration for &lt;code&gt;reticulate&lt;/code&gt;, along with the &lt;code&gt;keras&lt;/code&gt; availability:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reticulate::py_config() 
reticulate::py_module_available(&amp;quot;keras&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For torch, there is now a &lt;a href=&#34;https://torch.mlverse.org/&#34;&gt;native R package&lt;/a&gt; which &lt;em&gt;doesn’t&lt;/em&gt; use Pytorch under the hood. (Instead, it’s build on the same C++ backend, called &lt;a href=&#34;https://github.com/pytorch/pytorch/blob/master/docs/libtorch.rst&#34;&gt;libtorch&lt;/a&gt;, as the Python version.)&lt;/p&gt;
&lt;p&gt;The 1st step for installing &lt;code&gt;torch&lt;/code&gt; is this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;torch&amp;quot;, repos=&amp;quot;http://cran.r-project.org&amp;quot;, dependencies=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you need to activate it, which then downloads and installs necessary stuff:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(torch)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: if this step fails for no good reason then you want to try replacing it with &lt;code&gt;install_torch(timeout=1000)&lt;/code&gt;. This timeout is important, because the corresponding files are relative large and the default is only 360 seconds.&lt;/p&gt;
&lt;p&gt;And while you’re there, you might also want to get those extra packages for common use cases:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;torchvision&amp;quot;, repos=&amp;quot;http://cran.r-project.org&amp;quot;, dependencies=TRUE)
install.packages(&amp;quot;torchaudio&amp;quot;, repos=&amp;quot;http://cran.r-project.org&amp;quot;, dependencies=TRUE)
remotes::install_github(&amp;quot;mlverse/torchdatasets&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;does-everything-work&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Does everything work?&lt;/h1&gt;
&lt;p&gt;Now everything should be there on your machine. But does it all work as it should?&lt;/p&gt;
&lt;p&gt;In Python, you can check Tensorflow and Pytorch as such (and get some information about your GPU in the process):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import tensorflow as tf
tf.config.list_physical_devices(&amp;#39;GPU&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’re using &lt;code&gt;conda&lt;/code&gt;, don’t forget to &lt;code&gt;activate&lt;/code&gt; your environment:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import torch
torch.cuda.get_device_name()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In R, the installations steps should already have told you if something didn’t work. In addition, you can also check the status of the &lt;code&gt;keras&lt;/code&gt; and &lt;code&gt;torch&lt;/code&gt; packages like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(keras)
is_keras_available()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;library(torch)
cuda_is_available()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that’s it! You now have GPU-powered deep learning capabilities at your disposal. Use them wisely. Or, you know, just have fun with them. Either way, I hope this post was helpful.&lt;/p&gt;
&lt;p&gt;More info:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This is far from the only write-up on getting your GPU tools set up on (Ubuntu) Linux. For my first successful installation on Kubuntu 19.10 I was largely following &lt;a href=&#34;https://medium.com/@Oysiyl/install-tensorflow-2-with-gpu-support-on-ubuntu-19-10-f502ae85593c&#34;&gt;this post by Dmitriy Kisil&lt;/a&gt;. For the current 20.04 install, I compared the CUDA and cuDNN instructions to posts by &lt;a href=&#34;https://medium.com/@tunguz/installing-tensorflow-on-ubuntu-20-04-bcada5a9c7e1&#34;&gt;Bojan Tunguz&lt;/a&gt; and &lt;a href=&#34;https://medium.com/@stephengregory_69986/installing-cuda-10-1-on-ubuntu-20-04-e562a5e724a0&#34;&gt;Stephen Gregory&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remember that you don’t need to install GPU software if all you want to do is to experiment with deep learning tools. There are plenty of resources online where you can try out the code, e.g. via Google Colab, in a pre-configured cloud environment. This also includes &lt;a href=&#34;https://www.kaggle.com/code&#34;&gt;Kaggle Notebooks&lt;/a&gt;, which come equipped with a large set of data science and machine learning packages.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I managed to install CUDA 11.2 (and TF 2.4), but it was less smooth than version 11.0. Specifically, &lt;code&gt;sudo apt-get install cuda&lt;/code&gt; threw a &lt;code&gt;you have held broken packages&lt;/code&gt; error and refused to do the install. The solution was to use the more tenacious &lt;code&gt;aptitude&lt;/code&gt; to &lt;code&gt;sudo aptitude install cuda&lt;/code&gt; which suggested that &lt;code&gt;libnvidia-compute-460&lt;/code&gt; needed to be downgraded. After that the install worked without a hitch.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Another CUDA 11.2 related issue popped up for Tensorflow on Python. The installation worked and &lt;code&gt;import tensorflow&lt;/code&gt; also worked, but when using the library I got the error message &lt;code&gt;Could not load dynamic library &#39;libcusolver.so.10&#39;&lt;/code&gt;. This was most likely related to TF 2.4 being build against CUDA 11.0, not 11.2 (see &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues/45848&#34;&gt;here&lt;/a&gt;). The workaround was to make a hard-link to pretend that &lt;code&gt;.so.11&lt;/code&gt; is &lt;code&gt;.so.10&lt;/code&gt; (see &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues/44777&#34;&gt;here&lt;/a&gt;): &lt;code&gt;cd /usr/local/cuda-11.2/lib64; sudo ln libcusolver.so.11 libcusolver.so.10&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R &amp; Python Rosetta Stone: EDA with dplyr vs pandas</title>
      <link>https://heads0rtai1s.github.io/2020/11/05/r-python-dplyr-pandas/</link>
      <pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2020/11/05/r-python-dplyr-pandas/</guid>
      <description>


&lt;p&gt;This is the first post in a new series featuring translations between R and Python code for common data science and machine learning tasks. A &lt;a href=&#34;https://en.wikipedia.org/wiki/Rosetta_Stone&#34;&gt;Rosetta Stone&lt;/a&gt;, if you will. I’m writing this mainly as a documented cheat sheet for myself, as I’m frequently switching between the two languages. Personally, I have learned Python and R around the same time, several years ago, but tidy R is much more intuitive to me than any other language. Hopefully, these posts can be useful to others in a similar situation. My point of reference is primarily R - with the aim to provide equivalent Python code - but occasionally I will look at translations of Python features into R.&lt;/p&gt;
&lt;p&gt;The first post will focus on the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; backbone &lt;a href=&#34;https://dplyr.tidyverse.org/&#34;&gt;dplyr&lt;/a&gt; and compare it to Python’s data science workhorse &lt;a href=&#34;https://pandas.pydata.org/&#34;&gt;pandas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As with all my blog posts, the code will run in &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;Rmarkdown&lt;/a&gt; through the fantastic &lt;a href=&#34;https://rstudio.com/&#34;&gt;Rstudio IDE&lt;/a&gt;. All the output will be reproducible. Rstudio provides Python support via the great &lt;a href=&#34;https://rstudio.github.io/reticulate/&#34;&gt;reticulate&lt;/a&gt; package. If you haven’t heard of it yet, check out my &lt;a href=&#34;https://heads0rtai1s.github.io/2019/10/03/reticulate-intro/&#34;&gt;intro post on reticulate&lt;/a&gt; to get started.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note: you need at least RStudio version 1.2 to be able to pass objects between R and Python.&lt;/strong&gt; In addition, as always, here are the required packages. We are also setting the python path (see the &lt;a href=&#34;https://heads0rtai1s.github.io/2019/10/03/reticulate-intro/&#34;&gt;intro post&lt;/a&gt; for more details on the path) and importing &lt;em&gt;pandas&lt;/em&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
libs &amp;lt;- c(&amp;#39;dplyr&amp;#39;,                # wrangling
          &amp;#39;palmerpenguins&amp;#39;,       # data
          &amp;#39;knitr&amp;#39;,&amp;#39;kableExtra&amp;#39;,   # table styling
          &amp;#39;reticulate&amp;#39;)           # python support 
invisible(lapply(libs, library, character.only = TRUE))

use_python(&amp;quot;/usr/bin/python&amp;quot;)

df &amp;lt;- penguins %&amp;gt;% 
  mutate_if(is.integer, as.double)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
import pandas as pd
pd.set_option(&amp;#39;display.max_columns&amp;#39;, None)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will be using the &lt;a href=&#34;https://www.kaggle.com/parulpandey/palmer-archipelago-antarctica-penguin-data&#34;&gt;Palmer Penguins dataset&lt;/a&gt;, as provided by the brilliant &lt;a href=&#34;https://www.allisonhorst.com/&#34;&gt;Allison Horst&lt;/a&gt; through the &lt;a href=&#34;https://allisonhorst.github.io/palmerpenguins/&#34;&gt;eponymous R package&lt;/a&gt; - complete with her trademark adorable artwork. I was looking for an excuse to work with this dataset. Therefore, this will be a genuine example for an exploratory analysis, as I’m encountering the data for the first time.&lt;/p&gt;
&lt;div id=&#34;general-overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;General overview&lt;/h3&gt;
&lt;p&gt;It’s best to start your EDA by looking at the big picture: How large is the dataset? How many features are there? What are the data types? Here we have tabular data, but similar steps can be taken for text or images as well.&lt;/p&gt;
&lt;p&gt;In R, I typically go with a combination of &lt;strong&gt;head&lt;/strong&gt;, &lt;strong&gt;glimpse&lt;/strong&gt;, and &lt;strong&gt;summary&lt;/strong&gt; to get a broad idea of the data properties. (Beyond &lt;em&gt;dplyr&lt;/em&gt;, there’s also the &lt;a href=&#34;https://cran.r-project.org/package=skimr&#34;&gt;skimr package&lt;/a&gt; for more sophisticated data overviews.)&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;head&lt;/code&gt;, we see the first (by default) 6 rows of the dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##   species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g
##   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;           &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 Adelie  Torge…           39.1          18.7              181        3750
## 2 Adelie  Torge…           39.5          17.4              186        3800
## 3 Adelie  Torge…           40.3          18                195        3250
## 4 Adelie  Torge…           NA            NA                 NA          NA
## 5 Adelie  Torge…           36.7          19.3              193        3450
## 6 Adelie  Torge…           39.3          20.6              190        3650
## # … with 2 more variables: sex &amp;lt;fct&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We find that our data is a mix of numerical and categorical columns. There are 8 columns in total. We get an idea of the order of magnitude of the numeric features, see that the categorical ones have text, and already spot a couple of missing values. (Note, that I converted all integer columns to double for easier back-and-forth with Python).&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;pandas&lt;/em&gt; &lt;strong&gt;head&lt;/strong&gt; command is essentially the same. One general difference here is that in &lt;em&gt;pandas&lt;/em&gt; (and Python in general) everything is an object. Methods (and attributes) associated with the object, which is a &lt;em&gt;pandas&lt;/em&gt; &lt;code&gt;DataFrame&lt;/code&gt; here, are accessed via the dot “.” operator. For passing an R object to Python we preface it with &lt;code&gt;r.&lt;/code&gt; like such:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \
## 0  Adelie  Torgersen            39.1           18.7              181.0   
## 1  Adelie  Torgersen            39.5           17.4              186.0   
## 2  Adelie  Torgersen            40.3           18.0              195.0   
## 3  Adelie  Torgersen             NaN            NaN                NaN   
## 4  Adelie  Torgersen            36.7           19.3              193.0   
## 
##    body_mass_g     sex    year  
## 0       3750.0    male  2007.0  
## 1       3800.0  female  2007.0  
## 2       3250.0  female  2007.0  
## 3          NaN     NaN  2007.0  
## 4       3450.0  female  2007.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is less pretty than for Rmarkdown, but the result is pretty much the same. We don’t see column types, only their values. Another property of &lt;em&gt;pandas&lt;/em&gt; data frames is that they come with a row index. By default this is a sequential number of rows, but anything can become an index.&lt;/p&gt;
&lt;p&gt;With &lt;em&gt;dplyr&lt;/em&gt;’s &lt;strong&gt;glimpse&lt;/strong&gt; we can see a more compact, transposed display of column types and their values. Especially for datasets with many columns this can be a vital complement to &lt;code&gt;head&lt;/code&gt;. We also see the number of rows and columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
glimpse(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 344
## Variables: 8
## $ species           &amp;lt;fct&amp;gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie…
## $ island            &amp;lt;fct&amp;gt; Torgersen, Torgersen, Torgersen, Torgersen, To…
## $ bill_length_mm    &amp;lt;dbl&amp;gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, …
## $ bill_depth_mm     &amp;lt;dbl&amp;gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, …
## $ flipper_length_mm &amp;lt;dbl&amp;gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 19…
## $ body_mass_g       &amp;lt;dbl&amp;gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, …
## $ sex               &amp;lt;fct&amp;gt; male, female, female, NA, female, male, female…
## $ year              &amp;lt;dbl&amp;gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alongside the 8 columns, our dataset has 344 rows. That’s pretty small.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;pandas&lt;/em&gt;, there’s no identical equivalent to &lt;strong&gt;glimpse&lt;/strong&gt;. Instead, we can use the &lt;strong&gt;info&lt;/strong&gt; method to give us the feature types in vertical form. We also see the number of non-null features (the “sex” column has the fewest), together with the number of rows and columns.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
## RangeIndex: 344 entries, 0 to 343
## Data columns (total 8 columns):
## species              344 non-null category
## island               344 non-null category
## bill_length_mm       342 non-null float64
## bill_depth_mm        342 non-null float64
## flipper_length_mm    342 non-null float64
## body_mass_g          342 non-null float64
## sex                  333 non-null category
## year                 344 non-null float64
## dtypes: category(3), float64(5)
## memory usage: 14.8 KB&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While several columns have missing values, the overall number of those null entries is small.&lt;/p&gt;
&lt;p&gt;Additionally, &lt;em&gt;pandas&lt;/em&gt; gives us a summary of data types (3 categorical, 5 float) and tells us how much memory our data takes up. In R, you can use the &lt;code&gt;utils&lt;/code&gt; tool &lt;strong&gt;object.size&lt;/strong&gt; for the latter:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
object.size(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 21336 bytes&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next up is &lt;strong&gt;summary&lt;/strong&gt;, which provides basic overview stats for each feature. Here in particular, we learn that there are 3 penguin species, 3 islands, and 11 missing values in the sex column. “Chinstrap” penguins are rarer than the other two species; and Torgersen is the least frequent island. We also see the fundamental characteristics of the numeric features (min, max, quartiles, median). Mean and median are pretty close for most of those, which suggests little skewness:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
summary(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       species          island    bill_length_mm  bill_depth_mm  
##  Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  
##  Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  
##  Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  
##                                  Mean   :43.92   Mean   :17.15  
##                                  3rd Qu.:48.50   3rd Qu.:18.70  
##                                  Max.   :59.60   Max.   :21.50  
##                                  NA&amp;#39;s   :2       NA&amp;#39;s   :2      
##  flipper_length_mm  body_mass_g       sex           year     
##  Min.   :172.0     Min.   :2700   female:165   Min.   :2007  
##  1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  
##  Median :197.0     Median :4050   NA&amp;#39;s  : 11   Median :2008  
##  Mean   :200.9     Mean   :4202                Mean   :2008  
##  3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  
##  Max.   :231.0     Max.   :6300                Max.   :2009  
##  NA&amp;#39;s   :2         NA&amp;#39;s   :2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The closest &lt;em&gt;pandas&lt;/em&gt; equivalent to &lt;strong&gt;summary&lt;/strong&gt; is &lt;strong&gt;describe&lt;/strong&gt;. By default this only includes the numeric columns, but you can get around that by passing a list of features types that you want to include:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.describe(include = [&amp;#39;float&amp;#39;, &amp;#39;category&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        species  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \
## count      344     344      342.000000     342.000000         342.000000   
## unique       3       3             NaN            NaN                NaN   
## top     Adelie  Biscoe             NaN            NaN                NaN   
## freq       152     168             NaN            NaN                NaN   
## mean       NaN     NaN       43.921930      17.151170         200.915205   
## std        NaN     NaN        5.459584       1.974793          14.061714   
## min        NaN     NaN       32.100000      13.100000         172.000000   
## 25%        NaN     NaN       39.225000      15.600000         190.000000   
## 50%        NaN     NaN       44.450000      17.300000         197.000000   
## 75%        NaN     NaN       48.500000      18.700000         213.000000   
## max        NaN     NaN       59.600000      21.500000         231.000000   
## 
##         body_mass_g   sex         year  
## count    342.000000   333   344.000000  
## unique          NaN     2          NaN  
## top             NaN  male          NaN  
## freq            NaN   168          NaN  
## mean    4201.754386   NaN  2008.029070  
## std      801.954536   NaN     0.818356  
## min     2700.000000   NaN  2007.000000  
## 25%     3550.000000   NaN  2007.000000  
## 50%     4050.000000   NaN  2008.000000  
## 75%     4750.000000   NaN  2009.000000  
## max     6300.000000   NaN  2009.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You see that the formatting is less clever, since categorical indicators like “unique” or “top” are shown (with NAs) for the numeric columns and vice versa. Also, we only get told that there are 3 species and the most frequent one is “Adelie”; not the full counts per species.&lt;/p&gt;
&lt;p&gt;We have already learned a lot about our data from those basic overview tools. Typically, at this point in the EDA I would now start plotting feature distributions and interactions. Since we’re only focussing on &lt;em&gt;dplyr&lt;/em&gt; here, this part will have to wait for a future “ggplot2 vs seaborn” episode. For now, let’s look at the most simple overview before moving on to &lt;em&gt;dplyr&lt;/em&gt; verbs: number of rows and columns.&lt;/p&gt;
&lt;p&gt;In R, there is &lt;strong&gt;dim&lt;/strong&gt; while pandas has &lt;strong&gt;shape&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
dim(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 344   8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.shape&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (344, 8)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;subsetting-rows-and-columns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subsetting rows and columns&lt;/h3&gt;
&lt;p&gt;For extracting subsets of rows and columns, &lt;em&gt;dplyr&lt;/em&gt; has the verbs &lt;strong&gt;filter&lt;/strong&gt; and &lt;strong&gt;select&lt;/strong&gt;, respectively. For instance, let’s look at the species and sex of the penguins with the shortest bills:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
df %&amp;gt;% 
  filter(bill_length_mm &amp;lt; 34) %&amp;gt;% 
  select(species, sex, bill_length_mm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   species sex    bill_length_mm
##   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;           &amp;lt;dbl&amp;gt;
## 1 Adelie  female           33.5
## 2 Adelie  female           33.1
## 3 Adelie  female           32.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of those are female Adelie penguins. This is good news for a potential species classifier.&lt;/p&gt;
&lt;p&gt;In pandas, there are several ways to achieve the same result. All of them are a little more complicated than our two &lt;em&gt;dplyr&lt;/em&gt; verbs. The most pythonic way is probably to use the &lt;strong&gt;loc&lt;/strong&gt; operator like such:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.loc[r.df.bill_length_mm &amp;lt; 34, [&amp;#39;species&amp;#39;, &amp;#39;sex&amp;#39;, &amp;#39;bill_length_mm&amp;#39;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     species     sex  bill_length_mm
## 70   Adelie  female            33.5
## 98   Adelie  female            33.1
## 142  Adelie  female            32.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This indexing via the “[]” brackets is essentially the same as in base R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
na.omit(df[df$bill_length_mm &amp;lt; 34, c(&amp;#39;species&amp;#39;, &amp;#39;sex&amp;#39;, &amp;#39;bill_length_mm&amp;#39;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   species sex    bill_length_mm
##   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;           &amp;lt;dbl&amp;gt;
## 1 Adelie  female           33.5
## 2 Adelie  female           33.1
## 3 Adelie  female           32.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way is to use the pandas method &lt;strong&gt;query&lt;/strong&gt; as an equivalent for &lt;strong&gt;filter&lt;/strong&gt;. This allows us to essentially use &lt;em&gt;dplyr&lt;/em&gt;-style evaluation syntax. I found that in practice, &lt;strong&gt;query&lt;/strong&gt; is often notably slower than the other indexing tools. This can be important if your’re dealing with large datasets:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.query(&amp;quot;bill_length_mm &amp;lt; 34&amp;quot;).loc[:, [&amp;#39;species&amp;#39;, &amp;#39;sex&amp;#39;, &amp;#39;bill_length_mm&amp;#39;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     species     sex  bill_length_mm
## 70   Adelie  female            33.5
## 98   Adelie  female            33.1
## 142  Adelie  female            32.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In tidy R, the &lt;em&gt;dplyr&lt;/em&gt; verb &lt;strong&gt;select&lt;/strong&gt; can extract by value as well as by position. And for extracting rows by position there is &lt;strong&gt;slice&lt;/strong&gt;. This is just an arbitrary subset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
df %&amp;gt;% 
  slice(c(1,2,3)) %&amp;gt;% 
  select(c(4,5,6))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   bill_depth_mm flipper_length_mm body_mass_g
##           &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1          18.7               181        3750
## 2          17.4               186        3800
## 3          18                 195        3250&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In pandas, we would use bracket indexing again, but instead of &lt;strong&gt;loc&lt;/strong&gt; we now need &lt;strong&gt;iloc&lt;/strong&gt; (i.e. locating by index). This might be a good point to remind ourselves that Python starts counting from 0 and R starts from 1:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.iloc[[0, 1, 2], [3, 4, 5]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    bill_depth_mm  flipper_length_mm  body_mass_g
## 0           18.7              181.0       3750.0
## 1           17.4              186.0       3800.0
## 2           18.0              195.0       3250.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To emphasise again: pandas uses &lt;strong&gt;loc&lt;/strong&gt; for conditional indexing and &lt;strong&gt;iloc&lt;/strong&gt; for positional indexing. This takes some getting uses to, but this simple rule covers most of &lt;em&gt;pandas’&lt;/em&gt; subsetting operations.&lt;/p&gt;
&lt;p&gt;Retaining unique rows and removing duplicates can be thought of as another way of subsetting a data frame. In &lt;em&gt;dplyr&lt;/em&gt;, there’s the &lt;strong&gt;distinct&lt;/strong&gt; function which takes as arguments the columns that are considered for identifying duplicated rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
df %&amp;gt;% 
  slice(c(2, 4, 186)) %&amp;gt;% 
  distinct(species, island)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   species island   
##   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;    
## 1 Adelie  Torgersen
## 2 Gentoo  Biscoe&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;em&gt;pandas&lt;/em&gt; we can achieve the same result via the &lt;strong&gt;drop_duplicates&lt;/strong&gt; method:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;r.df.iloc[[2, 4, 186], :].drop_duplicates([&amp;#39;species&amp;#39;, &amp;#39;island&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \
## 2    Adelie  Torgersen            40.3           18.0              195.0   
## 186  Gentoo     Biscoe            49.1           14.8              220.0   
## 
##      body_mass_g     sex    year  
## 2         3250.0  female  2007.0  
## 186       5150.0  female  2008.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to retain only the two affected rows, like in the dplyr example above, you would have to select them using &lt;code&gt;.loc&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arrange-and-sample&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Arrange and Sample&lt;/h3&gt;
&lt;p&gt;Let’s deal with the arranging and sampling of rows in one fell swoop. In &lt;em&gt;dplyr&lt;/em&gt;, we use &lt;strong&gt;sample_n&lt;/strong&gt; (or &lt;strong&gt;sample_frac&lt;/strong&gt;) to choose a random subset of &lt;code&gt;n&lt;/code&gt; rows (or a fraction &lt;code&gt;frac&lt;/code&gt; of rows). Ordering a row by its values uses the verb &lt;strong&gt;arrange&lt;/strong&gt;, optionally with the &lt;strong&gt;desc&lt;/strong&gt; tool to specific descending order:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
set.seed(4321)
df %&amp;gt;% 
  select(species, bill_length_mm) %&amp;gt;% 
  sample_n(4) %&amp;gt;% 
  arrange(desc(bill_length_mm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 2
##   species   bill_length_mm
##   &amp;lt;fct&amp;gt;              &amp;lt;dbl&amp;gt;
## 1 Chinstrap           47.5
## 2 Adelie              42.7
## 3 Adelie              40.2
## 4 Adelie              34.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;em&gt;pandas&lt;/em&gt;, random sampling is done through the &lt;strong&gt;sample&lt;/strong&gt; function, which can take either a fraction or a number of rows. Here, we can also pass directly a random seed (called &lt;em&gt;random_state&lt;/em&gt;), instead of defining it outside the function via R’s &lt;strong&gt;set.seed&lt;/strong&gt;. Arranging is called &lt;strong&gt;sort_values&lt;/strong&gt; and we have to specify an &lt;em&gt;ascending&lt;/em&gt; flag because &lt;em&gt;pandas&lt;/em&gt; wants to be different:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.loc[:, [&amp;#39;species&amp;#39;, &amp;#39;bill_length_mm&amp;#39;]].\
  sample(n = 4, random_state = 4321).\
  sort_values(&amp;#39;bill_length_mm&amp;#39;, ascending = False)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        species  bill_length_mm
## 305  Chinstrap            52.8
## 301  Chinstrap            52.0
## 291  Chinstrap            50.5
## 165     Gentoo            48.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you see, the &lt;em&gt;pandas&lt;/em&gt; dot methods can be chained in a way reminiscent of the &lt;em&gt;dplyr&lt;/em&gt; pipe (&lt;code&gt;%&amp;gt;%&lt;/code&gt;). The scope for this style is much narrower than the pipe, though. It only works for methods and attributes associated with the specific &lt;em&gt;pandas&lt;/em&gt; data frame and its transformation results. Nevertheless, it has a certain power and elegance for pipe aficionados. When written accross multiple lines it requires the Python line continuation operator &lt;code&gt;\&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;group-by-and-summarise&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Group By and Summarise&lt;/h3&gt;
&lt;p&gt;Here is where &lt;em&gt;dplyr&lt;/em&gt; really shines. Grouping and summarising transformations fit seemlessly into any wrangling workflow by preserving the tidy &lt;code&gt;tibble&lt;/code&gt; data format. The verbs are &lt;strong&gt;group_by&lt;/strong&gt; and &lt;strong&gt;summarise&lt;/strong&gt;. Let’s compare average bill lengths among species to find that “Adelie” penguins have significantly shorter bills:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
df %&amp;gt;% 
  group_by(species) %&amp;gt;% 
  summarise(mean_bill_length = mean(bill_length_mm, na.rm = TRUE),
            sd_bill_length = sd(bill_length_mm, na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   species   mean_bill_length sd_bill_length
##   &amp;lt;fct&amp;gt;                &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 Adelie                38.8           2.66
## 2 Chinstrap             48.8           3.34
## 3 Gentoo                47.5           3.08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is consistent to what we had seen before for the 3 rows with shortes bills. The results also indicate that it would be much harder to try to separate “Chinstrap” vs “Gentoo” by &lt;code&gt;bill_length&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In terms of usage, &lt;em&gt;pandas&lt;/em&gt; is similar: we have &lt;strong&gt;groupby&lt;/strong&gt; (without the “&lt;code&gt;_&lt;/code&gt;”) to define the grouping, and &lt;strong&gt;agg&lt;/strong&gt; to aggregate (i.e. summarise) according to specific measures for certain features:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.groupby(&amp;#39;species&amp;#39;).agg({&amp;#39;bill_length_mm&amp;#39;: [&amp;#39;mean&amp;#39;, &amp;#39;std&amp;#39;]})&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           bill_length_mm          
##                     mean       std
## species                           
## Adelie         38.791391  2.663405
## Chinstrap      48.833824  3.339256
## Gentoo         47.504878  3.081857&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is pretty much the default way in &lt;em&gt;pandas&lt;/em&gt;. It is worth noting that &lt;em&gt;pandas&lt;/em&gt;, and Python in general, gets a lot of milage out of 2 data structures: &lt;strong&gt;lists&lt;/strong&gt; like &lt;code&gt;[1, 2, 3]&lt;/code&gt; which are the equivalent of &lt;code&gt;c(1, 2, 3)&lt;/code&gt;, and &lt;strong&gt;dictionaries&lt;/strong&gt; &lt;code&gt;{&#39;a&#39;: 1, &#39;b&#39;: 2}&lt;/code&gt; which are sets of key-value pairs. Values in a dictionary can be pretty much anything, including lists or dictionaries. (Most uses cases for dictionaries are probably served by &lt;em&gt;dplyr&lt;/em&gt; tibbles.) Here we use a dictionary to define which column we want to summarise and how. Note again the chaining via dots.&lt;/p&gt;
&lt;p&gt;The outcome of this operation, however, is slightly different from &lt;em&gt;dplyr&lt;/em&gt; in that we get a hierarchical data frame with a categorical index (this is no longer a “normal” column&amp;quot;) and 2 data columns that both have the designation &lt;code&gt;bill_length_mm&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.groupby(&amp;#39;species&amp;#39;).agg({&amp;#39;bill_length_mm&amp;#39;: [&amp;#39;mean&amp;#39;, &amp;#39;std&amp;#39;]}).info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
## CategoricalIndex: 3 entries, Adelie to Gentoo
## Data columns (total 2 columns):
## (bill_length_mm, mean)    3 non-null float64
## (bill_length_mm, std)     3 non-null float64
## dtypes: float64(2)
## memory usage: 155.0 bytes&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will see the limitations of that format if you try to chain another method. To get something instead that’s more closely resembling our &lt;em&gt;dplyr&lt;/em&gt; output, here is a different way: we forego the dictionary in favour of a simple list, then add a suffix later, and finally reset the index to a normal column:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.groupby(&amp;#39;species&amp;#39;).bill_length_mm.agg([&amp;#39;mean&amp;#39;, &amp;#39;std&amp;#39;]).add_suffix(&amp;quot;_bill_length&amp;quot;).reset_index()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      species  mean_bill_length  std_bill_length
## 0     Adelie         38.791391         2.663405
## 1  Chinstrap         48.833824         3.339256
## 2     Gentoo         47.504878         3.081857&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can treat the result like a typical data frame. Note, that here we use another form of column indexing to select &lt;code&gt;bill_length_mm&lt;/code&gt; after the &lt;code&gt;groupby&lt;/code&gt;. This shorthand, which treats a feature as an object attribute, only works for single columns. Told you that &lt;em&gt;pandas&lt;/em&gt; indexing can be a little confusing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;joining-and-binding&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Joining and binding&lt;/h3&gt;
&lt;p&gt;There’s another source of &lt;em&gt;dplyr&lt;/em&gt; vs &lt;em&gt;pandas&lt;/em&gt; confusion when it comes to SQL-style joins and to binding rows and columns. To demonstrate, we’ll create an additional data frame which holds the mean bill length by species. And we pretend that it’s a separate table. In terms of analysis steps, this crosses over from EDA into feature engineering territory, but that line can get blurry if you’re exploring a dataset’s hidden depths.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
df2 &amp;lt;- df %&amp;gt;% 
  group_by(species) %&amp;gt;% 
  summarise(mean_bill = mean(bill_length_mm, na.rm = TRUE))

df2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   species   mean_bill
##   &amp;lt;fct&amp;gt;         &amp;lt;dbl&amp;gt;
## 1 Adelie         38.8
## 2 Chinstrap      48.8
## 3 Gentoo         47.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;dplyr&lt;/em&gt; verbs for SQL-like joins are very similar to the various SQL flavours. We have &lt;strong&gt;left_join&lt;/strong&gt;, &lt;strong&gt;right_join&lt;/strong&gt;, &lt;strong&gt;inner_join&lt;/strong&gt;, &lt;strong&gt;outer_join&lt;/strong&gt;; as well as the very useful filtering joins &lt;strong&gt;semi_join&lt;/strong&gt; and &lt;strong&gt;anti_join&lt;/strong&gt; (keep and discard what matches, respectively):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
set.seed(4321)
df %&amp;gt;% 
  left_join(df2, by = &amp;quot;species&amp;quot;) %&amp;gt;% 
  select(species, bill_length_mm, mean_bill) %&amp;gt;% 
  sample_n(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 3
##   species   bill_length_mm mean_bill
##   &amp;lt;fct&amp;gt;              &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 Adelie              42.7      38.8
## 2 Chinstrap           47.5      48.8
## 3 Adelie              40.2      38.8
## 4 Adelie              34.6      38.8
## 5 Gentoo              53.4      47.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can for instance subtract the mean bill length from the individual values to get the residuals or to standardise our features.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;pandas&lt;/em&gt;, joining uses the &lt;strong&gt;merge&lt;/strong&gt; operator. You have to specify the type of join via the &lt;em&gt;how&lt;/em&gt; parameter and the join key via &lt;em&gt;on&lt;/em&gt;, like such:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.\
  sample(n = 5, random_state = 24).\
  merge(r.df2, how = &amp;quot;left&amp;quot;, on = &amp;quot;species&amp;quot;).\
  loc[:, [&amp;#39;species&amp;#39;, &amp;#39;bill_length_mm&amp;#39;, &amp;#39;mean_bill&amp;#39;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      species  bill_length_mm  mean_bill
## 0     Gentoo            43.4  47.504878
## 1  Chinstrap            51.7  48.833824
## 2     Gentoo            46.2  47.504878
## 3     Adelie            43.1  38.791391
## 4     Adelie            41.3  38.791391&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to bring in the aforementioned source of confusion: &lt;em&gt;pandas&lt;/em&gt; also has an operator called &lt;strong&gt;join&lt;/strong&gt; which joins by matching indeces, instead of columns, between two tables. This is a pretty pandas-specific convenience shortcut, since it relies on the data frame index. In practice I recommend using &lt;strong&gt;merge&lt;/strong&gt; instead. The little conveniece provided by &lt;strong&gt;join&lt;/strong&gt; is not worth the additional confusion.&lt;/p&gt;
&lt;p&gt;Binding rows and columns in &lt;em&gt;dplyr&lt;/em&gt; uses &lt;strong&gt;bind_rows&lt;/strong&gt; and &lt;strong&gt;bind_cols&lt;/strong&gt;. For the rows, there’s really not much of a practical requirement other than that some of the column names match and that those ideally have the same type (which is not strictly necessary):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
head(df, 2) %&amp;gt;% 
  bind_rows(tail(df, 2) %&amp;gt;% select(year, everything(), -sex))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 8
##   species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g
##   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;           &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 Adelie  Torge…           39.1          18.7              181        3750
## 2 Adelie  Torge…           39.5          17.4              186        3800
## 3 Chinst… Dream            50.8          19                210        4100
## 4 Chinst… Dream            50.2          18.7              198        3775
## # … with 2 more variables: sex &amp;lt;fct&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For columns, it is necessary that the features we’re binding to a tibble have the same number of rows. The big assumption here is that the row orders match (but that can be set beforehand):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
select(df, species) %&amp;gt;% 
  bind_cols(df %&amp;gt;% select(year)) %&amp;gt;% 
  head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   species  year
##   &amp;lt;fct&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Adelie   2007
## 2 Adelie   2007
## 3 Adelie   2007
## 4 Adelie   2007
## 5 Adelie   2007&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;pandas&lt;/em&gt; equivalent to &lt;strong&gt;bind_rows&lt;/strong&gt; is &lt;strong&gt;append&lt;/strong&gt;. This nomenclature is borrowed from Python’s overall reliance on list operations.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.head(2).append(r.df.tail(2).drop(&amp;quot;sex&amp;quot;, axis = &amp;quot;columns&amp;quot;), sort = False)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \
## 0       Adelie  Torgersen            39.1           18.7              181.0   
## 1       Adelie  Torgersen            39.5           17.4              186.0   
## 342  Chinstrap      Dream            50.8           19.0              210.0   
## 343  Chinstrap      Dream            50.2           18.7              198.0   
## 
##      body_mass_g     sex    year  
## 0         3750.0    male  2007.0  
## 1         3800.0  female  2007.0  
## 342       4100.0     NaN  2009.0  
## 343       3775.0     NaN  2009.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we also demonstrate how to &lt;strong&gt;drop&lt;/strong&gt; a column from a data frame (i.e. the equivalent to &lt;em&gt;dplyr&lt;/em&gt; &lt;strong&gt;select&lt;/strong&gt; with a minus).&lt;/p&gt;
&lt;p&gt;Binding &lt;em&gt;pandas&lt;/em&gt; columns uses &lt;strong&gt;concat&lt;/strong&gt;, which takes a Python list as its parameter:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
pd.concat([r.df.loc[:, &amp;#39;species&amp;#39;], r.df.loc[:, &amp;#39;year&amp;#39;]], axis = &amp;quot;columns&amp;quot;).head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   species    year
## 0  Adelie  2007.0
## 1  Adelie  2007.0
## 2  Adelie  2007.0
## 3  Adelie  2007.0
## 4  Adelie  2007.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that’s it for the basics!&lt;/p&gt;
&lt;p&gt;Of course, there are a number of intricacies and special cases here, but by and large this should serve as a useful list of examples to get you started in &lt;em&gt;pandas&lt;/em&gt; coming from &lt;em&gt;dplyr&lt;/em&gt;, and perhaps also vice versa. Future installments in this series will go beyond &lt;em&gt;dplyr&lt;/em&gt;, but likely also touch back on some aspect of it that are easy to get lost in translation.&lt;/p&gt;
&lt;p&gt;More info:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I have been planning to start this series for several months now; since I’m an R user in a Python-dominated team and often use both languages om any given day. What finally pushed me to overcome my laziness was an initiative by the ever prolific &lt;a href=&#34;https://twitter.com/BenjaminWolfe/status/1320770728028000256&#34;&gt;Benjamin Wolfe&lt;/a&gt; to gather resources for R users interested in Python. If you’re interested to contribute too, just drop by in the slack and say Hi.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next up: most likely &lt;a href=&#34;https://tidyr.tidyverse.org/&#34;&gt;tidyr&lt;/a&gt; and/or &lt;a href=&#34;https://stringr.tidyverse.org/&#34;&gt;stringr&lt;/a&gt; vs pandas. Watch this space.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The best of both worlds: R meets Python via reticulate</title>
      <link>https://heads0rtai1s.github.io/2019/10/03/reticulate-intro/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2019/10/03/reticulate-intro/</guid>
      <description>
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;As far as rivalries go, R vs Python can almost reach the levels of the glory days of Barca vs Madrid, Stones vs Beatles, or Sega vs Nintendo. Almost. Just dare to venture onto Twitter asking which language is best for data science to witness two tightly entrenched camps.&lt;/em&gt; Or at least that’s what seemingly hundreds of Medium articles would like you believe. In reality, beyond some good-natured and occasionally entertaining joshing, the whole debate is rather silly. Because the question itself is wrong. It’s the whole &lt;em&gt;“My kung fu is better than your kung fu”&lt;/em&gt; mindset that completely misses the point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Because what matters the most is choosing the best tool for the specific job.&lt;/strong&gt; Data challenges can be so diverse that no single language could possibly be best suited to solve them all. It’s like the &lt;a href=&#34;https://en.wikipedia.org/wiki/No_free_lunch_theorem&#34;&gt;no-free-lunch theorem&lt;/a&gt;, only for the tools that build those lunch tools. &lt;em&gt;Which makes it the no-free-kitchen theorem, I suppose … . I shall be working on this analogy.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I argue that data analysis needs to be problem-centric and language-agnostic to tap into its full potential.&lt;/strong&gt; Use whatever language gives you the best equipment to solve your problem. This also prevents you from only having a hammer and treating every problem like a nail. One recent development toward a problem-centric analysis style is the fantastic R package &lt;a href=&#34;https://rstudio.github.io/reticulate/&#34;&gt;reticulate&lt;/a&gt;. This package allows you to mix R and Python code in your data analysis, and to freely pass data between the two languages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The topic of this blog post will be an introductory example on how to use reticulate.&lt;/strong&gt; We will approach a simple supervised classification problem by first exploring the data with &lt;a href=&#34;https://cran.r-project.org/web/packages/ggplot2/index.html&#34;&gt;ggplot2&lt;/a&gt; plots, then turn to Python’s &lt;a href=&#34;https://scikit-learn.org/stable/&#34;&gt;scikit-learn&lt;/a&gt; for modelling, and finally visualise the results again in R.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note: you need at least RStudio version 1.2 to be able to pass objects between R and Python.&lt;/strong&gt; In addition, as always, here are the required packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs &amp;lt;- c(&amp;#39;dplyr&amp;#39;, &amp;#39;tidyr&amp;#39;, &amp;#39;stringr&amp;#39;,  # wrangling
          &amp;#39;knitr&amp;#39;,&amp;#39;kableExtra&amp;#39;,         # table styling
          &amp;#39;ggplot2&amp;#39;,&amp;#39;gridExtra&amp;#39;,        # plots
          &amp;#39;viridis&amp;#39;,                    # visuals styling
          &amp;#39;reticulate&amp;#39;)                 # e pluribus unum
invisible(lapply(libs, library, character.only = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll be using the famous &lt;a href=&#34;https://en.wikipedia.org/wiki/Iris_flower_data_set&#34;&gt;iris dataset&lt;/a&gt;, which is included in R as part of the &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html&#34;&gt;datasets&lt;/a&gt; package. Arguably the Hello World of supervised classification problems, this data describes the length and widths of sepals and petals from 3 different species of iris flower. Sepals are the green parts of a flower that first protect and then support the petals. Just in case you too were wondering that. Here are the first couple rows of the data:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Sepal.Length
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Sepal.Width
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Petal.Length
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Petal.Width
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Species
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is a small dataset with 50 instances each per species of iris flower:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris %&amp;gt;% 
  count(Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   Species        n
##   &amp;lt;fct&amp;gt;      &amp;lt;int&amp;gt;
## 1 setosa        50
## 2 versicolor    50
## 3 virginica     50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a simple example for exploratory data analysis plots we will look at the differences between those 3 species in terms of petal and sepal dimensions. Here, the &lt;a href=&#34;https://cran.r-project.org/web/packages/gridExtra/index.html&#34;&gt;gridExtra&lt;/a&gt; package provides the side-by-side layout:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- iris %&amp;gt;%  
  ggplot(aes(Petal.Length, Petal.Width, color = Species)) +
  geom_point(size = 4) +
  labs(x = &amp;quot;Petal Length&amp;quot;, y = &amp;quot;Petal Width&amp;quot;) +
  scale_color_viridis(discrete = TRUE, option = &amp;quot;viridis&amp;quot;) +
  theme_bw() +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  ggtitle(&amp;quot;Differences in Iris Species&amp;quot;,
          subtitle = str_c(&amp;quot;Petal and Sepal dimensions vary&amp;quot;,
                           &amp;quot;\n&amp;quot;,
                           &amp;quot;significantly between species&amp;quot;))

p2 &amp;lt;- iris %&amp;gt;% 
  ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) +
  geom_point(size = 4) +
  labs(x = &amp;quot;Sepal Length&amp;quot;, y = &amp;quot;Sepal Width&amp;quot;) +
  scale_color_viridis(discrete = TRUE, option = &amp;quot;viridis&amp;quot;) +
  theme_bw() +
  theme(legend.position = &amp;quot;top&amp;quot;)

grid.arrange(p1, p2, layout_matrix = rbind(c(1,2)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2019-10-03-reticulate-intro_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We find that there are clear clusters for each of the species - especially for setosa and in the petal dimensions. A well-trained classifier should be able to distinguish the three iris species. Now, R is perfectly capable of performing this classification task, but for the sake of the excercise we will turn to Python. Given the popularity of both &lt;code&gt;ggplot2&lt;/code&gt; and &lt;code&gt;scikit-learn&lt;/code&gt;, such a workflow is certainly realistic.&lt;/p&gt;
&lt;p&gt;First, we need to tell R where Python can be found. In &lt;code&gt;reticulate&lt;/code&gt;, the &lt;code&gt;use_python&lt;/code&gt; convenience function takes care of that; all we need is a path to the executable. On a Unix-based system, simply open a terminal and type &lt;code&gt;which python&lt;/code&gt;, then paste the resulting path below. (Or look for &lt;code&gt;python3&lt;/code&gt; instead, but this should really become your default version because for Python 2 the &lt;a href=&#34;https://pythonclock.org&#34;&gt;time is running out&lt;/a&gt;). This is my path:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_python(&amp;quot;/usr/bin/python&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you have the combined power of both R and Python at our fingertips. Use it wisely. In Rmarkdown, you can switch each invidual code chunk to the new language by putting &lt;code&gt;{python}&lt;/code&gt; instead of &lt;code&gt;{r}&lt;/code&gt; into the chunk header.&lt;/p&gt;
&lt;p&gt;So, what’s the easiest way to find out that you’re in Python? You suddenly find yourself starting to count from zero:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;foo = [1, 2, 3]
print(foo[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The real advantage, however, is that we can now pass objects from R to Python, and vice versa. To use R objects in Python we access them using the &lt;code&gt;r&lt;/code&gt; object and Python’s &lt;code&gt;.&lt;/code&gt; (dot) notation. For instance, our &lt;code&gt;iris&lt;/code&gt; dataset will be represented by &lt;code&gt;r.iris&lt;/code&gt;, which is a &lt;code&gt;pandas&lt;/code&gt; data frame:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(r.iris.loc[:5, [&amp;quot;Sepal.Length&amp;quot;, &amp;quot;Species&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Sepal.Length Species
## 0           5.1  setosa
## 1           4.9  setosa
## 2           4.7  setosa
## 3           4.6  setosa
## 4           5.0  setosa
## 5           5.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s prepare a simple &lt;code&gt;scikit-learn&lt;/code&gt; &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html&#34;&gt;decision tree classifier&lt;/a&gt;. First, we import the necessary Python libraries:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we split our iris dataset into train vs test samples using the &lt;code&gt;train_test_split&lt;/code&gt; convenience method. Of course, in real life you want to do the train/test split before looking at the data. For the sake of clarity, we choose to explicitely separate out the predictor features vs the species labels:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train, test = train_test_split(r.iris,
                test_size = 0.4, random_state = 4321)

X = train.drop(&amp;#39;Species&amp;#39;, axis = 1)
y = train.loc[:, &amp;#39;Species&amp;#39;].values
X_test = test.drop(&amp;#39;Species&amp;#39;, axis = 1)
y_test = test.loc[:, &amp;#39;Species&amp;#39;].values&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those are now Python objects. In order to see and handle them in R you have to use the &lt;code&gt;py$&lt;/code&gt; object. This is the equivalent of the &lt;code&gt;r.&lt;/code&gt; object for working with R variables in Python. For example, because &lt;code&gt;X&lt;/code&gt; is a Python object this R code doesn’t work:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X %&amp;gt;% head(5)  # doesn&amp;#39;t work&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But this R code does the trick:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;py$X %&amp;gt;% head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Sepal.Length Sepal.Width Petal.Length Petal.Width
## 41          4.5         2.3          1.3         0.3
## 16          5.4         3.9          1.3         0.4
## 26          5.0         3.4          1.6         0.4
## 99          5.7         2.8          4.1         1.3
## 5           5.4         3.9          1.7         0.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s switch back to Python code. We wil fit a simple decision tree with &lt;code&gt;sklearn&lt;/code&gt;, apply it to the test set, and visualise the results in R.&lt;/p&gt;
&lt;p&gt;First the fit and prediction. One major advantage of &lt;code&gt;sklearn&lt;/code&gt; is its intuitive and consistent syntax:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tree = DecisionTreeClassifier(random_state=4321)
clf = tree.fit(X, y)
pred = clf.predict(X_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we bring the test predictions back to R and plot some results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;foo &amp;lt;- py$test %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  rename(truth = Species) %&amp;gt;% 
  mutate(predicted = as.factor(py$pred),
         correct = (truth == predicted))

foo %&amp;gt;% 
  head(4) %&amp;gt;% 
  select(-Petal.Length, -Petal.Width) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Sepal.Length
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Sepal.Width
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
truth
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
predicted
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
correct
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
versicolor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
virginica
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
virginica
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
virginica
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1  &amp;lt;- foo %&amp;gt;% 
  select(-correct) %&amp;gt;% 
  gather(truth, predicted, key = type, value = species) %&amp;gt;% 
  ggplot(aes(Petal.Length, Petal.Width, color = species)) +
  geom_point(data = foo %&amp;gt;% filter(correct == FALSE),
             col = &amp;quot;black&amp;quot;, size = 5) +
  geom_point(size = 2) +
  scale_color_viridis(discrete = TRUE, option = &amp;quot;viridis&amp;quot;) +
  theme_bw() +
  theme(legend.position = &amp;quot;none&amp;quot;,
        text = element_text(size = 16)) +
  facet_wrap(~ type)

p2 &amp;lt;- foo %&amp;gt;% 
  select(-correct) %&amp;gt;% 
  gather(truth, predicted, key = type, value = species) %&amp;gt;% 
  ggplot(aes(Sepal.Length, Sepal.Width, color = species)) +
  geom_point(data = foo %&amp;gt;%
               filter(correct == FALSE),
             col = &amp;quot;black&amp;quot;, size = 5) +
  geom_point(size = 2) +
  scale_color_viridis(discrete = TRUE, option = &amp;quot;viridis&amp;quot;,
              guide = guide_legend(direction = &amp;quot;vertical&amp;quot;)) +
  labs(color = &amp;quot;Species&amp;quot;) +
  theme_bw() +
  theme(legend.position = &amp;quot;bottom&amp;quot;,
        text = element_text(size = 16)) +
  facet_wrap(~ type)

p3 &amp;lt;- foo %&amp;gt;% 
  count(truth, predicted) %&amp;gt;%
  complete(truth, predicted, fill = list(n = 0)) %&amp;gt;% 
  group_by(truth) %&amp;gt;% 
  add_tally(n, name = &amp;quot;true&amp;quot;) %&amp;gt;% 
  mutate(accuracy = n/true * 100) %&amp;gt;% 
  ggplot(aes(truth, predicted, fill = accuracy, label = n)) +
  geom_tile() +
  geom_text(size = 5, color = &amp;quot;grey60&amp;quot;) +
  labs(x = &amp;quot;True Species&amp;quot;, y = &amp;quot;Predicted Species&amp;quot;,
       fill = &amp;quot;Accuracy[%]&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;bottom&amp;quot;,
        text = element_text(size = 16),
        axis.text.x = element_text(
          angle=45, hjust=1, vjust=1.1),
        axis.text.y = element_text(
          angle = 45)) +
  scale_fill_viridis(option = &amp;quot;viridis&amp;quot;) +
  ggtitle(&amp;quot;Classification\nDiagnostics&amp;quot;,
          subtitle = str_c(&amp;quot;Left: confusion matrix&amp;quot;,
            &amp;quot;\n&amp;quot;,
            &amp;quot;Right: misclassified\ninstances&amp;quot;))

grid.arrange(p1, p2, p3,
        layout_matrix = cbind(c(3), c(rep(1,2), rep(2,3))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2019-10-03-reticulate-intro_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot layout provides diagnostics for the performance of the classifier:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;On the left, there is a confusion matrix which shows how many test instances of each species were classified as one of the 3 species. The numbers are absolute numbers (remember that this is a small dataset) and the colours encode percentages. For instance, 100% of the 19 setosa instances were correctly classified as setosa. This is the classification accuracy, i.e. the number of true positives. The accuracies for the other two species are pretty high, too; with iris virginica having the lowest proportion of 20 out of 24 instances correctly classified.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the right we show two sets of scatter plots that repeat the overview of petal (top) and sepal (bottom) properties from above. The difference is that now we (i) look at the test set only and (ii) plot the true classes on the right and the predicted classes on the left. The colour-coding is the same for both scatter plots (see legend at the bottom). In addition, all the misclassified instances have a black circle around them to highlight their position.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All in all, our simple classifier does a decent job. The setosas are clearly separated from the rest. And disentangling versicolor vs virginica is not trivial. Of course the performance could be improved, but this is not the topic of this post.&lt;/p&gt;
&lt;p&gt;Because more importantly we saw how the &lt;code&gt;reticulate&lt;/code&gt; approach allows us to seamlessly blend together R and Python code to use the combined power of both worlds.&lt;/p&gt;
&lt;p&gt;So, the next time somebody asks you “Python or R?” just reply with a simple “Yes.” (#inclusiveor).&lt;/p&gt;
&lt;p&gt;More resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For running R code in a Jupyter notebook with Python kernel there is the great &lt;a href=&#34;https://rpy2.bitbucket.io&#34;&gt;rpy2 library&lt;/a&gt; combined with Jupyter’s &lt;a href=&#34;https://ipython.readthedocs.io/en/stable/interactive/magics.html&#34;&gt;line or cell magic&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In R, decision trees are implemented via the &lt;a href=&#34;https://cran.r-project.org/web/packages/rpart/index.html&#34;&gt;rpart package&lt;/a&gt;. For general machine learning infrastructure there are the popular &lt;a href=&#34;https://cran.r-project.org/web/packages/caret/index.html&#34;&gt;caret&lt;/a&gt; and the new &lt;a href=&#34;https://cran.r-project.org/web/packages/tidymodels/index.html&#34;&gt;tidymodels&lt;/a&gt;; both led by developer &lt;a href=&#34;https://github.com/topepo&#34;&gt;Max Kuhn&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For creating visualisations in Python I recommend &lt;a href=&#34;https://seaborn.pydata.org&#34;&gt;seaborn&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
