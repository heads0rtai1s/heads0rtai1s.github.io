<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on head spin - the Heads or Tails blog</title>
    <link>https://heads0rtai1s.github.io/post/</link>
    <description>Recent content in Posts on head spin - the Heads or Tails blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Jan 2022 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://heads0rtai1s.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>EDA guidelines &amp; live coding experience</title>
      <link>https://heads0rtai1s.github.io/2022/01/27/eda-guide-livecoding/</link>
      <pubDate>Thu, 27 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2022/01/27/eda-guide-livecoding/</guid>
      <description>
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Last week I was spontaneously invited to contribute a live-coding exploratory data analysis (EDA) to a recently launched Kaggle community competition. I accepted the invite, and 24h later we were live-streaming my first encounter with the data on Youtube. Here are my reflections on this experience and on my EDA process in general.&lt;/p&gt;
&lt;div id=&#34;the-context&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The context&lt;/h3&gt;
&lt;p&gt;The Kaggle platform, my favourite machine learning (ML) and data science (DS) community, &lt;a href=&#34;https://www.kaggle.com/product-feedback/294337&#34;&gt;recently made it much easier for Kagglers to host their own competitions&lt;/a&gt;. The &lt;a href=&#34;https://www.kaggle.com/c/song-popularity-prediction&#34;&gt;Song Popularity Prediction competion&lt;/a&gt; was launched by &lt;a href=&#34;https://www.kaggle.com/abhishek&#34;&gt;Abhishek Thakur&lt;/a&gt; - one of Kaggle’s most prominent Grandmasters. I already had the chance a little while ago to &lt;a href=&#34;https://www.youtube.com/watch?v=kI4yxAL2wtM&#34;&gt;chat with Abhishek about EDA&lt;/a&gt; on his popular &lt;a href=&#34;https://www.youtube.com/channel/UCBPRJjIWfyNG4X-CRbnv78A&#34;&gt;Youtube channel&lt;/a&gt;. Perhaps as a result of that I got a message from him asking if I’d be interested in showcasing my EDA approach for this new competition. After a brief negotiation with my calendar, I was happy to say yes. And we set a time barely 24 hours later, so that the EDA could be useful for people within the first days of the competition launch.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;wuhuuuu 🎉 Martin Henze (aka Heads or Tails) has kindly agreed to do a live EDA for the first competition in Applied ML Competition Series. Tomorrow, 5 PM CET! Link to join: &lt;a href=&#34;https://t.co/Ra7QyyoGFh&#34;&gt;https://t.co/Ra7QyyoGFh&lt;/a&gt;&lt;br&gt;Competition Link: &lt;a href=&#34;https://t.co/AoNEGU7Id2&#34;&gt;https://t.co/AoNEGU7Id2&lt;/a&gt; &lt;a href=&#34;https://t.co/7pHttHaCTO&#34;&gt;pic.twitter.com/7pHttHaCTO&lt;/a&gt;&lt;/p&gt;&amp;mdash; abhishek (@abhi1thakur) &lt;a href=&#34;https://twitter.com/abhi1thakur/status/1483490638884659206?ref_src=twsrc%5Etfw&#34;&gt;January 18, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;While I was able to fit this spontaneous event into my calendar for the next day, there was no time for preparations. But that was fine for me. If it is a live-stream anyway, so were my thoughts, then let’s make it as authentic as possible. I would see the data for the first time and wrangle and plot it in real time. I knew that the data was tabular. I only downloaded the data in advance and set up a very basic Rmarkdown template, so that we wouldn’t waste time with library calls for &lt;code&gt;dplyr&lt;/code&gt; or &lt;code&gt;ggplot&lt;/code&gt;. And I quickly checked that I could read in the data without issue. That was it. Everything else would happen live.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-eda-process-guidelines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;My EDA process &amp;amp; guidelines&lt;/h3&gt;
&lt;p&gt;While I hadn’t looked at the data before the live-stream, I had a general game plan in my head that I intended to follow and to demonstrate. Over the years, I have built up a process for approaching new datasets that has served me well. It works best for tabular datasets, but can be adapted well for text data and, to a certain extend, also to image data.&lt;/p&gt;
&lt;p&gt;Here I want to briefly summarise the main steps of this process, which can be used as general guidelines to structure your analysis around. For a specific implementation of those steps, you can check the &lt;a href=&#34;https://www.kaggle.com/headsortails/song-popularity-eda-live-coding-fun&#34;&gt;Kaggle Notebook&lt;/a&gt; that was the result of the live EDA, or any or one of my other &lt;a href=&#34;https://www.kaggle.com/headsortails&#34;&gt;EDA Notebooks on Kaggle&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Understand the data context:&lt;/strong&gt; On Kaggle this means that you should carefully read the competition pages as they outline the overview, datasets, and evaluation metric of the challenge. In an industry setting this translates to knowing where the data comes from, who collected it, and which limitations and quirks those people have identified. No dataset is perfect.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Look at your data:&lt;/strong&gt; Once I start analysing the data itself, my first step is to simply look at it in its tabular form and see what kind of values the different features take. Here you can learn about data types, missing values, and simple summary statistics. I’m using the R tools &lt;code&gt;glimpse&lt;/code&gt; and &lt;code&gt;summary&lt;/code&gt; for some simple transformations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Always plot your data:&lt;/strong&gt; You can learn a lot about your dataset by finding the best way to visualise each feature. For huge numbers of columns grab a subset. But you want to at least get an impression of what your features look like. You will quickly detect skewness, outliers, imbalances, or otherwise troubling characteristics. I almost always start with individual plots for predictors and for the target to establish the foundations of the analysis. For continuous variables I start with density plots and for categorical ones with barplots. You can always change styles later, but those plots give you a solid first impression.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gradually build up complexity:&lt;/strong&gt; After learning about the individual features and their distributions, you can go a step further to see how they interact. A correlation matrix or a pairplot can get you started, but it is also worth it to look at each predictor feature’s interaction with the target in the way that is most revealing for that particular combination. For instance a density plot with two overlapping colors for a binary classification target. From there you can go into even higher-dimensional visuals to see how 2, 3, 4 features interact with each other and the target at the same time. I’m a big fan of facet plots to quickly and cleanly add another categorical dimension to your visuals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Investigate your data from different angles:&lt;/strong&gt; For many of the higher-dimensional plots you need to do some data wrangling to unlock their full potential. I’m thinking here primarily of reshaping the data, e.g. via &lt;code&gt;pivot_longer&lt;/code&gt; or &lt;code&gt;pivot_wider&lt;/code&gt; in &lt;code&gt;tidyr&lt;/code&gt;. Pivoting can give you new facet variable for a different perspective on a dataset. If you have more than one dataset then this is also a great time for joins of various kinds.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Document your thoughts and ideas:&lt;/strong&gt; Visuals are great, and it’s easy to get caught up in a journey of visual exploration. But don’t forget to pause every now and then to write down your thoughts and insights. Often this allows you to get a clearer idea of where your analysis is going, and maybe new inspiration. You’re also making your analysis much more accessible; not only to your readers but also to your future self. Some insights that seem obvious in the moment might be hard to recount even a day later. And when it comes to data science a very important but often overlooked aspect is communication. You want to be able to explain your findings and thinking to other people. And the better your able to do that, the higher the chance that you have a good understanding of the data yourself.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;live-coding-experience&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Live-coding experience&lt;/h3&gt;
&lt;p&gt;Those were the guidelines that I took with me into this live-stream. And a little bit of nervousness, although the less-than 24 hours were luckily not enough to build up a lot of doubt in my mind. But I had never done a live-coding session before in my life. I didn’t expect to freeze up, but there were quite a few things that could go wrong. To add to the challenge, I would be using new headphones, a relatively new microphone, and a video platform I hadn’t used before.&lt;/p&gt;
&lt;p&gt;But Abhishek had all the technical details covered in a relaxed way. My setup was quickly verified, and we chatted a bit before launching into the stream. I was using my Ubuntu laptop and had set up a virtual desktop that only contained my Rstudio session and a browser with the competition pages, so that I could easily switch between the two and go from context to coding. I even remembered to briefly recap my main slide from the previous EDA chat as a ways of introduction, before talking about the competition itself.&lt;/p&gt;
&lt;p&gt;The live-coding turned out to be a very fun experience and you can &lt;a href=&#34;https://www.youtube.com/watch?v=JXF-7rCcR1c&#34;&gt;watch the first session here&lt;/a&gt;. And yes, I’m writing &lt;em&gt;first&lt;/em&gt; session, because the interest and engagement of the participants motivated us to stream a &lt;a href=&#34;https://www.youtube.com/watch?v=2aE6SvCVOis&#34;&gt;second session 2 days later&lt;/a&gt; where we continued the analysis into multi-feature relationships. The &lt;a href=&#34;https://www.kaggle.com/headsortails/song-popularity-eda-live-coding-fun&#34;&gt;resulting Kaggle Notebook&lt;/a&gt; was primarily built during those sessions. I added the narration and some polish in between and afterwards; but most of the insights were revealed in the sessions.&lt;/p&gt;
&lt;p&gt;Given that I hadn’t done any preparation on this particular dataset, the streams went rather remarkably well in retrospect. This is particularly true for the first session, during which I looked at the data for the first time. Sure, there were plenty of mistakes and R even crashed on me once (which is a pretty rare thing in general). But I managed to follow my guidelines and to analyse the data in sufficient detail to get people started on it. We even had time for questions and recommendations.&lt;/p&gt;
&lt;p&gt;During the 2nd session I put a lot of emphasis on the pivoting and multi-variate visuals; especially using facet plots. Maybe it was too much emphasis, and other aspects didn’t get enough airtime. Parts of that 2nd stream might have gotten rather technical. But it was an important subject to me and I wanted to explain and show it in detail. I think this worked, and I hope that those strategies were useful to other people.&lt;/p&gt;
&lt;p&gt;One thing I didn’t figure out how to do properly was to watch the stream at the same time. Since I was sharing my screen I couldn’t just tab to the video and back. During the first session, I had another laptop running with the Youtube stream at the same time. But I found it too distracting and soon ended up focussing on my main screen only for the coding. This meant that I couldn’t read any of the live session questions and comments, and had to leave the question asking to Abhishek. Which he did great, of course. But I feel there should be a better way. Maybe if there’s a next time I’ll try a different multi-monitor setup.&lt;/p&gt;
&lt;p&gt;And I would certainly do another live-coding session again in the future. Maybe with a bit more preparation to make things run a little smoother; although this might take away much of the authenticity. We’ll see. Apparently these things can happen pretty spontaneously ;-)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Yearly goals: 2022 targets</title>
      <link>https://heads0rtai1s.github.io/2022/01/20/goals-2022-targets/</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2022/01/20/goals-2022-targets/</guid>
      <description>


&lt;p&gt;This is the second part in a series of posts about setting and tracking my yearly goals when it comes to Machine Learning &amp;amp; Data Science (ML &amp;amp; DS) as well as Sports &amp;amp; Exercise. In the &lt;a href=&#34;https://heads0rtai1s.github.io/2022/01/13/goals-2021-results/&#34;&gt;first post&lt;/a&gt; I wrote about my past goals for 2021 and the lessons learned from those. This post will focus on my goals and expectations for 2022, and how last year’s experiences shaped and informed them. This might be more fun to read if you’ve seen the &lt;del&gt;comedy show&lt;/del&gt; documentary Silicon Valley.&lt;/p&gt;
&lt;div id=&#34;stretch-to-avoid-injuries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stretch to avoid injuries&lt;/h3&gt;
&lt;p&gt;Here’s the summary tweet with my stretch goals from the 1st of January:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My 2022 stretch goals:&lt;br&gt;&lt;br&gt;- Spend 400 h learning ML/DS&lt;br&gt;&lt;br&gt;- Join 2 NLP + 2 Image competitions on &lt;a href=&#34;https://twitter.com/kaggle?ref_src=twsrc%5Etfw&#34;&gt;@kaggle&lt;/a&gt;&lt;br&gt;&lt;br&gt;- Team up in 3 competitions&lt;br&gt;&lt;br&gt;- Win 2 Kaggle comp medals&lt;br&gt;&lt;br&gt;- Write 50 blog posts&lt;br&gt;&lt;br&gt;- Run a sub-4h marathon&lt;br&gt;&lt;br&gt;- Do 1 muscle up&lt;br&gt;&lt;br&gt;- Sleep at least 8 h/night with a std dev &amp;lt;= 0.5 h&lt;/p&gt;&amp;mdash; Martin Henze (Heads or Tails) (@heads0rtai1s) &lt;a href=&#34;https://twitter.com/heads0rtai1s/status/1477360618432778242?ref_src=twsrc%5Etfw&#34;&gt;January 1, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;The idea of those being &lt;em&gt;stretch&lt;/em&gt; goals is that I’m not expecting to reach all of them. And that would be fine. Because the important aspect is the &lt;strong&gt;journey towards those goals&lt;/strong&gt;, and the skills and experiences that I hope this journey will give me.&lt;/p&gt;
&lt;p&gt;The goals themselves are still important, and I will do my best in reaching them. Some will go better than others. Trying to become better at something can be a vague target, and concrete goals can be very useful in providing a metric to measure progress and success. Similar to ML, finding a good metric can make a big difference. And of course also similar to ML any metric can be gamed; but you would only be fooling yourself. Thus, when striving for those goals it is essential to remember the purpose behind them, and the vision you had when setting them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/mXuPwP0kxQqvu0M168/giphy.gif&#34; title=&#34;Maybe not that kind of vision&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spend-400-h-learning-mlds&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Spend 400 h learning ML/DS&lt;/h3&gt;
&lt;p&gt;As I wrote in the previous post, hours themselves don’t create mastery. Spending your time effectively is more important that spending lots of it with vague intentions. My aspiration here is to grow my ML/DS skills in specific directions. I realised that I’m still lacking the kind of solid understanding of many ML concepts that would allow me to use them as confidently and creatively as I would like to.&lt;/p&gt;
&lt;p&gt;Let me try to illustrate what I mean. When I encounter a certain data problem - be it on Kaggle, at work, or anywhere - then in my head I construct ideas for ways to process the data to make it easier to solve the problem. For instance, for the &lt;a href=&#34;https://www.kaggle.com/c/titanic&#34;&gt;Kaggle Titanic&lt;/a&gt; challenge I would try to come up with feature engineering steps to figure out whether someone was travelling alone or in groups, and what kind of groups those could be (family? friends?). That’s the creative part. Translating these ideas into code is where the skill comes in. For traditional (tabular) ML I’m reasonably good at this translation, but when it comes to Deep Learning (DL) my process is still slow and inefficient.&lt;/p&gt;
&lt;p&gt;Besides the tabular ML and feature engineering flow, I also know what to aim for in my skill development through the analogy to data visualisation. I’m a big fan of dataviz, and it arguably contributed a lot to my early successes in the Kaggle community. When it comes to a visualisation problem, I’m able to envision the data in my head and then find the right (ggplot2) tools to realise my imagination through code. Ideas flow almost unimpeded through the power of dataviz skills and tools. From there, my limitations (which are still plenty) mostly arise from a lack in creativity rather than from my coding skills. And this is where I want to be with ML/DL. Maybe not this year; but I want to get much closer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;join-2-nlp-2-image-competitions-on-kaggle&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Join 2 NLP + 2 Image competitions on Kaggle&lt;/h3&gt;
&lt;p&gt;One way to make my ML goals even more concrete is through &lt;a href=&#34;https://www.kaggle.com/competitions&#34;&gt;Kaggle competitions&lt;/a&gt;. Challenges throughout the year are manifold and diverse. They typically attract a few thousand people, hundreds among whom are sharing their ideas or code. I can’t think of a better way of learning than to immerse yourself into such a competition for (typically) 3 months and soak up the skills and problem solving ingenuity that others display.&lt;/p&gt;
&lt;p&gt;Specifically, I want to improve my skills in Image problems (aka Computer Vision) and Natural Language Processing (NLP). Both of those are solidly in the domain of DL these days. Image-based challenges are usually plentiful throughout the year. NLP competitions can be less frequent, but it should hopefully still be possible to encounter 2 interesting ones in 2022.&lt;/p&gt;
&lt;p&gt;When it comes to Image data, I’m reasonably happy with my progress in 2021. I started learning about &lt;a href=&#34;https://www.fast.ai&#34;&gt;FastAI&lt;/a&gt; and using it more confidently in building my own competition pipeline and tinkering with custom dataloaders in some competitions. I also started to look into the underlying basic functionality in &lt;a href=&#34;https://pytorch.org&#34;&gt;Pytorch&lt;/a&gt; and &lt;a href=&#34;https://torch.mlverse.org&#34;&gt;torch for R&lt;/a&gt; (FastAI is a high-level wrapper for Pytorch). While the way that torch works is pretty intuitive, I need to spend more time with the code itself to become more confident in using it creatively.&lt;/p&gt;
&lt;p&gt;For NLP, the &lt;a href=&#34;https://huggingface.co&#34;&gt;huggingface libraries&lt;/a&gt; have fast become the gold standard, and the ecosystem is growing and evolving rapidly. I enjoyed taking the first &lt;a href=&#34;https://huggingface.co/course/chapter1/1&#34;&gt;huggingface course&lt;/a&gt; last year, and I plan to continue with those courses to learn more. There are a few libraries that bring the huggingface tools into the FastAI ecosystem; which is an ideal combination for me at this stage. I plan to focus on those, besides the general huggingface framework, and will hopefully blog about it every now and then.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/LpR8z8GqoyktA3141E/giphy.gif&#34; title=&#34;I promise I&amp;#39;ll try my best to blog about it.&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;team-up-in-3-competitions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Team up in 3 competitions&lt;/h3&gt;
&lt;p&gt;I thoroughly enjoyed my teaming up experiences in 2021 and plan to continue this approach in 2022. There is a lot that you can learn from your team mate(s) on Kaggle, since many people come from such different professional and technical backgrounds. The goal of 3 competitions seems relatively low compared to the rest of the goals, but I’m aiming for gradual progress here. Last year it was 2 competitions.&lt;/p&gt;
&lt;p&gt;Finding the right balance in a team can be a challenge, as is the way that progress is planned and implemented. Communication is really crucial here; as in so many areas of life. This applies not necessarily to the goal of doing well in a Kaggle competition, but to getting the most out of the learning experiences that are enabled by the process of collaboration. In all of those aspects I consider myself fortunate in having teamed up with &lt;a href=&#34;https://twitter.com/YassineAlouini&#34;&gt;Yassine&lt;/a&gt;, and I’m looking forward to our future collaborations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/jsI8nBXJl6s7r7iuJ5/giphy.gif&#34; title=&#34;Tres competiciones&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;win-2-kaggle-comp-medals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Win 2 Kaggle comp medals&lt;/h3&gt;
&lt;p&gt;In 2021, I purposefully didn’t set a goal of a certain performance in a Kaggle competition. I just wanted to grow my (DL) skills by spending more time on Kaggle problems. Now, I feel like I’m at a stage where I can expect to develop those skills to the extend where my progress translates to better finishes on competition leaderboards.&lt;/p&gt;
&lt;p&gt;I’m a fan of the Kaggle medal system, and of the gamification aspect in general. I consider it an incarnation of the idea of concrete metrics replacing otherwise vague goals that I wrote about at the beginning of this post. But here there can be an even greater incentive to game the system and take the medals as a goal in themselves, rather than a reflection of your skills. Sometimes, it might be possible to win a (bronze) medal by forking a public notebook or blindly ensembling other people’s solutions without understanding them (and getting lucky in the shake-up). That’s not what I want. Those medals don’t really count for anything, since they are disconnected from your actual abilities.&lt;/p&gt;
&lt;p&gt;I want to write my own code and understand (ideally) every line of it. Inspired by other people’s ideas, yes; but incorporating those inspirations purposefully and in an informed way into my own pipelines. Plan my experiments intelligently and choose the best progress based on the results of those experiments. Those medals will mean something. Most importantly, they will mean something to me. Setting this goal and putting it out there will create a bit more pressure to succeed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/QrcujnCu5qWVyea2Xc/giphy.gif&#34; title=&#34;Maybe it was a bit too much pressure.&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-50-blog-posts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Write 50 blog posts&lt;/h3&gt;
&lt;p&gt;Blogging was an aspect of my journey that suffered in 2021. Of the 12 posts that I had planned, I wrote only 2. But instead of lowering the bar more, I decided to raise it to 50 instead. This might not make a lot of intuitive sense, since a target of 50 looks way more intimidating than a mere dozen. However, I realised that it wasn’t really the number of posts that stopped me from writing. It was the process of writing a single posts, which I saw as a significant endeavour that needed preparation and research and comprehensive content to be worth it.&lt;/p&gt;
&lt;p&gt;I can be a bit of a perfectionist, which sometimes stops me from making public something that I had written but wasn’t happy with. And I would hate to turn into the kind of spammer who just churns out cookie-cutter posts in the search for engagement and followers. You know the ones. Although I don’t expect a large audience for those posts, I want to write something of value for those who stumble across my blog. There’s enough noise on the internet already.&lt;/p&gt;
&lt;p&gt;But here’s the crux: writing valuable content doesn’t have to mean writing long and/or polished content. Short snippets of concise code or brief reflections on recent learnings can have as much value as posts that have been months in the making. I simply need to get out of my comfort zone and write more.&lt;/p&gt;
&lt;p&gt;One benefit of writing is analogous to the idea that teaching a concept to someone is a great way of learning about this concept. If you can’t explain it, then do you really understand it? And if you can’t write about it clearly and concisely, then chances are you don’t actually understand it either. But understanding something is not necessarily a binary thing, and sharing even small progress in learning can be useful to people in similar situations.&lt;/p&gt;
&lt;p&gt;And after writing only 2 posts in 2021 (albeit more technical posts), this will be already the second post of 2022.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;run-a-sub-4h-marathon&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Run a sub-4h marathon&lt;/h3&gt;
&lt;p&gt;My first ever marathon in 2021 (not a race, just the distance) clocked in at 4 hours 25 minutes. Not too bad; now let’s see whether we can get under 4 hours. Shaving off 25 minutes of 42km might not look much at the outset, but it’s about 35s per km, which here is the difference between a 5:45 pace (4h-goal) and a 6:20 pace (last year), which is pretty noticeable. (Being European, I measure my pace in min/km.)&lt;/p&gt;
&lt;p&gt;My pace for shorter runs is significantly faster than that; and I’ve run a personal best 1h 40min half-marathon (i.e. 4:45 pace). However, a marathon becomes very different after the 2h mark, and most certainly during the final 10km. I will probably still need to take short stretches of walking with water and energy gels, which need to be factored into the overall running pace. I feel like being at 5:00 pace for the first half and then aiming for 5:30 - 6:30 pace for the remainder sounds realistic. In contrast, last year I dropped to 7:30 pace for the last 12km, albeit having done quite well until km 25. My plan is to work more on the stages between km 20 and 30 with long training runs of 2h to 3h durations during most weekends. Should be fun!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/cnipK4uGHhudr6tels/giphy.gif&#34; title=&#34;We might have different definitions of &amp;#39;fun&amp;#39;.&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;do-1-muscle-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Do 1 muscle up&lt;/h3&gt;
&lt;p&gt;This is a repetition of the 2021 goal, since I didn’t accomplish anything in that direction. For 2022, I will have to build a better plan to make at least some progress. I’m pretty sure that a muscle up is mostly a question of technique, given a certain level of strength. None of that silly cross-fit-style swingy stuff though; I’m talking about proper technique. There are different stages of the muscle up movement, and I hope to be able to train some of those in isolation.&lt;/p&gt;
&lt;p&gt;Instead of trying to build out the strength first (and my pull up continuity in 2021 wasn’t too bad), this year I will invest more time to research the technique and put together a plan to get me progressively closer to my goal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sleep-at-least-8-hnight-with-a-std-dev-0.5-h&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sleep at least 8 h/night with a std dev &amp;lt;= 0.5 h&lt;/h3&gt;
&lt;p&gt;This last goal might sound weirdly specific, but it’s all about continuity. As a student, then PhD student, then astronomer (of all things!) it’s easy to slip into weird sleeping habits. Even if you don’t actually have to spend your nights at a telescope. Working at a startup might sound like one of those “out of the frying pan and into the fire” type situations when it comes to being sleep deprived, but I’m happy to tell you that there are startups who recognise the value of a healthy work-life balance. Since more and more research is highlighting the importance of a good night’s sleep, I identified this goal as foundational for all the other goals in this post.&lt;/p&gt;
&lt;p&gt;It’s honestly also something that you should listen to your body for. From time to time it might be necessary to work late to meet a deadline, but the thought that your brain won’t be impacted by only 4h of shut-eye is pretty untenable. You pay for burning the midnight oil with reduced cognitive spark over the following days, so your average productivity still goes down even though you worked all those extra hours. And you might be able to get away with it for a while in your 20s, since that’s pretty much the kind of bad decisions that your 20s are for, but it’s still gonna catch up with you eventually. Anyway, no lecturing intended here. Sleep is super important, and I suggest to use it smartly to get where you want to get.&lt;/p&gt;
&lt;p&gt;So what about that goal? Well, last year I managed to exceed my planned average of 8h / night. But you can famously drown in a river that’s only 30cm deep on average. Variance matters, and there was too much of that in my sleep patterns. Once more, I was looking for a concrete goal to improve my habits. A standard deviation of less than 30 mins means that in 67% of nights I would be less than +/- 15 min away from my average goal. That sounds broadly doable. Last year’s standard deviation was 1h.&lt;/p&gt;
&lt;p&gt;I’m not suggesting to run around with a stop watch either, or try to plan your day down to the minute. That’s a recipe for a neurotic type of disaster worthy of a silly and slightly problematic 80s comedy. My approach is to pay a bit more attention to guide my habits towards being more sustainable and then let routine take over. You won’t always able to stick to those habits every single day, but if you’re generally maintaining them that’s already a big improvement. So don’t sleep on sleep.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/Qw4X3FJbFEXeglZ7s6A/giphy.gif&#34; title=&#34;Especially when travelling long distances.&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Now you know some of my plans for 2022. I’d be happy to read about yours, in whichever shape or form you’d like to share them.&lt;/p&gt;
&lt;p&gt;You might have noticed a pattern in most of my goals, and that is something I will write about more in detail in a future post: my overarching aim is for my 2022 self to be better than my 2021 self. Comparing yourself to other people can be a futile and demoralising exercise, especially if you, like me, tend to look to the most high-achieving people in your field or community for inspiration. Even more so in this age of social media highlight reels. In contrast, I believe that comparing yourself to a previous baseline of yourself can be a great way to learn from past mistakes and experiences. Look at what worked, slightly tweak those things that didn’t work to see if that improves the situation.&lt;/p&gt;
&lt;p&gt;It’s the basis for ML experiments, which comes from the basis of any science experiments, which comes from the scientific method, which comes from an evidence-based approach to trying to understand this often perplexing world in which we find ourselves. Reflecting on your choices and their consequences is a valuable tool in any context. Or as Socrates used to say: “The unexamined life is not worth living”.&lt;/p&gt;
&lt;p&gt;In the third and final part of this goals series I will write about measuring my goals and (finally!) bring in some code and visualisations. Stay tuned!&lt;/p&gt;
&lt;p&gt;This is the end of this post.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/mXuPwxly5V2xL1uAow/giphy.gif&#34; title=&#34;Go and be awesome!&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Yearly goals: 2021 experiences</title>
      <link>https://heads0rtai1s.github.io/2022/01/13/goals-2021-results/</link>
      <pubDate>Thu, 13 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2022/01/13/goals-2021-results/</guid>
      <description>


&lt;p&gt;At the beginning of last year, I got the motivation from other people’s Twitter posts to set and track my own goals for what would surely be the year we would emerge from the shadows of the pandemic. With an ever so slightly more jaded outlook to life, I’m doing the same this year. On a global level, many societies didn’t seem to have learned an aweful lot from the Covid years of 2020 and 2021. From a personal perspective, I hope that the experience of setting and sharing my goals has helped me to define better goals for 2022. This post is a brief retrospective on the 2021 part of my journey, and the lessons I hope to have learned for the road ahead.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://thumbs.gfycat.com/NauticalPastLadybird-size_restricted.gif&#34; title=&#34;Carry on ...&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;how-did-your-year-go&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How did your year go?&lt;/h3&gt;
&lt;p&gt;Those were my 2021 stretch goals and the tally I drew at the end of the year:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Final tally, not too bad:&lt;br&gt;&lt;br&gt;✅ Spend 500 h on &lt;a href=&#34;https://twitter.com/kaggle?ref_src=twsrc%5Etfw&#34;&gt;@Kaggle&lt;/a&gt;&lt;br&gt;&lt;br&gt;✅ Join 4 competitions and team up in 2 of them&lt;br&gt;&lt;br&gt;✅ Publish 52 episodes of &lt;a href=&#34;https://twitter.com/hashtag/KaggleHiddenGems?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#KaggleHiddenGems&lt;/a&gt;&lt;br&gt;&lt;br&gt;❌ Write 12 blog posts&lt;br&gt;&lt;br&gt;✅ Run my 1st marathon&lt;br&gt;&lt;br&gt;❌ Do 1 muscle up&lt;br&gt;&lt;br&gt;✅ Sleep 8 h/night&lt;/p&gt;&amp;mdash; Martin Henze (Heads or Tails) (@heads0rtai1s) &lt;a href=&#34;https://twitter.com/heads0rtai1s/status/1476293217779920901?ref_src=twsrc%5Etfw&#34;&gt;December 29, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Given that 2021 was year two of a global plague, it might be fair to add ‘having survived all that’ to the list of accomplishments. But I’m aware (and grateful) of my priviledged situation of being able to work from home on my computer, so let’s not cheapen the accomplishments of those who actually were out there working hard and who managed to stave off (or overcome) serious illness. A bit of perspective is necessary, from time to time.&lt;/p&gt;
&lt;p&gt;At the same time, I think it is important to highlight the degree of structure and control that can be provided by setting goals and trying to reach them; however much arbitrary or insignificant those goals might appear in the bigger picture. I believe that especially in uncertain times there is a lot of comfort in wrestling a modicum of control from the chaotic universe. As long as we do the best we can do to our abilities (and circumstances) then we’re doing rather well. But enough existentialism. Let’s get back to data science.&lt;/p&gt;
&lt;p&gt;On the goals themselves, I’d like to go into more detail than the Twitter thread allowed. I’d like to emphasise that those are stretch goals, and that I didn’t expect to reach all of them. Wouldn’t be fun it were too easy, after all.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spend-500-hours-on-kaggle&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Spend 500 hours on Kaggle&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; Now, hours themselves don’t create mastery; it’s about what you do during those hours that counts. A short time of focussed effort counts for more than a much longer period of distracted dabbling. Having said that: setting a specific goal that translates to 1-2 hours per day can help immensely in creating habits and budgeting time.&lt;/p&gt;
&lt;p&gt;For the latter, I would often spend about 1 hour per day on Kaggle problems during the week, and 2 or 3 hours on the weekend. Calling it a (Kaggle) day after that helped to avoid the temptation of working later at night, especially during the week. Kaggle challenges are more akin to a marathon (&lt;em&gt;foreshadowing …&lt;/em&gt;) than a sprint, which means that pacing yourself becomes important. In general, having a healthy work-life balance is necessary to avoid burn out and loss of motivation (or health).&lt;/p&gt;
&lt;p&gt;The habit-building effect of having a specific goal can hardly be overstated. Continuity is one of the main factors of successful learning and growing. If you’re spending (almost) every day on a certain project, or practicing a specific tool, then you are bound to make at least some progress in understanding. And learning is what Kaggle is mostly about, for me. (And the &lt;a href=&#34;https://heads0rtai1s.github.io/2019/04/19/great-community-kaggledays19/&#34;&gt;community&lt;/a&gt;, of course.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;join-4-competition-and-team-up-in-2-of-them&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Join 4 competition and team up in 2 of them&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; This continues the previous train of thought: competiting is less about the ranks and medals, but more a motivation for learning new skills and tools. I always found it most efficient to learn by doing - and to learn from examples and applications. Instead of merely reading books of theory, or even code, I prefered to focus on the skills that I needed to solve a specific problem. And what better challenge to spend 3 months at a time competing with the brightest applied-ML minds on Kaggle on a new and exiting problem.&lt;/p&gt;
&lt;p&gt;In 2021, I participated earnestly in 5 competitions. I also briefly dabbled in a few others; but honestly less is more in this kind of situation. You want to focus on one thing at a time, instead of scattering your attention across lots of projects and not really devoting enough time to any of them. I know, the temptation is great to jump into every new competition that launches. I’m trying myself to recover from that; so I recommend you do as I say, not do as I do. Having said that, 2021 for me was a notable improvement over previous year.&lt;/p&gt;
&lt;p&gt;A lot of the learning effect on Kaggle comes from reading other people’s contributions in form of Notebooks or Discussion posts. This kind of effect is applified by a factor of a lot when you team up with other competitors. I had only done this very occasionally in the past. The main reason was probably that I didn’t have enough confidence in my own skills to contribute meaningfully to a team. I don’t want to be carried. I want to make a significant difference. Having learnt a bit more in 2020 I decided to get outside of my comfort zone and challenge myself in a team.&lt;/p&gt;
&lt;p&gt;It turned out to be a very fun and rewarding experience. I joined forces with my Kaggle buddy &lt;a href=&#34;https://twitter.com/YassineAlouini&#34;&gt;Yassine&lt;/a&gt; on 2 imaging competitions and learned a lot from him on deep learning architectures and training strategies. On the way, I became much more confident in using the high-level &lt;a href=&#34;https://www.fast.ai/&#34;&gt;FastAI&lt;/a&gt; framework and also &lt;a href=&#34;https://colab.research.google.com&#34;&gt;Google Colab&lt;/a&gt;, which is a great cloud environment for GPU notebooks. Read my mid-2020 review on cloud GPUs &lt;a href=&#34;https://heads0rtai1s.github.io/2021/08/24/colab-plus-kaggle-cloud-gpu/&#34;&gt;right here&lt;/a&gt;. Bottom line: there are lots of great things to be found in the clouds these days.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media0.giphy.com/media/3oEdv6UTqzNk9Y5i36/giphy.gif?cid=790b76119299a8b65e0c47548fe0e8dbd2d4ca4fefe9698e&amp;amp;rid=giphy.gif&amp;amp;ct=g%20...&#34; title=&#34;Burgers not included in the Colab Pro Plus subscription ... yet&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;publish-52-episodes-of-kaggle-hidden-gems&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Publish 52 episodes of Kaggle Hidden Gems&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; The Hidden Gems series is my attempt to give back something to the Kaggle community by discovering and promoting underrated Notebooks published on the platform. The Kaggle community has grown enormously over the last years. So many bright people from many walks of life contribute to the community; which is great because you have the chance to learn from them. One difficulty, though, of this higher volume of contributions is that sometimes great content can have difficulties standing out and being recognised for its quality.&lt;/p&gt;
&lt;p&gt;Every week I pick 3 new, underrated Notebooks to highlight. Here’s the most recent example at the time of writing: &lt;a href=&#34;https://www.kaggle.com/general/300228&#34;&gt;episode number 88&lt;/a&gt;. As a result my week doesn’t feel complete now without reading cool
Kaggle Notebooks. In 2021 I succesfuly managed to establish a continuity of one episode every Tuesday, without fail. 52 episodes in 52 weeks. On the occasion of episode 50, I gathered all the Gems data into a &lt;a href=&#34;https://www.kaggle.com/headsortails/notebooks-of-the-week-hidden-gems&#34;&gt;Kaggle dataset&lt;/a&gt; and put together a &lt;a href=&#34;https://www.kaggle.com/headsortails/hidden-gems-a-collection-of-underrated-notebooks/&#34;&gt;Starter Notebook&lt;/a&gt;. This is Kaggle, after all, where even the data has data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-12-blog-posts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Write 12 blog posts&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Fail.&lt;/strong&gt; I only wrote 2 posts in total in 2021 (after writing 9 in 2019, and 5 in 2020). Establishing and keeping up a blogging habit is a question of time, certainly. But in my case, it might also be a challenge of perfectionism and preference for long posts. Like this post here, which is already shaping up to be longer than I had intended.&lt;/p&gt;
&lt;p&gt;Going out of my comfort zone when it comes to writing isn’t easy at all. I don’t expect a large audience for this blog. I’m writing mostly for myself, so that I can remember and digest some of the things that I picked up over time. At the same time, I want for anyone who stumbles across those posts to get some quality information out of reading them. And that holds me back from publishing more frequent and shorter posts. That is one lesson learnt from 2021.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;run-my-1st-marathon&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Run my 1st marathon&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; I gotta be honest: this is probably my proudest achievement. My first ever marathon. 42 km in one go. After failing a few attempts during the year, I finally got there on Dec 6th. Just in time to make it a 2021 achievement; but in time nonetheless. Running in colder weather was also more conducive to success, as opposed to my summer attempt in August. Who would have guessed.&lt;/p&gt;
&lt;p&gt;For me, exercise has always been a way to destress and detach from coding and data analysis work, which isn’t the most physically active of all occupations. As such, I didn’t come cold into this marathon challenge but have a certain level of fitness that I’ve built and maintained over the years. When the pandemic started, running outdoors was one of the few exercises that I felt comfortable doing. Having never ran more than 10 km at a time before (and that was quite a few years ago), I then gradually worked up to this marathon challenge. From 5k to 10k, and then running my first half-marathon (21 km) in 2020.&lt;/p&gt;
&lt;p&gt;I didn’t read a lot of running blogs or stuff, so some of the things I discovered for myself might have been easier to read up on, in hindsight. None of this is expert recommendation, obviously. One insight is that a full marathon is a very different beast from a half-marathon. The 21 km of the half-marathon are something that I could run without stopping or nutrition; even without water (although that might not be generally recommended). None of that worked for me for the marathon.&lt;/p&gt;
&lt;p&gt;Turns out that after 2-ish hours of continuous exercise you have burned through most of your carb reserves (i.e. glycogen). I genuinely didn’t know that. Coincidentally, my longest non-stop exercises thus far were just under the 2h limit (and years ago). So, in my first past 30-km attempt I had to stop with leg cramps and fatigue, and it took me a couple hours and a good meal to recover from that. I later found out about that glycogen limit and that I had literally ran out of energy to burn. Fun times. So for the next attempts I got some energy gels plus water with electrolytes and it made a huge difference. Almost no issues after the successful marathon. Just the fun of walking up and down stairs for the days after.&lt;/p&gt;
&lt;p&gt;In numbers: I ran a total of 1332 km in 2021, which translates to about 1/30th around the earth, in 116 hours. I also logged about 3.7 million steps, for an average of about 10k steps per day. As a data addict, I’m using a Garmin watch and app for all those measurements. No link, since they haven’t given me a sponsorship yet. Gotta keep running.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media4.giphy.com/media/9rRacglGbs68E/giphy.gif?cid=ecf05e472rkl4hg798m9nz6ikhrrr7nh9snqzkicv67yti1i&amp;amp;rid=giphy.gif&amp;amp;ct=g&#34; title=&#34;And running. And running.&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;do-1-muscle-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Do 1 muscle up&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Fail.&lt;/strong&gt; Another failure. I didn’t even get close. I did pull-ups frequently, after my shorter runs, but couldn’t get a rhythm going. Another excuse is that I lost access to a decent pull-up bar early in the year, so that I had to make do with the crossbars of small football goals or with playground structures, both of which always have bars that are at the very least slightly too thick to grab them comfortably. If they’re not weirdly shaped to begin with. I’m sure that everyone knows exactly what I’m talking about. Very frequent situation.&lt;/p&gt;
&lt;p&gt;In my eternal optimism, I had also hoped to finish the marathon by mid year. And then focus on the muscle up afterwards. As it turned out, that didn’t quite work either; what with me needing until December to run the marathon and all. So, the muscle up needed more effort due to the conditions not being as ideal, which was more than I could do at the time. Another reason for the failure was poor planning, as I didn’t look for specific muscle-up progressions that would take me there.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sleep-8-hnight&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sleep 8 h/night&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Success.&lt;/strong&gt; This was likely the most important aspect. Sleep and rest are super important. We can only burn the midnight oil for so long before it has bad consequences for our mental and physical well-being. Balance is vital. I was very happy to reach and exceed my goal.&lt;/p&gt;
&lt;p&gt;But even though I got an average 8.4h over the year, there remained a notable variance of about 1 hour. I sometimes had to make up for shorter nights with sleeping longer at other times. I’m far from being an expert on sleep science, but from what I have read I believe that consistency is needed for building and maintaining healthy sleep patterns and restful recovery. Those patterns of mine still need some work, as the variations in sleep and wake times were also larger than I would have wanted.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;I’m going to end this post here. Originally, I was planning to put 2021 results and 2022 goals in the same post, but this one is pretty long already. See the lesson above on failing to write frequent and short posts.&lt;/p&gt;
&lt;p&gt;Coming up: how I’m building on these experiences for my 2022 journey.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Colab Pro&#43; Features, Kaggling on Colab, and Cloud GPU Platforms</title>
      <link>https://heads0rtai1s.github.io/2021/08/24/colab-plus-kaggle-cloud-gpu/</link>
      <pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2021/08/24/colab-plus-kaggle-cloud-gpu/</guid>
      <description>


&lt;p&gt;In the final, hectic days of a recent &lt;a href=&#34;https://www.kaggle.com/c/seti-breakthrough-listen&#34;&gt;Kaggle competition&lt;/a&gt; I found myself in want of more GPU power. As one often does in such an occasion. My own laptop, with its &lt;a href=&#34;https://heads0rtai1s.github.io/2021/02/25/gpu-setup-r-python-ubuntu/&#34;&gt;GPU setup&lt;/a&gt;, was doing a fine job with various small models and small images, but scaling up both aspects became necessary to climb the leaderboard further. My first choice options GCP and AWS quickly turned out to require quota increases which either didn’t happen or took too long. Thus, I decided to explore the paid options of &lt;a href=&#34;https://research.google.com/colaboratory/&#34;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I had only ever used the free version of Colab, and found 2 paid subscriptions: Colab Pro and Colab Pro+. The Plus version, at a not insignificant $50/month, was advertising “priority access to faster GPUs” compared to the 10 bucks/month Pro tier. While researching more about this and other vaguenesses in the feature descriptions that the website was offering up, I quickly realised that Plus had only be launched a day or two earlier. And nobody really seemed to know what the exact specs were. So I thought to myself “in for a penny, in for a pound”. Or about 36 pounds at the current exchange rate. Might as well get Plus and figure out what it’s about.&lt;/p&gt;
&lt;p&gt;This post describes the features I found in Colab Pro+, alongside some notes how to best use any version of Colab in a Kaggle competition. After sharing some findings &lt;a href=&#34;https://twitter.com/heads0rtai1s/status/1428833896330899463&#34;&gt;on Twitter&lt;/a&gt;, I received a number of very useful suggestions for Colab alternatives which are compiled in the final part of the post.&lt;/p&gt;
&lt;div id=&#34;colab-pro-features&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Colab Pro+ features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;GPU resources:&lt;/strong&gt; The Plus subscription gave me access to 1 V100 GPU in its “High-RAM” GPU runtime setting. I could only run a single of these sessions at a time. Alternatively, the “Standard” RAM runtime option allowed me to run 2 concurrent sessions with 1 P100 GPU each.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The “High-RAM” runtime did justify its name by providing 53GB of RAM, alongside 8 CPU cores. In my “Standard” RAM sessions I got 13GB RAM and 2 CPUs (which I think might be what’s included in the free Colab).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;runtime limit&lt;/strong&gt; for any session is 24 hours; which was consistent throughout my tests. The Plus subscription advertises that the notebook keeps running even after closing the browser, which can be a useful feature. But I didn’t test this. Be aware that even in Pro+ the runtime still disconnects after a certain time of inactivity (i.e. no cells are running).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With large data, &lt;strong&gt;storage&lt;/strong&gt; is important. Compared to the 100GB of the Pro subscription, Pro+ provided me with 150GB of disk space. This turned out to make a crucial difference in allowing me to copy all the train plus test data, in addition to pip installing updated libraries.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What I didn’t test: Colab’s TPU runtime as well as the number of concurrent CPU sessions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Are those features worth the $50/month? On the one hand, having 24h of V100 power is a notable step up from the free Colab and Kaggle resources. On the other hand, being restricted to 1 session at a time, or 2 sessions with slower P100s, can be a limiting factor in a time crunch.&lt;/p&gt;
&lt;p&gt;Also note, that the Colab FAQ states that “Resources in Colab Pro and Pro+ are prioritized for subscribers who have recently used less resources, in order to prevent the monopolization of limited resources by a small number of users.” Thus, it seems unlikely that one could use a V100 GPU 24/7 for an entire month. I intend to run more experiments and might encounter this limit sooner or later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kaggling-on-colab&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Kaggling on Colab&lt;/h3&gt;
&lt;p&gt;If you’ve exceeded your (considerable) Kaggle resources for the week or, like me, need a bit more horse power for a short time, then moving your Kaggle Notebook into Colab is a good option to keep training and experimenting. It can be non-trivial, though, and the 2 main challenges for me were getting the data and setting up the notebook environment. Well, that and Colab’s slightly infuriating choice to forego many of the standard Jupyter keyboard shortcuts (seriously: why?).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Get your data into Colab:&lt;/strong&gt; by far the best and fastest way here is to copy the data via their &lt;code&gt;GCS_DS_PATH&lt;/code&gt;; i.e. Google Cloud Storage path. Since Kaggle was acquired by Google in 2017, there has been significant integration of its framework into Google’s cloud environments. Kaggle datasets and competition data have cloud storage addresses and can be quickly moved to Colab from there.&lt;/p&gt;
&lt;p&gt;You can get the &lt;code&gt;GCS_DS_PATH&lt;/code&gt; by running the following code in a Kaggle Notebook. Substitute &lt;code&gt;seti-breakthrough-listen&lt;/code&gt; with the name of your competition or dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from kaggle_datasets import KaggleDatasets
GCS_DS_PATH = KaggleDatasets().get_gcs_path(&amp;quot;seti-breakthrough-listen&amp;quot;)
print(GCS_DS_PATH)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within Colab you can then use the &lt;code&gt;gsutil&lt;/code&gt; tool to copy the dataset, or even individual folders, like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;!gsutil -m cp -r {GCS_DS_PATH}/&amp;quot;train&amp;quot; .
!gsutil -m cp -r {GCS_DS_PATH}/&amp;quot;test&amp;quot; .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This retrieves the data significantly faster than by copying from Google Drive or downloading via the (otherwise great) &lt;a href=&#34;https://github.com/Kaggle/kaggle-api&#34;&gt;Kaggle API&lt;/a&gt;. And of course getting the data counts towards your 24 hour runtime limit. Keep in mind that the data is gone after your session gets disconnected, and you need to repeat the setup in a new session.&lt;/p&gt;
&lt;p&gt;The same is true for any files you create in your session (such as trained model weights or submission files) or for &lt;strong&gt;custom libraries&lt;/strong&gt; that you install. Colab has the usual Python and Deep Learning tools installed, but I found the versions to be rather old. You can update via &lt;code&gt;pip&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;!pip install --upgrade --force-reinstall fastai==2.4.1 -qq
!pip install --upgrade --force-reinstall timm==0.4.12 -qq
!pip install --upgrade --force-reinstall torch -qq&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two things to note: after installing you need to restart your runtime to be able to &lt;code&gt;import&lt;/code&gt; the new libraries. No worries: your data will still be there after the restart. Also make sure to leave enough disk space to install everything you need.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Save your outputs on Drive:&lt;/strong&gt; A final note to make sure to copy the results of your experiments - whether they’re trained weights, submission files, or EDA visuals - to your Google Drive account to make sure you don’t lose them when the runtime disconnects. Better to learn from my mistakes than by making them yourself. You can always download stuff manually, but I found that copying them automatically is more reliable.&lt;/p&gt;
&lt;p&gt;You can mount Drive in your Colab notebook like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from google.colab import drive
drive.mount(&amp;#39;/content/drive&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then copy files e.g. via Python’s &lt;code&gt;os.system&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-cloud-gpu-options&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other cloud GPU options&lt;/h3&gt;
&lt;p&gt;While Colab and its Pro versions, as outlined above, have several strong points in their favour, you might want explore other cloud GPU alternatives that either offer more power (A100s!) or are cheaper or more flexible to use. The options here were contributed by other ML practitioners &lt;a href=&#34;https://twitter.com/heads0rtai1s/status/1428833896330899463&#34;&gt;via Twitter&lt;/a&gt; and are listed in no particular order. I’m not including the well-known GCP or AWS here, although someone recommended preemptible (aka “spot”) instances on these platforms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://gradient.paperspace.com/pricing&#34;&gt;Paperspace Gradient&lt;/a&gt;: the G1 subscription costs $8/month and gives free instances with smaller GPUs and a 6h runtime limit. Beyond that, pay $2.30/h to run &lt;a href=&#34;https://docs.paperspace.com/gradient/more/instance-types&#34;&gt;an V100 instance&lt;/a&gt;. 200GB storage included, and 5 concurrent running notebooks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.jarvislabs.ai/&#34;&gt;JarvisCloud&lt;/a&gt;: great landing page, plus A100 GPUs at $2.4/h are being attractive features here. They also offer up-to-date Pytorch, FastAI, Tensorflow as pre-installed frameworks. Storage up to 500GB at max 7 cents per hour.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vast.ai/console/create/&#34;&gt;Vast.ai&lt;/a&gt;: is a marketplace for people to rent out their GPUs. You can also access GCP, AWS, and Paperspace resources here. Prices vary quite a bit, but some look significantly cheaper than those of the big players at a similar level of reliability.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.oracle.com/cloud/compute/pricing.html&#34;&gt;OracleCloud&lt;/a&gt;: seems to be at about $3/h for V100, which is comparable to AWS. A100s “available soon”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.ovhcloud.com/en/public-cloud/prices/&#34;&gt;OHVcloud&lt;/a&gt;: a French provider known for being not very expensive. They have 1 V100 with 400GB storage starting at $1.7/h; which is not bad at all.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Plenty of options available to explore further. Maybe we’ll see prices drop a bit more amidst this healthy competition.&lt;/p&gt;
&lt;p&gt;Hope those are useful for your projects on Kaggle or otherwise. Have fun!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Installing CUDA, tensorflow, torch for R &amp; Python on Ubuntu 20.04</title>
      <link>https://heads0rtai1s.github.io/2021/02/25/gpu-setup-r-python-ubuntu/</link>
      <pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2021/02/25/gpu-setup-r-python-ubuntu/</guid>
      <description>


&lt;p&gt;Last weekend, I finally managed to get round to upgrading Ubuntu from version 19.10 to the &lt;a href=&#34;https://releases.ubuntu.com/20.04/&#34;&gt;long-term support release 20.04&lt;/a&gt; on my workhorse laptop. To be precise, I’m using the &lt;a href=&#34;https://kubuntu.org/&#34;&gt;Kubuntu flavour&lt;/a&gt; since &lt;a href=&#34;https://www.youtube.com/watch?v=9QxJQRVQqao&#34;&gt;I’m more of a KDE guy myself&lt;/a&gt;. I usually do a fresh install on those occasions, instead of a &lt;code&gt;dist_upgrade&lt;/code&gt;, because it’s a good opportunity to remove clutter and update software that I might otherwise just keep at an older version, out of convenience.&lt;/p&gt;
&lt;p&gt;One of my main goals this year is to get better at deep learning (DL) in R and Python - and there’s no way around using GPUs for those purposes. My laptop, a Dell G3 15, has a Nvidia GeForce GTX 1660, which at the time of writing does a decent job at playing with smaller neural networks which can then be scaled up on cloud platforms such as &lt;a href=&#34;https://www.kaggle.com/code&#34;&gt;Kaggle Notebooks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Setting up GPU-powered DL libraries on your local machine can still be a somewhat daunting task. Having just done it successfully (crossing fingers; nothing broke yet) I decided to write down my notes and experiences while they are still fresh. At a minimum, this will help me the next time I set up at DL machine. And maybe my experience can even be helpful to others in a similar situation.&lt;/p&gt;
&lt;p&gt;Note: before starting you want to be sure that your machine has a Nvidia GPU that’s recent enough to run DL software. If in doubt, read up on compute capability (and consult &lt;a href=&#34;https://developer.nvidia.com/cuda-gpus#compute&#34;&gt;those tables&lt;/a&gt;).&lt;/p&gt;
&lt;div id=&#34;high-level-overview-main-challenge&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;High-level overview &amp;amp; main challenge&lt;/h1&gt;
&lt;p&gt;These are the main ingredients you need to enable your R &amp;amp; Python DL packages:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;CUDA drivers to access your GPU.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The cuDNN library which provides GPU acceleration.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For Python, the DL framework of your choice: Tensorflow or Pytorch.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For R, the &lt;code&gt;reticulate&lt;/code&gt; package for &lt;code&gt;keras&lt;/code&gt; and/or the new &lt;code&gt;torch&lt;/code&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These steps by themselves are not that hard, and there is a reasonable amount of documentation available online. &lt;strong&gt;The main challenge lies in finding the right library versions that play nicely together.&lt;/strong&gt; This difficulty stems primarily from the breakneck speed at which all the parts of the DL ecosystem continue to evolve. New features are constantly being implemented, and older versions might no longer be supported. For instance, Tensorflow version 2 is significantly re-imagined (and considerably more beginner friendly) than version 1.&lt;/p&gt;
&lt;p&gt;As a result, the latest GPU driver library versions might not always be supported by the latest DL package version. I ran into this problem at the very end of my first installation attempt (when installing Pytorch) and decided that it would be easier to redo everything from scratch. And indeed, the second installation went much smoother and faster. I hope that my lost hours are your gain, dear reader, and that my repeated experience will prove useful in one way or another.&lt;/p&gt;
&lt;p&gt;Below I outline the necessary installation steps.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-by-step-installation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step-by-step installation&lt;/h1&gt;
&lt;div id=&#34;prerequisites-a-clean-system&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prerequisites: a clean system&lt;/h2&gt;
&lt;p&gt;It’s possible that you already have some CUDA or Nvidia libraries installed. But honestly, the best way is to remove everything and start with a clean install. Otherwise there’s just too much danger of version clashes or duplicated paths. The following steps accomplish this. This is also the way in which you can clean up a botched or wrong CUDA installation (like I did) and start afresh. The following is copied from the &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html&#34;&gt;CUDA installation manual&lt;/a&gt; (more on this in the next step):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Remove CUDA Toolkit:
sudo apt-get --purge remove &amp;quot;*cublas*&amp;quot; &amp;quot;*cufft*&amp;quot; &amp;quot;*curand*&amp;quot; &amp;quot;*cusolver*&amp;quot; &amp;quot;*cusparse*&amp;quot; &amp;quot;*npp*&amp;quot; &amp;quot;*nvjpeg*&amp;quot; &amp;quot;cuda*&amp;quot; &amp;quot;nsight*&amp;quot; &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Remove Nvidia Drivers:
sudo apt-get --purge remove &amp;quot;*nvidia*&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Clean up the uninstall:
sudo apt-get autoremove&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some more clean-up tips are given in &lt;a href=&#34;cleanup:%20https://medium.com/@stephengregory_69986/installing-cuda-10-1-on-ubuntu-20-04-e562a5e724a0&#34;&gt;this article&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cuda-drivers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CUDA drivers&lt;/h2&gt;
&lt;p&gt;Let’s get the CUDA GPU drivers (aka CUDA toolkit). Note, that there are instructions for this on software-specific websites, such as &lt;a href=&#34;https://www.tensorflow.org/install/gpu&#34;&gt;for Tensorflow&lt;/a&gt;. However, those aren’t always up to date, and I recommend instead to follow the &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html&#34;&gt;official CUDA installation manual&lt;/a&gt; which is really good and detailed.&lt;/p&gt;
&lt;p&gt;So detailed, in fact, that in can be a little overwhelming at first contact. Here I break down the essential steps:&lt;/p&gt;
&lt;div id=&#34;choose-and-install-the-appropriate-cuda-version&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Choose and install the appropriate CUDA version&lt;/h3&gt;
&lt;p&gt;There’s a nice little platform selector linked in the manual, but &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;do not use this version&lt;/a&gt;. Or at least double check if you want this version. Because this link always chooses the most recent CUDA version, which is 11.2 as I’m writing these lines. Now, also at the time of writing, Pytorch &amp;amp; torchlib only support CUDA 11.0 (not the latest 11.2) and Tensorflow 2.4 is also build against the same version. Therefore, we want to install CUDA 11.0.&lt;/p&gt;
&lt;p&gt;(If you decide to install the latest CUDA version instead, there are some troubleshooting notes at the very bottom of this article that might help you out in a pinch.)&lt;/p&gt;
&lt;p&gt;You can &lt;a href=&#34;https://developer.nvidia.com/cuda-11.0-update1-download-archive&#34;&gt;get the CUDA 11.0 toolkit here&lt;/a&gt;. This gives you the exact same platform selection steps. &lt;a href=&#34;https://developer.nvidia.com/cuda-11.0-update1-download-archive?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;target_distro=Ubuntu&amp;amp;target_version=2004&amp;amp;target_type=deblocal&#34;&gt;This is my configuration&lt;/a&gt;, which gives me the following install commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin
sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda-repo-ubuntu2004-11-0-local_11.0.3-450.51.06-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2004-11-0-local_11.0.3-450.51.06-1_amd64.deb
sudo apt-key add /var/cuda-repo-ubuntu2004-11-0-local/7fa2af80.pub
sudo apt-get update
sudo apt-get install cuda&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Essentially, you download the CUDA toolkit as a &lt;code&gt;.deb&lt;/code&gt; package, add the CUDA repository for Ubuntu 20.04, and install. The &lt;code&gt;pin&lt;/code&gt; stuff makes sure that you continue to pull CUDA stuff from the right repository in the future (&lt;a href=&#34;https://help.ubuntu.com/community/PinningHowto&#34;&gt;see e.g. here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;.deb&lt;/code&gt; file is about 2.2 GB, so you might want to get a cup of coffee or tea while downloading.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-the-correct-library-paths&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Set the correct library paths&lt;/h3&gt;
&lt;p&gt;The easiest way is to copy those three lines into your &lt;code&gt;.bashrc&lt;/code&gt;. (&lt;a href=&#34;https://unix.stackexchange.com/questions/129143/what-is-the-purpose-of-bashrc-and-how-does-it-work&#34;&gt;What is bashrc?&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export PATH=/usr/local/cuda-11.0/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=/usr/local/cuda-11.0/include:$LD_LIBRARY_PATH&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;confirm-the-install&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confirm the install&lt;/h3&gt;
&lt;p&gt;To make sure that everything is working, run those commands. None of them should throw an error:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat /proc/driver/nvidia/version&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;nvcc -V&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This last tool, SMI, is very useful to see your driver versions and also the GPU memory usage during training.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;optional-libraries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Optional libraries&lt;/h3&gt;
&lt;p&gt;Not strictly necessary, but probably useful in one way or another. In my case, I had most of those already installed anyway:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install g++ freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libglu1-mesa libglu1-mesa-dev&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;cudnn-libraries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;cuDNN libraries&lt;/h2&gt;
&lt;p&gt;You also need Nvidia’s &lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN&lt;/a&gt;, the CUDA Deep Neural Network library. Those tools provide GPU-optimised implementation for neural network fundamentals.&lt;/p&gt;
&lt;p&gt;Getting the appropriate cuDNN libraries is easier than the previous step. You can download them from the &lt;a href=&#34;https://developer.nvidia.com/rdp/cudnn-download&#34;&gt;Nvidia developer portal&lt;/a&gt;. That website requires you to make a free account, which is just a formality. When choosing the cuDNN version you will see the options with their matching CUDA versions, e.g.: &lt;code&gt;Download cuDNN v8.1.0 (January 26th, 2021), for CUDA 11.0,11.1 and 11.2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We just installed CUDA 11.0, so we’ll click on the above option which provides a list of download links for different operating systems and architectures. There is a generic version &lt;code&gt;cuDNN Library for Linux (x86_64)&lt;/code&gt; which provides a &lt;code&gt;.tgz&lt;/code&gt; file we could use. But as (K)Ubuntu users we can also download tailored &lt;code&gt;.deb&lt;/code&gt; packages instead. There is a “Developer Version”, a “Runtime Version”, and “Code Samples and User Guide” - all for “Ubuntu20.04 x86_64 (Deb)”. Perfect! Just download everything.&lt;/p&gt;
&lt;p&gt;Once you’ve got the packages, there is a pretty nice &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html&#34;&gt;cuDNN installation guide&lt;/a&gt; which boils down to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo dpkg -i libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb 
sudo dpkg -i libcudnn8-dev_8.1.0.77-1+cuda11.2_amd64.deb 
sudo dpkg -i libcudnn8-samples_8.1.0.77-1+cuda11.2_amd64.deb &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The guide also includes some troubleshooting and verification steps, but this part rarely goes wrong.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tensorflow-pytorch-for-python&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tensorflow &amp;amp; Pytorch for Python&lt;/h2&gt;
&lt;p&gt;The drivers are the main challenge; from here everything should be straightforward. There are 2 main deep learning packages in 2020: Tensorflow and Pytorch. If you’re just starting out with deep learning, then in my view it doesn’t matter much which one you pick. They are both pretty user friendly by now, and the fundamentals are similar enough so that familiarity with one package will help you to get started quickly with the other.&lt;/p&gt;
&lt;p&gt;For &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Tensorflow&lt;/a&gt;, not long ago there were two different Python packages for GPU and CPU, respectively. But now you get everything via:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install tensorflow keras&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://keras.io/&#34;&gt;Keras&lt;/a&gt; is a well-designed high-level API for Tensorflow. These other 2 packages are useful additions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install tensorflow_datasets tensorflow_addons&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For &lt;a href=&#34;https://pytorch.org/&#34;&gt;Pytorch&lt;/a&gt;, I have a penchant for &lt;a href=&#34;https://www.fast.ai/&#34;&gt;FastAI&lt;/a&gt; as a higher-level gateway. Using my preferred &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;miniconda&lt;/a&gt; environment, you can get both from their respective channels like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda create -n fastai -c fastai -c pytorch fastai&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll need some kind of environment manager for the next R step anyway, and it’s easier to keep up with the rapidly evolving libraries if you use some version of &lt;a href=&#34;https://docs.anaconda.com/&#34;&gt;anaconda&lt;/a&gt;. This &lt;code&gt;conda&lt;/code&gt; install will also get you stuff like &lt;code&gt;torchvision&lt;/code&gt; for image models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tensorflow-torch-for-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tensorflow &amp;amp; Torch for R&lt;/h2&gt;
&lt;p&gt;In R, Tensorflow and Keras are best installed via the &lt;a href=&#34;https://keras.rstudio.com/&#34;&gt;keras package&lt;/a&gt;. This uses the fantastic &lt;a href=&#34;https://rstudio.github.io/reticulate/&#34;&gt;reticulate package&lt;/a&gt; as a wrapper around Python’s Tensorflow/Keras, so make sure you got it installed. For an introduction to reticulate check out my &lt;a href=&#34;https://heads0rtai1s.github.io/2019/10/03/reticulate-intro/&#34;&gt;earlier blogpost&lt;/a&gt;. Install as such:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;keras&amp;quot;, repos=&amp;quot;http://cran.r-project.org&amp;quot;, dependencies=TRUE)
keras::install_keras(tensorflow = &amp;quot;gpu&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you want to check your Python configuration for &lt;code&gt;reticulate&lt;/code&gt;, along with the &lt;code&gt;keras&lt;/code&gt; availability:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reticulate::py_config() 
reticulate::py_module_available(&amp;quot;keras&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For torch, there is now a &lt;a href=&#34;https://torch.mlverse.org/&#34;&gt;native R package&lt;/a&gt; which &lt;em&gt;doesn’t&lt;/em&gt; use Pytorch under the hood. (Instead, it’s build on the same C++ backend, called &lt;a href=&#34;https://github.com/pytorch/pytorch/blob/master/docs/libtorch.rst&#34;&gt;libtorch&lt;/a&gt;, as the Python version.)&lt;/p&gt;
&lt;p&gt;The 1st step for installing &lt;code&gt;torch&lt;/code&gt; is this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;torch&amp;quot;, repos=&amp;quot;http://cran.r-project.org&amp;quot;, dependencies=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you need to activate it, which then downloads and installs necessary stuff:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(torch)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: if this step fails for no good reason then you want to try replacing it with &lt;code&gt;install_torch(timeout=1000)&lt;/code&gt;. This timeout is important, because the corresponding files are relative large and the default is only 360 seconds.&lt;/p&gt;
&lt;p&gt;And while you’re there, you might also want to get those extra packages for common use cases:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;torchvision&amp;quot;, repos=&amp;quot;http://cran.r-project.org&amp;quot;, dependencies=TRUE)
install.packages(&amp;quot;torchaudio&amp;quot;, repos=&amp;quot;http://cran.r-project.org&amp;quot;, dependencies=TRUE)
remotes::install_github(&amp;quot;mlverse/torchdatasets&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;does-everything-work&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Does everything work?&lt;/h1&gt;
&lt;p&gt;Now everything should be there on your machine. But does it all work as it should?&lt;/p&gt;
&lt;p&gt;In Python, you can check Tensorflow and Pytorch as such (and get some information about your GPU in the process):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import tensorflow as tf
tf.config.list_physical_devices(&amp;#39;GPU&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’re using &lt;code&gt;conda&lt;/code&gt;, don’t forget to &lt;code&gt;activate&lt;/code&gt; your environment:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import torch
torch.cuda.get_device_name()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In R, the installations steps should already have told you if something didn’t work. In addition, you can also check the status of the &lt;code&gt;keras&lt;/code&gt; and &lt;code&gt;torch&lt;/code&gt; packages like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(keras)
is_keras_available()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;library(torch)
cuda_is_available()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that’s it! You now have GPU-powered deep learning capabilities at your disposal. Use them wisely. Or, you know, just have fun with them. Either way, I hope this post was helpful.&lt;/p&gt;
&lt;p&gt;More info:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This is far from the only write-up on getting your GPU tools set up on (Ubuntu) Linux. For my first successful installation on Kubuntu 19.10 I was largely following &lt;a href=&#34;https://medium.com/@Oysiyl/install-tensorflow-2-with-gpu-support-on-ubuntu-19-10-f502ae85593c&#34;&gt;this post by Dmitriy Kisil&lt;/a&gt;. For the current 20.04 install, I compared the CUDA and cuDNN instructions to posts by &lt;a href=&#34;https://medium.com/@tunguz/installing-tensorflow-on-ubuntu-20-04-bcada5a9c7e1&#34;&gt;Bojan Tunguz&lt;/a&gt; and &lt;a href=&#34;https://medium.com/@stephengregory_69986/installing-cuda-10-1-on-ubuntu-20-04-e562a5e724a0&#34;&gt;Stephen Gregory&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remember that you don’t need to install GPU software if all you want to do is to experiment with deep learning tools. There are plenty of resources online where you can try out the code, e.g. via Google Colab, in a pre-configured cloud environment. This also includes &lt;a href=&#34;https://www.kaggle.com/code&#34;&gt;Kaggle Notebooks&lt;/a&gt;, which come equipped with a large set of data science and machine learning packages.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I managed to install CUDA 11.2 (and TF 2.4), but it was less smooth than version 11.0. Specifically, &lt;code&gt;sudo apt-get install cuda&lt;/code&gt; threw a &lt;code&gt;you have held broken packages&lt;/code&gt; error and refused to do the install. The solution was to use the more tenacious &lt;code&gt;aptitude&lt;/code&gt; to &lt;code&gt;sudo aptitude install cuda&lt;/code&gt; which suggested that &lt;code&gt;libnvidia-compute-460&lt;/code&gt; needed to be downgraded. After that the install worked without a hitch.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Another CUDA 11.2 related issue popped up for Tensorflow on Python. The installation worked and &lt;code&gt;import tensorflow&lt;/code&gt; also worked, but when using the library I got the error message &lt;code&gt;Could not load dynamic library &#39;libcusolver.so.10&#39;&lt;/code&gt;. This was most likely related to TF 2.4 being build against CUDA 11.0, not 11.2 (see &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues/45848&#34;&gt;here&lt;/a&gt;). The workaround was to make a hard-link to pretend that &lt;code&gt;.so.11&lt;/code&gt; is &lt;code&gt;.so.10&lt;/code&gt; (see &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues/44777&#34;&gt;here&lt;/a&gt;): &lt;code&gt;cd /usr/local/cuda-11.2/lib64; sudo ln libcusolver.so.11 libcusolver.so.10&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R &amp; Python Rosetta Stone: EDA with dplyr vs pandas</title>
      <link>https://heads0rtai1s.github.io/2020/11/05/r-python-dplyr-pandas/</link>
      <pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2020/11/05/r-python-dplyr-pandas/</guid>
      <description>


&lt;p&gt;This is the first post in a new series featuring translations between R and Python code for common data science and machine learning tasks. A &lt;a href=&#34;https://en.wikipedia.org/wiki/Rosetta_Stone&#34;&gt;Rosetta Stone&lt;/a&gt;, if you will. I’m writing this mainly as a documented cheat sheet for myself, as I’m frequently switching between the two languages. Personally, I have learned Python and R around the same time, several years ago, but tidy R is much more intuitive to me than any other language. Hopefully, these posts can be useful to others in a similar situation. My point of reference is primarily R - with the aim to provide equivalent Python code - but occasionally I will look at translations of Python features into R.&lt;/p&gt;
&lt;p&gt;The first post will focus on the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; backbone &lt;a href=&#34;https://dplyr.tidyverse.org/&#34;&gt;dplyr&lt;/a&gt; and compare it to Python’s data science workhorse &lt;a href=&#34;https://pandas.pydata.org/&#34;&gt;pandas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As with all my blog posts, the code will run in &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;Rmarkdown&lt;/a&gt; through the fantastic &lt;a href=&#34;https://rstudio.com/&#34;&gt;Rstudio IDE&lt;/a&gt;. All the output will be reproducible. Rstudio provides Python support via the great &lt;a href=&#34;https://rstudio.github.io/reticulate/&#34;&gt;reticulate&lt;/a&gt; package. If you haven’t heard of it yet, check out my &lt;a href=&#34;https://heads0rtai1s.github.io/2019/10/03/reticulate-intro/&#34;&gt;intro post on reticulate&lt;/a&gt; to get started.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note: you need at least RStudio version 1.2 to be able to pass objects between R and Python.&lt;/strong&gt; In addition, as always, here are the required packages. We are also setting the python path (see the &lt;a href=&#34;https://heads0rtai1s.github.io/2019/10/03/reticulate-intro/&#34;&gt;intro post&lt;/a&gt; for more details on the path) and importing &lt;em&gt;pandas&lt;/em&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
libs &amp;lt;- c(&amp;#39;dplyr&amp;#39;,                # wrangling
          &amp;#39;palmerpenguins&amp;#39;,       # data
          &amp;#39;knitr&amp;#39;,&amp;#39;kableExtra&amp;#39;,   # table styling
          &amp;#39;reticulate&amp;#39;)           # python support 
invisible(lapply(libs, library, character.only = TRUE))

use_python(&amp;quot;/usr/bin/python&amp;quot;)

df &amp;lt;- penguins %&amp;gt;% 
  mutate_if(is.integer, as.double)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
import pandas as pd
pd.set_option(&amp;#39;display.max_columns&amp;#39;, None)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will be using the &lt;a href=&#34;https://www.kaggle.com/parulpandey/palmer-archipelago-antarctica-penguin-data&#34;&gt;Palmer Penguins dataset&lt;/a&gt;, as provided by the brilliant &lt;a href=&#34;https://www.allisonhorst.com/&#34;&gt;Allison Horst&lt;/a&gt; through the &lt;a href=&#34;https://allisonhorst.github.io/palmerpenguins/&#34;&gt;eponymous R package&lt;/a&gt; - complete with her trademark adorable artwork. I was looking for an excuse to work with this dataset. Therefore, this will be a genuine example for an exploratory analysis, as I’m encountering the data for the first time.&lt;/p&gt;
&lt;div id=&#34;general-overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;General overview&lt;/h3&gt;
&lt;p&gt;It’s best to start your EDA by looking at the big picture: How large is the dataset? How many features are there? What are the data types? Here we have tabular data, but similar steps can be taken for text or images as well.&lt;/p&gt;
&lt;p&gt;In R, I typically go with a combination of &lt;strong&gt;head&lt;/strong&gt;, &lt;strong&gt;glimpse&lt;/strong&gt;, and &lt;strong&gt;summary&lt;/strong&gt; to get a broad idea of the data properties. (Beyond &lt;em&gt;dplyr&lt;/em&gt;, there’s also the &lt;a href=&#34;https://cran.r-project.org/package=skimr&#34;&gt;skimr package&lt;/a&gt; for more sophisticated data overviews.)&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;head&lt;/code&gt;, we see the first (by default) 6 rows of the dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##   species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g
##   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;           &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 Adelie  Torge…           39.1          18.7              181        3750
## 2 Adelie  Torge…           39.5          17.4              186        3800
## 3 Adelie  Torge…           40.3          18                195        3250
## 4 Adelie  Torge…           NA            NA                 NA          NA
## 5 Adelie  Torge…           36.7          19.3              193        3450
## 6 Adelie  Torge…           39.3          20.6              190        3650
## # … with 2 more variables: sex &amp;lt;fct&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We find that our data is a mix of numerical and categorical columns. There are 8 columns in total. We get an idea of the order of magnitude of the numeric features, see that the categorical ones have text, and already spot a couple of missing values. (Note, that I converted all integer columns to double for easier back-and-forth with Python).&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;pandas&lt;/em&gt; &lt;strong&gt;head&lt;/strong&gt; command is essentially the same. One general difference here is that in &lt;em&gt;pandas&lt;/em&gt; (and Python in general) everything is an object. Methods (and attributes) associated with the object, which is a &lt;em&gt;pandas&lt;/em&gt; &lt;code&gt;DataFrame&lt;/code&gt; here, are accessed via the dot “.” operator. For passing an R object to Python we preface it with &lt;code&gt;r.&lt;/code&gt; like such:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \
## 0  Adelie  Torgersen            39.1           18.7              181.0   
## 1  Adelie  Torgersen            39.5           17.4              186.0   
## 2  Adelie  Torgersen            40.3           18.0              195.0   
## 3  Adelie  Torgersen             NaN            NaN                NaN   
## 4  Adelie  Torgersen            36.7           19.3              193.0   
## 
##    body_mass_g     sex    year  
## 0       3750.0    male  2007.0  
## 1       3800.0  female  2007.0  
## 2       3250.0  female  2007.0  
## 3          NaN     NaN  2007.0  
## 4       3450.0  female  2007.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is less pretty than for Rmarkdown, but the result is pretty much the same. We don’t see column types, only their values. Another property of &lt;em&gt;pandas&lt;/em&gt; data frames is that they come with a row index. By default this is a sequential number of rows, but anything can become an index.&lt;/p&gt;
&lt;p&gt;With &lt;em&gt;dplyr&lt;/em&gt;’s &lt;strong&gt;glimpse&lt;/strong&gt; we can see a more compact, transposed display of column types and their values. Especially for datasets with many columns this can be a vital complement to &lt;code&gt;head&lt;/code&gt;. We also see the number of rows and columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
glimpse(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 344
## Variables: 8
## $ species           &amp;lt;fct&amp;gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie…
## $ island            &amp;lt;fct&amp;gt; Torgersen, Torgersen, Torgersen, Torgersen, To…
## $ bill_length_mm    &amp;lt;dbl&amp;gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, …
## $ bill_depth_mm     &amp;lt;dbl&amp;gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, …
## $ flipper_length_mm &amp;lt;dbl&amp;gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 19…
## $ body_mass_g       &amp;lt;dbl&amp;gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, …
## $ sex               &amp;lt;fct&amp;gt; male, female, female, NA, female, male, female…
## $ year              &amp;lt;dbl&amp;gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alongside the 8 columns, our dataset has 344 rows. That’s pretty small.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;pandas&lt;/em&gt;, there’s no identical equivalent to &lt;strong&gt;glimpse&lt;/strong&gt;. Instead, we can use the &lt;strong&gt;info&lt;/strong&gt; method to give us the feature types in vertical form. We also see the number of non-null features (the “sex” column has the fewest), together with the number of rows and columns.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
## RangeIndex: 344 entries, 0 to 343
## Data columns (total 8 columns):
## species              344 non-null category
## island               344 non-null category
## bill_length_mm       342 non-null float64
## bill_depth_mm        342 non-null float64
## flipper_length_mm    342 non-null float64
## body_mass_g          342 non-null float64
## sex                  333 non-null category
## year                 344 non-null float64
## dtypes: category(3), float64(5)
## memory usage: 14.8 KB&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While several columns have missing values, the overall number of those null entries is small.&lt;/p&gt;
&lt;p&gt;Additionally, &lt;em&gt;pandas&lt;/em&gt; gives us a summary of data types (3 categorical, 5 float) and tells us how much memory our data takes up. In R, you can use the &lt;code&gt;utils&lt;/code&gt; tool &lt;strong&gt;object.size&lt;/strong&gt; for the latter:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
object.size(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 21336 bytes&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next up is &lt;strong&gt;summary&lt;/strong&gt;, which provides basic overview stats for each feature. Here in particular, we learn that there are 3 penguin species, 3 islands, and 11 missing values in the sex column. “Chinstrap” penguins are rarer than the other two species; and Torgersen is the least frequent island. We also see the fundamental characteristics of the numeric features (min, max, quartiles, median). Mean and median are pretty close for most of those, which suggests little skewness:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
summary(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       species          island    bill_length_mm  bill_depth_mm  
##  Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  
##  Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  
##  Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  
##                                  Mean   :43.92   Mean   :17.15  
##                                  3rd Qu.:48.50   3rd Qu.:18.70  
##                                  Max.   :59.60   Max.   :21.50  
##                                  NA&amp;#39;s   :2       NA&amp;#39;s   :2      
##  flipper_length_mm  body_mass_g       sex           year     
##  Min.   :172.0     Min.   :2700   female:165   Min.   :2007  
##  1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  
##  Median :197.0     Median :4050   NA&amp;#39;s  : 11   Median :2008  
##  Mean   :200.9     Mean   :4202                Mean   :2008  
##  3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  
##  Max.   :231.0     Max.   :6300                Max.   :2009  
##  NA&amp;#39;s   :2         NA&amp;#39;s   :2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The closest &lt;em&gt;pandas&lt;/em&gt; equivalent to &lt;strong&gt;summary&lt;/strong&gt; is &lt;strong&gt;describe&lt;/strong&gt;. By default this only includes the numeric columns, but you can get around that by passing a list of features types that you want to include:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.describe(include = [&amp;#39;float&amp;#39;, &amp;#39;category&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        species  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \
## count      344     344      342.000000     342.000000         342.000000   
## unique       3       3             NaN            NaN                NaN   
## top     Adelie  Biscoe             NaN            NaN                NaN   
## freq       152     168             NaN            NaN                NaN   
## mean       NaN     NaN       43.921930      17.151170         200.915205   
## std        NaN     NaN        5.459584       1.974793          14.061714   
## min        NaN     NaN       32.100000      13.100000         172.000000   
## 25%        NaN     NaN       39.225000      15.600000         190.000000   
## 50%        NaN     NaN       44.450000      17.300000         197.000000   
## 75%        NaN     NaN       48.500000      18.700000         213.000000   
## max        NaN     NaN       59.600000      21.500000         231.000000   
## 
##         body_mass_g   sex         year  
## count    342.000000   333   344.000000  
## unique          NaN     2          NaN  
## top             NaN  male          NaN  
## freq            NaN   168          NaN  
## mean    4201.754386   NaN  2008.029070  
## std      801.954536   NaN     0.818356  
## min     2700.000000   NaN  2007.000000  
## 25%     3550.000000   NaN  2007.000000  
## 50%     4050.000000   NaN  2008.000000  
## 75%     4750.000000   NaN  2009.000000  
## max     6300.000000   NaN  2009.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You see that the formatting is less clever, since categorical indicators like “unique” or “top” are shown (with NAs) for the numeric columns and vice versa. Also, we only get told that there are 3 species and the most frequent one is “Adelie”; not the full counts per species.&lt;/p&gt;
&lt;p&gt;We have already learned a lot about our data from those basic overview tools. Typically, at this point in the EDA I would now start plotting feature distributions and interactions. Since we’re only focussing on &lt;em&gt;dplyr&lt;/em&gt; here, this part will have to wait for a future “ggplot2 vs seaborn” episode. For now, let’s look at the most simple overview before moving on to &lt;em&gt;dplyr&lt;/em&gt; verbs: number of rows and columns.&lt;/p&gt;
&lt;p&gt;In R, there is &lt;strong&gt;dim&lt;/strong&gt; while pandas has &lt;strong&gt;shape&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
dim(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 344   8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.shape&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (344, 8)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;subsetting-rows-and-columns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subsetting rows and columns&lt;/h3&gt;
&lt;p&gt;For extracting subsets of rows and columns, &lt;em&gt;dplyr&lt;/em&gt; has the verbs &lt;strong&gt;filter&lt;/strong&gt; and &lt;strong&gt;select&lt;/strong&gt;, respectively. For instance, let’s look at the species and sex of the penguins with the shortest bills:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
df %&amp;gt;% 
  filter(bill_length_mm &amp;lt; 34) %&amp;gt;% 
  select(species, sex, bill_length_mm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   species sex    bill_length_mm
##   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;           &amp;lt;dbl&amp;gt;
## 1 Adelie  female           33.5
## 2 Adelie  female           33.1
## 3 Adelie  female           32.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of those are female Adelie penguins. This is good news for a potential species classifier.&lt;/p&gt;
&lt;p&gt;In pandas, there are several ways to achieve the same result. All of them are a little more complicated than our two &lt;em&gt;dplyr&lt;/em&gt; verbs. The most pythonic way is probably to use the &lt;strong&gt;loc&lt;/strong&gt; operator like such:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.loc[r.df.bill_length_mm &amp;lt; 34, [&amp;#39;species&amp;#39;, &amp;#39;sex&amp;#39;, &amp;#39;bill_length_mm&amp;#39;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     species     sex  bill_length_mm
## 70   Adelie  female            33.5
## 98   Adelie  female            33.1
## 142  Adelie  female            32.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This indexing via the “[]” brackets is essentially the same as in base R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
na.omit(df[df$bill_length_mm &amp;lt; 34, c(&amp;#39;species&amp;#39;, &amp;#39;sex&amp;#39;, &amp;#39;bill_length_mm&amp;#39;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   species sex    bill_length_mm
##   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;           &amp;lt;dbl&amp;gt;
## 1 Adelie  female           33.5
## 2 Adelie  female           33.1
## 3 Adelie  female           32.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way is to use the pandas method &lt;strong&gt;query&lt;/strong&gt; as an equivalent for &lt;strong&gt;filter&lt;/strong&gt;. This allows us to essentially use &lt;em&gt;dplyr&lt;/em&gt;-style evaluation syntax. I found that in practice, &lt;strong&gt;query&lt;/strong&gt; is often notably slower than the other indexing tools. This can be important if your’re dealing with large datasets:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.query(&amp;quot;bill_length_mm &amp;lt; 34&amp;quot;).loc[:, [&amp;#39;species&amp;#39;, &amp;#39;sex&amp;#39;, &amp;#39;bill_length_mm&amp;#39;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     species     sex  bill_length_mm
## 70   Adelie  female            33.5
## 98   Adelie  female            33.1
## 142  Adelie  female            32.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In tidy R, the &lt;em&gt;dplyr&lt;/em&gt; verb &lt;strong&gt;select&lt;/strong&gt; can extract by value as well as by position. And for extracting rows by position there is &lt;strong&gt;slice&lt;/strong&gt;. This is just an arbitrary subset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
df %&amp;gt;% 
  slice(c(1,2,3)) %&amp;gt;% 
  select(c(4,5,6))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   bill_depth_mm flipper_length_mm body_mass_g
##           &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1          18.7               181        3750
## 2          17.4               186        3800
## 3          18                 195        3250&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In pandas, we would use bracket indexing again, but instead of &lt;strong&gt;loc&lt;/strong&gt; we now need &lt;strong&gt;iloc&lt;/strong&gt; (i.e. locating by index). This might be a good point to remind ourselves that Python starts counting from 0 and R starts from 1:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.iloc[[0, 1, 2], [3, 4, 5]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    bill_depth_mm  flipper_length_mm  body_mass_g
## 0           18.7              181.0       3750.0
## 1           17.4              186.0       3800.0
## 2           18.0              195.0       3250.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To emphasise again: pandas uses &lt;strong&gt;loc&lt;/strong&gt; for conditional indexing and &lt;strong&gt;iloc&lt;/strong&gt; for positional indexing. This takes some getting uses to, but this simple rule covers most of &lt;em&gt;pandas’&lt;/em&gt; subsetting operations.&lt;/p&gt;
&lt;p&gt;Retaining unique rows and removing duplicates can be thought of as another way of subsetting a data frame. In &lt;em&gt;dplyr&lt;/em&gt;, there’s the &lt;strong&gt;distinct&lt;/strong&gt; function which takes as arguments the columns that are considered for identifying duplicated rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
df %&amp;gt;% 
  slice(c(2, 4, 186)) %&amp;gt;% 
  distinct(species, island)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   species island   
##   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;    
## 1 Adelie  Torgersen
## 2 Gentoo  Biscoe&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;em&gt;pandas&lt;/em&gt; we can achieve the same result via the &lt;strong&gt;drop_duplicates&lt;/strong&gt; method:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;r.df.iloc[[2, 4, 186], :].drop_duplicates([&amp;#39;species&amp;#39;, &amp;#39;island&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \
## 2    Adelie  Torgersen            40.3           18.0              195.0   
## 186  Gentoo     Biscoe            49.1           14.8              220.0   
## 
##      body_mass_g     sex    year  
## 2         3250.0  female  2007.0  
## 186       5150.0  female  2008.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to retain only the two affected rows, like in the dplyr example above, you would have to select them using &lt;code&gt;.loc&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arrange-and-sample&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Arrange and Sample&lt;/h3&gt;
&lt;p&gt;Let’s deal with the arranging and sampling of rows in one fell swoop. In &lt;em&gt;dplyr&lt;/em&gt;, we use &lt;strong&gt;sample_n&lt;/strong&gt; (or &lt;strong&gt;sample_frac&lt;/strong&gt;) to choose a random subset of &lt;code&gt;n&lt;/code&gt; rows (or a fraction &lt;code&gt;frac&lt;/code&gt; of rows). Ordering a row by its values uses the verb &lt;strong&gt;arrange&lt;/strong&gt;, optionally with the &lt;strong&gt;desc&lt;/strong&gt; tool to specific descending order:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
set.seed(4321)
df %&amp;gt;% 
  select(species, bill_length_mm) %&amp;gt;% 
  sample_n(4) %&amp;gt;% 
  arrange(desc(bill_length_mm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 2
##   species   bill_length_mm
##   &amp;lt;fct&amp;gt;              &amp;lt;dbl&amp;gt;
## 1 Chinstrap           47.5
## 2 Adelie              42.7
## 3 Adelie              40.2
## 4 Adelie              34.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;em&gt;pandas&lt;/em&gt;, random sampling is done through the &lt;strong&gt;sample&lt;/strong&gt; function, which can take either a fraction or a number of rows. Here, we can also pass directly a random seed (called &lt;em&gt;random_state&lt;/em&gt;), instead of defining it outside the function via R’s &lt;strong&gt;set.seed&lt;/strong&gt;. Arranging is called &lt;strong&gt;sort_values&lt;/strong&gt; and we have to specify an &lt;em&gt;ascending&lt;/em&gt; flag because &lt;em&gt;pandas&lt;/em&gt; wants to be different:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.loc[:, [&amp;#39;species&amp;#39;, &amp;#39;bill_length_mm&amp;#39;]].\
  sample(n = 4, random_state = 4321).\
  sort_values(&amp;#39;bill_length_mm&amp;#39;, ascending = False)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        species  bill_length_mm
## 305  Chinstrap            52.8
## 301  Chinstrap            52.0
## 291  Chinstrap            50.5
## 165     Gentoo            48.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you see, the &lt;em&gt;pandas&lt;/em&gt; dot methods can be chained in a way reminiscent of the &lt;em&gt;dplyr&lt;/em&gt; pipe (&lt;code&gt;%&amp;gt;%&lt;/code&gt;). The scope for this style is much narrower than the pipe, though. It only works for methods and attributes associated with the specific &lt;em&gt;pandas&lt;/em&gt; data frame and its transformation results. Nevertheless, it has a certain power and elegance for pipe aficionados. When written accross multiple lines it requires the Python line continuation operator &lt;code&gt;\&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;group-by-and-summarise&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Group By and Summarise&lt;/h3&gt;
&lt;p&gt;Here is where &lt;em&gt;dplyr&lt;/em&gt; really shines. Grouping and summarising transformations fit seemlessly into any wrangling workflow by preserving the tidy &lt;code&gt;tibble&lt;/code&gt; data format. The verbs are &lt;strong&gt;group_by&lt;/strong&gt; and &lt;strong&gt;summarise&lt;/strong&gt;. Let’s compare average bill lengths among species to find that “Adelie” penguins have significantly shorter bills:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
df %&amp;gt;% 
  group_by(species) %&amp;gt;% 
  summarise(mean_bill_length = mean(bill_length_mm, na.rm = TRUE),
            sd_bill_length = sd(bill_length_mm, na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   species   mean_bill_length sd_bill_length
##   &amp;lt;fct&amp;gt;                &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 Adelie                38.8           2.66
## 2 Chinstrap             48.8           3.34
## 3 Gentoo                47.5           3.08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is consistent to what we had seen before for the 3 rows with shortes bills. The results also indicate that it would be much harder to try to separate “Chinstrap” vs “Gentoo” by &lt;code&gt;bill_length&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In terms of usage, &lt;em&gt;pandas&lt;/em&gt; is similar: we have &lt;strong&gt;groupby&lt;/strong&gt; (without the “&lt;code&gt;_&lt;/code&gt;”) to define the grouping, and &lt;strong&gt;agg&lt;/strong&gt; to aggregate (i.e. summarise) according to specific measures for certain features:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.groupby(&amp;#39;species&amp;#39;).agg({&amp;#39;bill_length_mm&amp;#39;: [&amp;#39;mean&amp;#39;, &amp;#39;std&amp;#39;]})&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           bill_length_mm          
##                     mean       std
## species                           
## Adelie         38.791391  2.663405
## Chinstrap      48.833824  3.339256
## Gentoo         47.504878  3.081857&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is pretty much the default way in &lt;em&gt;pandas&lt;/em&gt;. It is worth noting that &lt;em&gt;pandas&lt;/em&gt;, and Python in general, gets a lot of milage out of 2 data structures: &lt;strong&gt;lists&lt;/strong&gt; like &lt;code&gt;[1, 2, 3]&lt;/code&gt; which are the equivalent of &lt;code&gt;c(1, 2, 3)&lt;/code&gt;, and &lt;strong&gt;dictionaries&lt;/strong&gt; &lt;code&gt;{&#39;a&#39;: 1, &#39;b&#39;: 2}&lt;/code&gt; which are sets of key-value pairs. Values in a dictionary can be pretty much anything, including lists or dictionaries. (Most uses cases for dictionaries are probably served by &lt;em&gt;dplyr&lt;/em&gt; tibbles.) Here we use a dictionary to define which column we want to summarise and how. Note again the chaining via dots.&lt;/p&gt;
&lt;p&gt;The outcome of this operation, however, is slightly different from &lt;em&gt;dplyr&lt;/em&gt; in that we get a hierarchical data frame with a categorical index (this is no longer a “normal” column&amp;quot;) and 2 data columns that both have the designation &lt;code&gt;bill_length_mm&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.groupby(&amp;#39;species&amp;#39;).agg({&amp;#39;bill_length_mm&amp;#39;: [&amp;#39;mean&amp;#39;, &amp;#39;std&amp;#39;]}).info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
## CategoricalIndex: 3 entries, Adelie to Gentoo
## Data columns (total 2 columns):
## (bill_length_mm, mean)    3 non-null float64
## (bill_length_mm, std)     3 non-null float64
## dtypes: float64(2)
## memory usage: 155.0 bytes&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will see the limitations of that format if you try to chain another method. To get something instead that’s more closely resembling our &lt;em&gt;dplyr&lt;/em&gt; output, here is a different way: we forego the dictionary in favour of a simple list, then add a suffix later, and finally reset the index to a normal column:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.groupby(&amp;#39;species&amp;#39;).bill_length_mm.agg([&amp;#39;mean&amp;#39;, &amp;#39;std&amp;#39;]).add_suffix(&amp;quot;_bill_length&amp;quot;).reset_index()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      species  mean_bill_length  std_bill_length
## 0     Adelie         38.791391         2.663405
## 1  Chinstrap         48.833824         3.339256
## 2     Gentoo         47.504878         3.081857&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can treat the result like a typical data frame. Note, that here we use another form of column indexing to select &lt;code&gt;bill_length_mm&lt;/code&gt; after the &lt;code&gt;groupby&lt;/code&gt;. This shorthand, which treats a feature as an object attribute, only works for single columns. Told you that &lt;em&gt;pandas&lt;/em&gt; indexing can be a little confusing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;joining-and-binding&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Joining and binding&lt;/h3&gt;
&lt;p&gt;There’s another source of &lt;em&gt;dplyr&lt;/em&gt; vs &lt;em&gt;pandas&lt;/em&gt; confusion when it comes to SQL-style joins and to binding rows and columns. To demonstrate, we’ll create an additional data frame which holds the mean bill length by species. And we pretend that it’s a separate table. In terms of analysis steps, this crosses over from EDA into feature engineering territory, but that line can get blurry if you’re exploring a dataset’s hidden depths.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
df2 &amp;lt;- df %&amp;gt;% 
  group_by(species) %&amp;gt;% 
  summarise(mean_bill = mean(bill_length_mm, na.rm = TRUE))

df2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   species   mean_bill
##   &amp;lt;fct&amp;gt;         &amp;lt;dbl&amp;gt;
## 1 Adelie         38.8
## 2 Chinstrap      48.8
## 3 Gentoo         47.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;dplyr&lt;/em&gt; verbs for SQL-like joins are very similar to the various SQL flavours. We have &lt;strong&gt;left_join&lt;/strong&gt;, &lt;strong&gt;right_join&lt;/strong&gt;, &lt;strong&gt;inner_join&lt;/strong&gt;, &lt;strong&gt;outer_join&lt;/strong&gt;; as well as the very useful filtering joins &lt;strong&gt;semi_join&lt;/strong&gt; and &lt;strong&gt;anti_join&lt;/strong&gt; (keep and discard what matches, respectively):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
set.seed(4321)
df %&amp;gt;% 
  left_join(df2, by = &amp;quot;species&amp;quot;) %&amp;gt;% 
  select(species, bill_length_mm, mean_bill) %&amp;gt;% 
  sample_n(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 3
##   species   bill_length_mm mean_bill
##   &amp;lt;fct&amp;gt;              &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 Adelie              42.7      38.8
## 2 Chinstrap           47.5      48.8
## 3 Adelie              40.2      38.8
## 4 Adelie              34.6      38.8
## 5 Gentoo              53.4      47.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can for instance subtract the mean bill length from the individual values to get the residuals or to standardise our features.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;pandas&lt;/em&gt;, joining uses the &lt;strong&gt;merge&lt;/strong&gt; operator. You have to specify the type of join via the &lt;em&gt;how&lt;/em&gt; parameter and the join key via &lt;em&gt;on&lt;/em&gt;, like such:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.\
  sample(n = 5, random_state = 24).\
  merge(r.df2, how = &amp;quot;left&amp;quot;, on = &amp;quot;species&amp;quot;).\
  loc[:, [&amp;#39;species&amp;#39;, &amp;#39;bill_length_mm&amp;#39;, &amp;#39;mean_bill&amp;#39;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      species  bill_length_mm  mean_bill
## 0     Gentoo            43.4  47.504878
## 1  Chinstrap            51.7  48.833824
## 2     Gentoo            46.2  47.504878
## 3     Adelie            43.1  38.791391
## 4     Adelie            41.3  38.791391&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to bring in the aforementioned source of confusion: &lt;em&gt;pandas&lt;/em&gt; also has an operator called &lt;strong&gt;join&lt;/strong&gt; which joins by matching indeces, instead of columns, between two tables. This is a pretty pandas-specific convenience shortcut, since it relies on the data frame index. In practice I recommend using &lt;strong&gt;merge&lt;/strong&gt; instead. The little conveniece provided by &lt;strong&gt;join&lt;/strong&gt; is not worth the additional confusion.&lt;/p&gt;
&lt;p&gt;Binding rows and columns in &lt;em&gt;dplyr&lt;/em&gt; uses &lt;strong&gt;bind_rows&lt;/strong&gt; and &lt;strong&gt;bind_cols&lt;/strong&gt;. For the rows, there’s really not much of a practical requirement other than that some of the column names match and that those ideally have the same type (which is not strictly necessary):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
head(df, 2) %&amp;gt;% 
  bind_rows(tail(df, 2) %&amp;gt;% select(year, everything(), -sex))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 8
##   species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g
##   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;           &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 Adelie  Torge…           39.1          18.7              181        3750
## 2 Adelie  Torge…           39.5          17.4              186        3800
## 3 Chinst… Dream            50.8          19                210        4100
## 4 Chinst… Dream            50.2          18.7              198        3775
## # … with 2 more variables: sex &amp;lt;fct&amp;gt;, year &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For columns, it is necessary that the features we’re binding to a tibble have the same number of rows. The big assumption here is that the row orders match (but that can be set beforehand):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R
select(df, species) %&amp;gt;% 
  bind_cols(df %&amp;gt;% select(year)) %&amp;gt;% 
  head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   species  year
##   &amp;lt;fct&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Adelie   2007
## 2 Adelie   2007
## 3 Adelie   2007
## 4 Adelie   2007
## 5 Adelie   2007&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;pandas&lt;/em&gt; equivalent to &lt;strong&gt;bind_rows&lt;/strong&gt; is &lt;strong&gt;append&lt;/strong&gt;. This nomenclature is borrowed from Python’s overall reliance on list operations.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
r.df.head(2).append(r.df.tail(2).drop(&amp;quot;sex&amp;quot;, axis = &amp;quot;columns&amp;quot;), sort = False)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \
## 0       Adelie  Torgersen            39.1           18.7              181.0   
## 1       Adelie  Torgersen            39.5           17.4              186.0   
## 342  Chinstrap      Dream            50.8           19.0              210.0   
## 343  Chinstrap      Dream            50.2           18.7              198.0   
## 
##      body_mass_g     sex    year  
## 0         3750.0    male  2007.0  
## 1         3800.0  female  2007.0  
## 342       4100.0     NaN  2009.0  
## 343       3775.0     NaN  2009.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we also demonstrate how to &lt;strong&gt;drop&lt;/strong&gt; a column from a data frame (i.e. the equivalent to &lt;em&gt;dplyr&lt;/em&gt; &lt;strong&gt;select&lt;/strong&gt; with a minus).&lt;/p&gt;
&lt;p&gt;Binding &lt;em&gt;pandas&lt;/em&gt; columns uses &lt;strong&gt;concat&lt;/strong&gt;, which takes a Python list as its parameter:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Python
pd.concat([r.df.loc[:, &amp;#39;species&amp;#39;], r.df.loc[:, &amp;#39;year&amp;#39;]], axis = &amp;quot;columns&amp;quot;).head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   species    year
## 0  Adelie  2007.0
## 1  Adelie  2007.0
## 2  Adelie  2007.0
## 3  Adelie  2007.0
## 4  Adelie  2007.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that’s it for the basics!&lt;/p&gt;
&lt;p&gt;Of course, there are a number of intricacies and special cases here, but by and large this should serve as a useful list of examples to get you started in &lt;em&gt;pandas&lt;/em&gt; coming from &lt;em&gt;dplyr&lt;/em&gt;, and perhaps also vice versa. Future installments in this series will go beyond &lt;em&gt;dplyr&lt;/em&gt;, but likely also touch back on some aspect of it that are easy to get lost in translation.&lt;/p&gt;
&lt;p&gt;More info:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I have been planning to start this series for several months now; since I’m an R user in a Python-dominated team and often use both languages om any given day. What finally pushed me to overcome my laziness was an initiative by the ever prolific &lt;a href=&#34;https://twitter.com/BenjaminWolfe/status/1320770728028000256&#34;&gt;Benjamin Wolfe&lt;/a&gt; to gather resources for R users interested in Python. If you’re interested to contribute too, just drop by in the slack and say Hi.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next up: most likely &lt;a href=&#34;https://tidyr.tidyverse.org/&#34;&gt;tidyr&lt;/a&gt; and/or &lt;a href=&#34;https://stringr.tidyverse.org/&#34;&gt;stringr&lt;/a&gt; vs pandas. Watch this space.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Animations in the time of Coronavirus</title>
      <link>https://heads0rtai1s.github.io/2020/04/30/animate-map-covid/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2020/04/30/animate-map-covid/</guid>
      <description>
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The first four months of 2020 have been dominated by the Coronavirus pandemic (aka COVID-19), which has transformed global life in an unprecedented way. Societies and economies struggle to adapt to the new conditions and necessary contraints. A reassuringly large fraction of governments around the world continue to take evidence-based approaches to this crisis that are grounded in large scale data collection efforts. Most of this data is being made publicly available and can be studied in real time. This post will describe how to extract and prepare the necessary data to animate the spread of the virus over time within my native country of Germany.&lt;/p&gt;
&lt;p&gt;I have published a pre-processed version of the relevant data for this project as a &lt;a href=&#34;https://www.kaggle.com/headsortails/covid19-tracking-germany&#34;&gt;Kaggle dataset&lt;/a&gt;, together with the geospatial shape files you need to plot the resulting map. This post outlines how to build that dataset from the original source data using a set of &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; tools. Then we will use the &lt;a href=&#34;https://github.com/thomasp85/gganimate&#34;&gt;gganimate&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/sf/index.html&#34;&gt;sf&lt;/a&gt; packages to create animated map visuals.&lt;/p&gt;
&lt;p&gt;Those are the packages we need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs &amp;lt;- c(&amp;#39;dplyr&amp;#39;, &amp;#39;tibble&amp;#39;,      # wrangling
          &amp;#39;stringr&amp;#39;, &amp;#39;readr&amp;#39;,     # strings, input
          &amp;#39;lubridate&amp;#39;, &amp;#39;tidyr&amp;#39;,   # time, wrangling
          &amp;#39;knitr&amp;#39;, &amp;#39;kableExtra&amp;#39;,  # table styling
          &amp;#39;ggplot2&amp;#39;, &amp;#39;viridis&amp;#39;,   # visuals
          &amp;#39;gganimate&amp;#39;, &amp;#39;sf&amp;#39;,      # animations, maps
          &amp;#39;ggthemes&amp;#39;)             # visuals
invisible(lapply(libs, library, character.only = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The COVID-19 data for Germany are being collected by the &lt;a href=&#34;https://www.rki.de/EN/Home/homepage_node.html&#34;&gt;Robert Koch Institute&lt;/a&gt; and can be download through the &lt;a href=&#34;https://npgeo-corona-npgeo-de.hub.arcgis.com/&#34;&gt;National Platform for Geographic Data&lt;/a&gt; (which also hosts an interactive dashboard). The earliest recorded cases are from 2020-01-24. Here we define the corresponding link and read the dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;infile &amp;lt;- &amp;quot;https://opendata.arcgis.com/datasets/dd4580c810204019a7b8eb3e0b329dd6_0.csv&amp;quot;
covid_de &amp;lt;- read_csv(infile, col_types = cols())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This data contains a number of columns which are, unsurprisingly, named in German:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covid_de %&amp;gt;% 
  head(5) %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 5
## Variables: 18
## $ FID                  &amp;lt;dbl&amp;gt; 4281356, 4281357, 4281358, 4281359, 4281360
## $ IdBundesland         &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1
## $ Bundesland           &amp;lt;chr&amp;gt; &amp;quot;Schleswig-Holstein&amp;quot;, &amp;quot;Schleswig-Holstein&amp;quot;,…
## $ Landkreis            &amp;lt;chr&amp;gt; &amp;quot;SK Flensburg&amp;quot;, &amp;quot;SK Flensburg&amp;quot;, &amp;quot;SK Flensbu…
## $ Altersgruppe         &amp;lt;chr&amp;gt; &amp;quot;A15-A34&amp;quot;, &amp;quot;A15-A34&amp;quot;, &amp;quot;A15-A34&amp;quot;, &amp;quot;A15-A34&amp;quot;,…
## $ Geschlecht           &amp;lt;chr&amp;gt; &amp;quot;M&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;M&amp;quot;
## $ AnzahlFall           &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1
## $ AnzahlTodesfall      &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0
## $ Meldedatum           &amp;lt;chr&amp;gt; &amp;quot;2020/03/14 00:00:00&amp;quot;, &amp;quot;2020/03/19 00:00:00…
## $ IdLandkreis          &amp;lt;chr&amp;gt; &amp;quot;01001&amp;quot;, &amp;quot;01001&amp;quot;, &amp;quot;01001&amp;quot;, &amp;quot;01001&amp;quot;, &amp;quot;01001&amp;quot;
## $ Datenstand           &amp;lt;chr&amp;gt; &amp;quot;30.04.2020, 00:00 Uhr&amp;quot;, &amp;quot;30.04.2020, 00:00…
## $ NeuerFall            &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0
## $ NeuerTodesfall       &amp;lt;dbl&amp;gt; -9, -9, -9, -9, -9
## $ Refdatum             &amp;lt;chr&amp;gt; &amp;quot;2020/03/16 00:00:00&amp;quot;, &amp;quot;2020/03/13 00:00:00…
## $ NeuGenesen           &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0
## $ AnzahlGenesen        &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1
## $ IstErkrankungsbeginn &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1
## $ Altersgruppe2        &amp;lt;chr&amp;gt; &amp;quot;nicht übermittelt&amp;quot;, &amp;quot;nicht übermittelt&amp;quot;, &amp;quot;…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following code block reshapes and translates the data to make it better accessible. This includes replacing our beloved German umlauts with simplified diphthongs, creating age groups, and aggregating COVID-19 numbers by county, age group, gender, and date:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covid_de &amp;lt;- covid_de %&amp;gt;% 
  select(state = Bundesland,
         county = Landkreis,
         age_group = Altersgruppe,
         gender = Geschlecht,
         cases = AnzahlFall,
         deaths = AnzahlTodesfall,
         recovered = AnzahlGenesen,
         date = Meldedatum) %&amp;gt;% 
  mutate(date = date(date)) %&amp;gt;% 
  mutate(age_group = str_remove_all(age_group, &amp;quot;A&amp;quot;)) %&amp;gt;% 
  mutate(age_group = case_when(
    age_group == &amp;quot;unbekannt&amp;quot; ~ NA_character_,
    age_group == &amp;quot;80+&amp;quot; ~ &amp;quot;80-99&amp;quot;,
    TRUE ~ age_group
  )) %&amp;gt;% 
  mutate(gender = case_when(
    gender == &amp;quot;W&amp;quot; ~ &amp;quot;F&amp;quot;,
    gender == &amp;quot;unbekannt&amp;quot; ~ NA_character_,
    TRUE ~ gender
  )) %&amp;gt;% 
  group_by(state, county, age_group, gender, date) %&amp;gt;% 
  summarise(cases = sum(cases),
            deaths = sum(deaths),
            recovered = sum(recovered)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  filter(cases &amp;gt;= 0 &amp;amp; deaths &amp;gt;= 0) %&amp;gt;%
  filter(date &amp;lt; today()) %&amp;gt;% 
  mutate(state = str_replace_all(state, &amp;quot;ü&amp;quot;, &amp;quot;ue&amp;quot;)) %&amp;gt;% 
  mutate(state = str_replace_all(state, &amp;quot;ä&amp;quot;, &amp;quot;ae&amp;quot;)) %&amp;gt;% 
  mutate(state = str_replace_all(state, &amp;quot;ö&amp;quot;, &amp;quot;oe&amp;quot;)) %&amp;gt;% 
  mutate(state = str_replace_all(state, &amp;quot;ß&amp;quot;, &amp;quot;ss&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ü&amp;quot;, &amp;quot;ue&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ä&amp;quot;, &amp;quot;ae&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ö&amp;quot;, &amp;quot;oe&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ß&amp;quot;, &amp;quot;ss&amp;quot;)) %&amp;gt;% 
  mutate(county = str_remove(county, &amp;quot;\\(.+\\)&amp;quot;)) %&amp;gt;% 
  mutate(county = str_trim(county)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is a dataset that lists daily (&lt;em&gt;not cumulative!&lt;/em&gt;) cases, deaths, and recovered cases for 6 age groups, gender, and the German counties and their corresponding federal states. Similar to the US, Germany has a federal system in which the 16 federal states have a large amout of legislative power. The German equivalent of the US county is the “Kreis”, which can either be associated with a city (“Stadtkreis” = “SK”) or the country side (“Landkreis” = “LK”). Here only a subset of columns are shown for reasons of clarity:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covid_de %&amp;gt;%
  filter(state == &amp;quot;Sachsen&amp;quot;) %&amp;gt;% 
  select(-deaths, -recovered) %&amp;gt;% 
  head(5) %&amp;gt;% 
  kable() %&amp;gt;% 
  column_spec(1:6, width = c(&amp;quot;15%&amp;quot;, &amp;quot;25%&amp;quot;, &amp;quot;15%&amp;quot;, &amp;quot;10%&amp;quot;, &amp;quot;25%&amp;quot;, &amp;quot;10%&amp;quot;)) %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
state
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
county
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
age_group
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
gender
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
date
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
cases
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
Sachsen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
LK Bautzen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
00-04
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
F
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
2020-03-20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 15%; &#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
Sachsen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
LK Bautzen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
00-04
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
F
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
2020-04-07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 15%; &#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
Sachsen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
LK Bautzen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
00-04
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
M
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
2020-03-21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 15%; &#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
Sachsen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
LK Bautzen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
05-14
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
F
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
2020-03-20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 15%; &#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
Sachsen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
LK Bautzen
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
05-14
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
F
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 15%; &#34;&gt;
2020-03-21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 15%; &#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is the cleaned dataset which is available on &lt;a href=&#34;https://www.kaggle.com/headsortails/covid19-tracking-germany&#34;&gt;Kaggle&lt;/a&gt; as &lt;code&gt;covid_de.csv&lt;/code&gt;. With this data, you can already already slice and analyse Germany’s COVID-19 characteristics by various demographic and geographical features.&lt;/p&gt;
&lt;p&gt;However, for the maps that we’re interested in one more input is missing: shapefiles. A &lt;a href=&#34;https://en.wikipedia.org/wiki/Shapefile&#34;&gt;shapefile&lt;/a&gt; uses a standard vector format for specifying spatial geometries. It packages the map boundary data of the required entities (like countries, federal states) into a small set of related files. For this project, I found publicly available shapefiles on the state and county level provided by Germany’s &lt;a href=&#34;https://www.bkg.bund.de/EN/Home/home.html&#34;&gt;Federal Agency for Cartography and Geodesy&lt;/a&gt;. Both levels are available in the Kaggle dataset. Here I put the county level files (&lt;code&gt;de_county.*&lt;/code&gt;) into a local, static directory.&lt;/p&gt;
&lt;p&gt;Shapefiles can be read into R using the &lt;code&gt;sf&lt;/code&gt; package tool &lt;code&gt;st_read&lt;/code&gt;. In order to soon join them to our COVID-19 data, we need to do a bit of string translating and wrangling again. The &lt;code&gt;tidyr&lt;/code&gt; tool &lt;code&gt;unite&lt;/code&gt; is being used to combine the county type (&lt;code&gt;BEZ in c(&amp;quot;LK&amp;quot;, &amp;quot;SK&amp;quot;)&lt;/code&gt;) and county name into the format we have in our COVID-19 data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shape_county &amp;lt;- st_read(str_c(&amp;quot;../../static/files/&amp;quot;, &amp;quot;de_county.shp&amp;quot;), quiet = TRUE) %&amp;gt;% 
  rename(county = GEN) %&amp;gt;% 
  select(county, BEZ, geometry) %&amp;gt;% 
  mutate(county = as.character(county)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ü&amp;quot;, &amp;quot;ue&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ä&amp;quot;, &amp;quot;ae&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ö&amp;quot;, &amp;quot;oe&amp;quot;)) %&amp;gt;% 
  mutate(county = str_replace_all(county, &amp;quot;ß&amp;quot;, &amp;quot;ss&amp;quot;)) %&amp;gt;% 
  mutate(county = str_remove(county, &amp;quot;\\(.+\\)&amp;quot;)) %&amp;gt;% 
  mutate(county = str_trim(county)) %&amp;gt;% 
  mutate(BEZ = case_when(
    BEZ == &amp;quot;Kreis&amp;quot; ~ &amp;quot;LK&amp;quot;,
    BEZ == &amp;quot;Landkreis&amp;quot; ~ &amp;quot;LK&amp;quot;,
    BEZ == &amp;quot;Stadtkreis&amp;quot; ~ &amp;quot;SK&amp;quot;,
    BEZ == &amp;quot;Kreisfreie Stadt&amp;quot; ~ &amp;quot;SK&amp;quot;
  )) %&amp;gt;% 
  unite(county, BEZ, county, sep = &amp;quot; &amp;quot;, remove = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this stage, there are still some county names that don’t match precisely. It would have been too easy, otherwise. These cases mostly come down to different styles of abbreviations being used for counties with longer names. A scalable way to deal with these wonders of the German language would be &lt;a href=&#34;https://cran.r-project.org/web/packages/fuzzyjoin/&#34;&gt;fuzzy matching&lt;/a&gt; by &lt;a href=&#34;https://cran.r-project.org/web/packages/stringdist/&#34;&gt;string distance&lt;/a&gt; similarities. Here, the number of mismatches is small and I decided to adjust them manually.&lt;/p&gt;
&lt;p&gt;Then, I group everything by &lt;code&gt;county&lt;/code&gt; and &lt;code&gt;date&lt;/code&gt; and sum over the remaining features. One major issue here is that not all counties will report numbers for all days. Those are small areas, after all. In this dataset, these cases are implicitely missing; i.e. the corresponding rows are just not present. It is important to convert those cases into explicitely missing entries: rows that have a count of zero. Otherwise, our eventual map will have “holes” in it for specific days and specific counties. The elegant solution in the code is made possible by the &lt;code&gt;tidyr&lt;/code&gt; function &lt;code&gt;complete&lt;/code&gt;: simply name all the columns for which we want to have all the combinations and specify how they should be filled. This approach applies to any situation where we have a set of features and need a complete grid of all possible combinations.&lt;/p&gt;
&lt;p&gt;Finally, we sum up the cumulative cases and deaths. Here, I also applied a &lt;code&gt;filter&lt;/code&gt; to extract data from March 1st - 31st only, to prevent the animation file from becoming too large. Feel free to expand this to a longer time frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;foo &amp;lt;- covid_de %&amp;gt;% 
  mutate(county = case_when(
    county == &amp;quot;Region Hannover&amp;quot; ~ &amp;quot;LK Region Hannover&amp;quot;,
    county == &amp;quot;SK Muelheim a.d.Ruhr&amp;quot; ~ &amp;quot;SK Muelheim an der Ruhr&amp;quot;,
    county == &amp;quot;StadtRegion Aachen&amp;quot; ~ &amp;quot;LK Staedteregion Aachen&amp;quot;,
    county == &amp;quot;SK Offenbach&amp;quot; ~ &amp;quot;SK Offenbach am Main&amp;quot;,
    county == &amp;quot;LK Bitburg-Pruem&amp;quot; ~ &amp;quot;LK Eifelkreis Bitburg-Pruem&amp;quot;,
    county == &amp;quot;SK Landau i.d.Pfalz&amp;quot; ~ &amp;quot;SK Landau in der Pfalz&amp;quot;,
    county == &amp;quot;SK Ludwigshafen&amp;quot; ~ &amp;quot;SK Ludwigshafen am Rhein&amp;quot;,
    county == &amp;quot;SK Neustadt a.d.Weinstrasse&amp;quot; ~ &amp;quot;SK Neustadt an der Weinstrasse&amp;quot;,
    county == &amp;quot;SK Freiburg i.Breisgau&amp;quot; ~ &amp;quot;SK Freiburg im Breisgau&amp;quot;,
    county == &amp;quot;LK Landsberg a.Lech&amp;quot; ~ &amp;quot;LK Landsberg am Lech&amp;quot;,
    county == &amp;quot;LK Muehldorf a.Inn&amp;quot; ~ &amp;quot;LK Muehldorf a. Inn&amp;quot;,
    county == &amp;quot;LK Pfaffenhofen a.d.Ilm&amp;quot; ~ &amp;quot;LK Pfaffenhofen a.d. Ilm&amp;quot;,
    county == &amp;quot;SK Weiden i.d.OPf.&amp;quot; ~ &amp;quot;SK Weiden i.d. OPf.&amp;quot;,
    county == &amp;quot;LK Neumarkt i.d.OPf.&amp;quot; ~ &amp;quot;LK Neumarkt i.d. OPf.&amp;quot;,
    county == &amp;quot;LK Neustadt a.d.Waldnaab&amp;quot; ~ &amp;quot;LK Neustadt a.d. Waldnaab&amp;quot;,
    county == &amp;quot;LK Wunsiedel i.Fichtelgebirge&amp;quot; ~ &amp;quot;LK Wunsiedel i. Fichtelgebirge&amp;quot;,
    county == &amp;quot;LK Neustadt a.d.Aisch-Bad Windsheim&amp;quot; ~ &amp;quot;LK Neustadt a.d. Aisch-Bad Windsheim&amp;quot;,
    county == &amp;quot;LK Dillingen a.d.Donau&amp;quot; ~ &amp;quot;LK Dillingen a.d. Donau&amp;quot;,
    county == &amp;quot;LK Stadtverband Saarbruecken&amp;quot; ~ &amp;quot;LK Regionalverband Saarbruecken&amp;quot;,
    county == &amp;quot;LK Saar-Pfalz-Kreis&amp;quot; ~ &amp;quot;LK Saarpfalz-Kreis&amp;quot;,
    county == &amp;quot;LK Sankt Wendel&amp;quot; ~ &amp;quot;LK St. Wendel&amp;quot;,
    county == &amp;quot;SK Brandenburg a.d.Havel&amp;quot; ~ &amp;quot;SK Brandenburg an der Havel&amp;quot;,
    str_detect(county, &amp;quot;Berlin&amp;quot;) ~ &amp;quot;SK Berlin&amp;quot;,
    TRUE ~ county
  )) %&amp;gt;% 
  group_by(county, date) %&amp;gt;% 
  summarise(cases = sum(cases),
            deaths = sum(deaths)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  complete(county, date, fill = list(cases = 0, deaths = 0)) %&amp;gt;% 
  group_by(county) %&amp;gt;% 
  mutate(cumul_cases = cumsum(cases),
         cumul_deaths = cumsum(deaths)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  filter(between(date, date(&amp;quot;2020-03-01&amp;quot;), date(&amp;quot;2020-03-31&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have all the ingredients for animating a county-level map of cumulative cases. Here we first define the animation object by specifying &lt;code&gt;geom_sf()&lt;/code&gt; and &lt;code&gt;theme_map()&lt;/code&gt; for the map style, then providing the animation steps column &lt;code&gt;date&lt;/code&gt; to the &lt;code&gt;transition_time()&lt;/code&gt; method. The advantage of &lt;a href=&#34;https://rdrr.io/github/thomasp85/gganimate/man/transition_time.html&#34;&gt;transition_time&lt;/a&gt; is that the length of transitions between steps takes is proportional to the intrinsic time differences. Here, we have a very well behaved dataset and all our steps are of length 1 day. Thus, we could also use &lt;code&gt;transition_states()&lt;/code&gt; directly. However, I consider it good practice to use &lt;code&gt;transition_time&lt;/code&gt; whenever actual time steps are involved; to be prepared for unequal time intervals.&lt;/p&gt;
&lt;p&gt;The animation parameters are provided in the &lt;code&gt;animate&lt;/code&gt; function, such as the transition style from one day to the next (&lt;code&gt;cubic-in-out&lt;/code&gt;), the animation speed (10 frames per s), or the size of the plot. For cumulative animations like this, it’s always a good idea to include an &lt;code&gt;end_pause&lt;/code&gt; freeze-frame, so that the reader can have a closer look at the final state before the loop begins anew:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- shape_county %&amp;gt;% 
  right_join(foo, by = &amp;quot;county&amp;quot;) %&amp;gt;% 
  ggplot(aes(fill = cumul_cases)) +
  geom_sf() +
  scale_fill_viridis(trans = &amp;quot;log1p&amp;quot;, breaks = c(0, 10, 100, 1000)) +
  theme_map() +
  theme(title = element_text(size = 15), legend.text = element_text(size = 12),
        legend.title = element_text(size = 15)) +
  labs(title = &amp;quot;Total COVID-19 cases in Germany: {frame_time}&amp;quot;, fill = &amp;quot;Cases&amp;quot;) +
  transition_time(date)

animate(gg + ease_aes(&amp;#39;cubic-in-out&amp;#39;), fps = 10, end_pause = 25, height = 800, width = round(800/1.61803398875))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2020-04-30-animate-map-covid_files/figure-html/unnamed-chunk-8-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Our final map shows how the number of COVID-19 cases in Germany first started to rise in the South and West, and how they spread to other parts of the country. The geographical middle of Germany appears to be lagging behind in case counts even at later times. Note the logarithmic colour scale.&lt;/p&gt;
&lt;p&gt;More info:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;One caveat: This view does not take into account population density, which makes large cities like Berlin (north-east) stand out more towards the end. My Kaggle dataset currently includes population counts for the state-level only, but I’m planning to add county data in the near future.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you’re looking for further inspiration on how to analyse this dataset then check out the various &lt;a href=&#34;https://www.kaggle.com/headsortails/covid19-tracking-germany/kernels&#34;&gt;Notebooks&lt;/a&gt; (aka “Kernels”) which are associated with it on Kaggle. Kaggle has the big advantage that you can run R or Python scripts and notebooks in a pretty powerful cloud environment; and present your work alongside datasets and competitions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Another Kaggle dataset of mine with daily COVID-19 cases, deaths, and recoveries in the US can be found &lt;a href=&#34;https://www.kaggle.com/headsortails/covid19-us-county-jhu-data-demographics&#34;&gt;here&lt;/a&gt;. This data also has a county-level resolution. It is based on &lt;a href=&#34;https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series&#34;&gt;Johns Hopkins University data&lt;/a&gt; and I’m updating it on a daily basis.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Analysing tweets with the rtweet package</title>
      <link>https://heads0rtai1s.github.io/2020/02/20/rtweet-intro/</link>
      <pubDate>Thu, 20 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2020/02/20/rtweet-intro/</guid>
      <description>
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is a brief post on collecting and analysing tweets. I will show how to use the &lt;a href=&#34;https://github.com/ropensci/rtweet&#34;&gt;rtweet&lt;/a&gt; package to extract Twitter posts about the R community. This ties into last weeks post on &lt;a href=&#34;https://heads0rtai1s.github.io/2020/02/13/rstudio-conf/&#34;&gt;rstudio::conf and community values&lt;/a&gt;, and is also related to my previous &lt;a href=&#34;https://heads0rtai1s.github.io/2020/01/23/rvest-intro-astro/&#34;&gt;intro post on web scraping&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, we load &lt;code&gt;rtweet&lt;/code&gt; and other (&lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt;) packages we will need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs &amp;lt;- c(&amp;#39;dplyr&amp;#39;, &amp;#39;tibble&amp;#39;,      # wrangling
          &amp;#39;stringr&amp;#39;, &amp;#39;rtweet&amp;#39;,    # strings, tweets
          &amp;#39;knitr&amp;#39;, &amp;#39;kableExtra&amp;#39;,  # table styling
          &amp;#39;lubridate&amp;#39;,            # time
          &amp;#39;ggplot2&amp;#39;, &amp;#39;ggthemes&amp;#39;)  # plots
invisible(lapply(libs, library, character.only = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to use the &lt;code&gt;rtweet&lt;/code&gt; package you need a &lt;a href=&#34;https://twitter.com&#34;&gt;Twitter&lt;/a&gt; account and the package needs to be authorised to access that account. The authorisation process is very smooth: just run a command like the one below and an authorisation tab will open in your browser (via &lt;code&gt;rstats2twitter&lt;/code&gt;). Approving the authorisation will store a credential token on your machine (called &lt;code&gt;.rtweet_token.rds&lt;/code&gt;) for a all future requests. This token should end up safe in your home directory, but you might still want to double check that it doesn’t get pushed to any public github repos.&lt;/p&gt;
&lt;p&gt;This is how we search for recent tweets about “rstats” + “community”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rt &amp;lt;- search_tweets(
  &amp;quot;rstats community&amp;quot;, n = 1000, include_rts = FALSE
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the search string, white space acts like “AND” and order doesn’t matter. To search for either term (inclusive) you can use “OR”. To search for specific sequences of terms wrap them in single quotes (e.g. ‘rstats community’). The &lt;code&gt;search_tweets&lt;/code&gt; &lt;a href=&#34;https://www.rdocumentation.org/packages/rtweet/versions/0.7.0/topics/search_tweets&#34;&gt;help page&lt;/a&gt; has further and more detailed query instructions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note: the free Twitter API only allows you to access tweets from the last 6-9 days.&lt;/strong&gt; For anything more you need a paid API. I was originally planning to search for &lt;code&gt;#rstudioconf&lt;/code&gt; tweets, but the conference is 3 weeks ago now. So I guess that’s what I get for taking so long with this post.&lt;/p&gt;
&lt;p&gt;Twitter’s &lt;em&gt;request limit is 18k tweets per 15 minute interval&lt;/em&gt;. To download more tweets in one go you can set the parameter &lt;code&gt;retryonratelimit = TRUE&lt;/code&gt;, but you would have to wait 15 min for the retry. This is a convenience function, so that you don’t need to run individual 18k queries and stack them together by yourself.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;search_tweets&lt;/code&gt; we can derive pretty much all the information about a tweet, its author, and any quoted tweets: the full text, retweet count, hashtags, mentions, author bio; you name it. A total of 90 columns. Not all of them will always have values, but it looks like a very comprehensive set of features to me. Here is a small subset of those features: the date-time of the tweet (in UT), the user name, and the number of followers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rt %&amp;gt;% 
  select(tweeted = created_at,
         screen_name,
         followers = followers_count) %&amp;gt;% 
  head(5) %&amp;gt;% 
  kable() %&amp;gt;% 
  column_spec(1:3, width = c(&amp;quot;45%&amp;quot;, &amp;quot;35%&amp;quot;, &amp;quot;20%&amp;quot;)) %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
tweeted
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
screen_name
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
followers
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 45%; &#34;&gt;
2020-02-20 18:32:47
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 45%; &#34;&gt;
danicassol
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 45%; &#34;&gt;
157
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 45%; &#34;&gt;
2020-02-20 17:51:03
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 45%; &#34;&gt;
LK_Fitzpatrick
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 45%; &#34;&gt;
511
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 45%; &#34;&gt;
2020-02-20 17:40:19
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 45%; &#34;&gt;
PyData
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 45%; &#34;&gt;
48209
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 45%; &#34;&gt;
2020-02-20 16:34:52
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 45%; &#34;&gt;
rOpenSci
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 45%; &#34;&gt;
28207
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 45%; &#34;&gt;
2020-02-20 13:24:03
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 45%; &#34;&gt;
GiusCalamita
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 45%; &#34;&gt;
38
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We’ll conclude with some (small number) statistics about “rstats” “community” posts. The &lt;a href=&#34;https://rtweet.info/&#34;&gt;rtweet docs&lt;/a&gt; include many more examples and inspiration for projects. This is the frequency of tweets over the last week:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rt %&amp;gt;% 
  mutate(date = date(created_at)) %&amp;gt;% 
  count(date) %&amp;gt;% 
  ggplot(aes(date, n)) +
  geom_line(col = &amp;quot;blue&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;) +
  theme_fivethirtyeight() +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  ggtitle(&amp;quot;Daily number of tweets about the &amp;#39;rstats community&amp;#39;&amp;quot;,
          subtitle = &amp;quot;Extracted using the rtweet package&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2020-02-20-rtweet-intro_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The notable spike on Feb 12 was caused by &lt;code&gt;@kierisi&lt;/code&gt; asking about people’s experiences with the community. I’m looking forward to her blog post:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;I&amp;#39;m working on a presentation about the R Community (yes, yes I&amp;#39;ll make a blog post!) and while I have *my* opinions, I&amp;#39;d really love to showcase your experiences!&lt;br&gt;&lt;br&gt;What motivated you to join the &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; community? Why do you participate? What makes you stay?&lt;br&gt;&lt;br&gt;(DMs are OK, too!) &lt;a href=&#34;https://t.co/7h3xKZ0Cg2&#34;&gt;pic.twitter.com/7h3xKZ0Cg2&lt;/a&gt;&lt;/p&gt;&amp;mdash; Jesse Mostipak 🦦 (@kierisi) &lt;a href=&#34;https://twitter.com/kierisi/status/1227666597860577281?ref_src=twsrc%5Etfw&#34;&gt;February 12, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</description>
    </item>
    
    <item>
      <title>rstudio::conf retrospective</title>
      <link>https://heads0rtai1s.github.io/2020/02/13/rstudio-conf/</link>
      <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2020/02/13/rstudio-conf/</guid>
      <description>


&lt;p&gt;Two weeks ago, I was fortunate to attend my very first &lt;a href=&#34;https://rstudio.com/conference/&#34;&gt;Rstudio conference&lt;/a&gt;. Spoiler: it was an amazing event - packed to the brim with new ideas, tools, impressions, and most important of all: smart and kind people. This was the first time that San Francisco was hosting “rstudio::conf” (the affectionate shorthand is mirroring R’s package::tool syntax). Having moved to the Bay Area not long ago I really counted myself lucky to have such a promising event in my neighbourhood.&lt;/p&gt;
&lt;p&gt;I have written about the importance of a strong and supportive community once before: in &lt;a href=&#34;https://heads0rtai1s.github.io/2019/04/19/great-community-kaggledays19/&#34;&gt;last year’s post&lt;/a&gt; on meeting my favourite &lt;a href=&#34;https://www.kaggle.com&#34;&gt;Kaggle&lt;/a&gt; crowd for the first time in real life. The rstudio::conf confirmed my belief that progress in any field is greatly boosted by having friendly and competent people around to support you and lift you up.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://heads0rtai1s.github.io/pics/rstudioconf20_1.jpg&#34; alt=&#34;rstudio::conf opening; with about 20% of the audience visible&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;em&gt;rstudio::conf opening; with about 20% of the audience visible&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Nothing exemplifies this attitude better than the way in which the R community has embraced the &lt;a href=&#34;https://www.ericholscher.com/blog/2017/aug/2/pacman-rule-conferences/&#34;&gt;Pac-Man rule&lt;/a&gt;: “When standing as a group of people, always leave room for 1 person to join your group.” In other words, if you’re talking as a group then don’t stand in a closed circle but leave a bit of free space for anyone who’d like to participate in the conversation. At rstudio::conf I saw this concept work extremely well and it allowed me (and many others) to quickly connect with new people and have fun and inspiring conversations. It may have also helped that we all had at least one thing in common to talk about.&lt;/p&gt;
&lt;p&gt;Another key aspect the R community excels in is to actively champion diversity and inclusion. This diversity within the community is mirrored by, and without a doubt contributes to, the richness and variety of the growing R ecosystem. In a tech sector that is still predominantly white and/or male, I strongly believe that the more a community welcomes all strata of society the better it can evolve and strive.&lt;/p&gt;
&lt;p&gt;And the R universe, including the tidyverse, is growing rapidly indeed. I learned lots of new tricks from seasoned expeRts (like the unpacking operator &lt;code&gt;%&amp;lt;-%&lt;/code&gt; from the &lt;a href=&#34;https://rdrr.io/cran/zeallot/man/operator.html&#34;&gt;zeallot package&lt;/a&gt;) and discovered so many cool packages I hadn’t been aware of. To name only three, in no particular order:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/HenrikBengtsson/future&#34;&gt;future&lt;/a&gt;: the promising heir to the popular &lt;a href=&#34;https://cran.r-project.org/web/packages/foreach/index.html&#34;&gt;foreach package&lt;/a&gt; in the parallel processing domain; developed by &lt;a href=&#34;https://github.com/HenrikBengtsson&#34;&gt;Henrik Bengtsson&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/EvaMaeRey/flipbookr&#34;&gt;flipbookr&lt;/a&gt;: &lt;a href=&#34;https://github.com/EvaMaeRey&#34;&gt;Gina Reynolds’&lt;/a&gt; side-by-side and step-by-step tool to illustrate how code turns into output. Great resource for teaching and demonstrations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/tidymodels/tune&#34;&gt;tune&lt;/a&gt;: the newest &lt;a href=&#34;https://github.com/tidymodels&#34;&gt;tidymodels&lt;/a&gt; family member; responsible for tuning hyperparameters. Presented by mastermind &lt;a href=&#34;https://github.com/topepo&#34;&gt;Max Kuhn&lt;/a&gt;, it was great to see how the individual tidymodels components come together to soon form a powerful and consistent machine learning workflow. Exciting times ahead!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But of course the most important consequence of the healthy expansion of the R ecosystem is the existence of many new hex stickers ;-) . This was my first opportunity to become a hunter and gatherer of those colourful collectibles, and I managed to ultimately design a whole new look for my laptop:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://heads0rtai1s.github.io/pics/rstudioconf20_2.jpg&#34; alt=&#34;I’m gonna need a bigger laptop&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;em&gt;I’m gonna need a bigger laptop&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;More resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For all your R and Data Science needs check out the R for Data Science (R4DS) online learning community on &lt;a href=&#34;https://twitter.com/R4DScommunity&#34;&gt;twitter&lt;/a&gt; and slack!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You’re probably not surprised to hear that there is an R package that lets you design your own hex stickers. It is called &lt;a href=&#34;https://github.com/GuangchuangYu/hexSticker&#34;&gt;hexSticker&lt;/a&gt;. That name is probably even less surprising.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you’re interested to meet fellow R enthusiasts you can &lt;a href=&#34;https://jumpingrivers.github.io/meetingsR/&#34;&gt;find here&lt;/a&gt; a pretty comprehensive list of events and user groups.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Web Scraping with rvest &#43; Astro Throwback</title>
      <link>https://heads0rtai1s.github.io/2020/01/23/rvest-intro-astro/</link>
      <pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2020/01/23/rvest-intro-astro/</guid>
      <description>
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In my first post of the year I will provide &lt;strong&gt;a gentle introduction to web scraping&lt;/strong&gt; with the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; package &lt;a href=&#34;https://github.com/tidyverse/rvest&#34;&gt;rvest&lt;/a&gt;. As the package name pun suggests, &lt;a href=&#34;https://en.wikipedia.org/wiki/Web_scraping&#34;&gt;web scraping&lt;/a&gt; is the process of harvesting, or extracting, data from websites. The extraction process is greatly simplified by the fact that websites are predominantly built using &lt;a href=&#34;https://en.wikipedia.org/wiki/HTML&#34;&gt;HTML&lt;/a&gt; (= Hyper Text Markup Language), which essentially uses a set of formatting and structuring rules to tell your browser how to display a website. By navigating the website structure and requesting the desired formatting element (e.g. a certain image, hyperlink, or bold-face text) we can scrape the underlying information. With more and more information stored online, web scraping is a valuable tool in your data science tool box. &lt;strong&gt;There’s not much background required to understand this post, other than &lt;code&gt;dplyr&lt;/code&gt; basics and a bit of HTML.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This post will also provide a throwback to my former life as astronomer. The dataset we will scrape is related to an ongoing long-term project of observing a &lt;a href=&#34;https://www.universetoday.com/141536/this-star-has-been-going-nova-every-year-for-millions-of-years/&#34;&gt;fascinating exploding star&lt;/a&gt; in our neighbour galaxy &lt;a href=&#34;https://en.wikipedia.org/wiki/Andromeda_Galaxy&#34;&gt;Andromeda&lt;/a&gt;. Our star is one of a kind: it belongs to the class of &lt;a href=&#34;https://en.wikipedia.org/wiki/Nova&#34;&gt;Novae&lt;/a&gt; and erupts roughly once a year - much more frequently than any other nova.&lt;/p&gt;
&lt;p&gt;The dataset is publicly available in the archives of NASA’s &lt;a href=&#34;https://swift.gsfc.nasa.gov/&#34;&gt;Neil Gehrels Swift Observatory&lt;/a&gt;. Fun fact: &lt;strong&gt;all&lt;/strong&gt; data of publicly funded astronomical observatories (e.g. from all &lt;a href=&#34;https://www.nasa.gov/&#34;&gt;NASA&lt;/a&gt; or &lt;a href=&#34;https://www.esa.int/&#34;&gt;ESA&lt;/a&gt; telescopes) is freely available to anyone (due to that very public funding); sometimes immediately after it was taken, sometimes a year or so later to give the observing team a head start. Lots of discovery space out there.&lt;/p&gt;
&lt;p&gt;To begin, as usual we load the required packages first:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs &amp;lt;- c(&amp;#39;dplyr&amp;#39;, &amp;#39;tibble&amp;#39;,      # wrangling
          &amp;#39;stringr&amp;#39;, &amp;#39;rvest&amp;#39;,     # strings, scraping
          &amp;#39;knitr&amp;#39;, &amp;#39;kableExtra&amp;#39;,  # table styling
          &amp;#39;lubridate&amp;#39;, &amp;#39;ggplot2&amp;#39;) # time, plots
invisible(lapply(libs, library, character.only = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the link for the page we’re starting with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;target_link &amp;lt;- &amp;quot;https://www.swift.ac.uk/swift_portal/getobject.php?name=m31n+2008-12a&amp;amp;submit=Search+Names&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We know that our star has the prosaic catalogue name “M31N 2008-12a” (or at least I still know that name in my sleep), so this is what we will plug into the search name syntax of the &lt;a href=&#34;https://www.swift.ac.uk/&#34;&gt;UK Swift Science Data Centre&lt;/a&gt;. Below is the result:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://heads0rtai1s.github.io/pics/swift_name_search.jpg&#34; alt=&#34;Search result for M31N 2008-12a&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;em&gt;Search result for M31N 2008-12a&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Among some other elements, this page has a prominent table which contains all &lt;em&gt;Swift&lt;/em&gt; observations that covered the position of our object on the sky. As you see, not all of those were pointed at M31N 2008-12a; some serendipitous observations featured our explosive friend somewhere near the edge of the field of view. But we’re not picky, we take all we can get. We download the entire page using the &lt;code&gt;read_html&lt;/code&gt; tool and store it in the variable &lt;code&gt;target_page&lt;/code&gt;. This might take a moment or two:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;target_page &amp;lt;- read_html(target_link)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result isn’t very pretty, yet. It just shows us the HTML header and the HTML body and a whole lot of text and formatting associated with it. (I’m not showing it here because the output confuses some rss readers.)&lt;/p&gt;
&lt;p&gt;Luckily, we don’t have to mess around with the HTML text in this unstructured form. In order to find out exactly which element we need, we can use the &lt;em&gt;Inspector&lt;/em&gt; tool of our browsers. Chances are, you just need to press &lt;code&gt;F12&lt;/code&gt; to launch it.&lt;/p&gt;
&lt;p&gt;Now you should see something like this, with the Inspector tool taking up part of the screen. The Inspector window (here on the right) shows the HTML code of the website in the upper half of the window (plus some stuff we don’t care about in this post in the lower half):&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://heads0rtai1s.github.io/pics/swift_name_search_inspect.jpg&#34; alt=&#34;Website with Inspector tool&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;em&gt;Website with Inspector tool&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To identify the &lt;em&gt;path&lt;/em&gt; for the element we need (here the big table) we will go through the following 3 steps, which I’ve marked in the image above:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Click on the arrow in the top left corner of the Inspector window to get the element picker [1].&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Go to the website window and position your mouse pointer so that the selection mask covers the entire element you’re interested in, but not more [2]. In the image above, my mouse pointer masks the entire table with a blue shade but doesn’t select “Coordinate style” or the “Local Archives” box on the top right. (Ignore the yellow shade; there are no elements there.) Once you’re happy with the selection confirm it with a left click.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Upon clicking, the Inspector code window will automatically navigate to the selected element and highlight it in blue [3]. Now all you have to do is go anywhere on that blue background, do a right click, and select “Copy -&amp;gt; XPath”. In our case, the result is &lt;code&gt;/html/body/div/table&lt;/code&gt;. This is the path that navigates through the HTML hierarchy to our table.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now we can use the tool &lt;code&gt;html_nodes&lt;/code&gt; to extract the element. This gives us the HTML code; including the formatting. Because it is a table, we then use &lt;code&gt;html_table&lt;/code&gt; to extract only the table itself. Other extraction tools include &lt;code&gt;html_text&lt;/code&gt; or &lt;code&gt;html_form&lt;/code&gt;. This &lt;a href=&#34;https://rpubs.com/ryanthomas/webscraping-with-rvest&#34;&gt;great tutorial&lt;/a&gt; by Ryan Thomas contains a list of the various extraction functions (and much more).&lt;/p&gt;
&lt;p&gt;The rest of the code cell below turns the table into a tidy tibble and does a bit of formatting. Then we print the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;target &amp;lt;- target_page %&amp;gt;% 
  html_nodes(xpath = &amp;quot;/html/body/div/table&amp;quot;) %&amp;gt;% 
  html_table()

target &amp;lt;- target[[1]] %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  rename(target = Name,
         num_obs = `Number of observations`,
         target_id = `Target ID`,
         coords = `RA, Dec (J2000)`) %&amp;gt;% 
  select(-`Download(target)`) %&amp;gt;% 
  filter(target != &amp;quot;All of the above&amp;quot;)

target %&amp;gt;% 
  select(-coords) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
target
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
num_obs
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
target_id
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
M31N2008-12a
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00010498
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
M31N2008-12a
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00010965
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
M31N2008-12a
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
19
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00012176
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
M31N2012-10a
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
190
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00032613
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
M31N2013-11e
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00033061
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SWIFTJ0045.2+4151
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00033800
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
M31UVOTSurvey2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00034903
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
M31_C7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00037722
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
M31_Field_E
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00088182
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
M31_Field_G
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00088184
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SWIFT J0045.2+4151
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00633105
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can easily confirm we successfully extracted the entire table. Note: since this is an ongoing project and our star subject is prone to roughly yearly eruptions you can expect some of these numbers to change within less than a year from now (Jan 2020). Some stellar evolutions take billions of years, some are in a bit more of a hurry.&lt;/p&gt;
&lt;p&gt;But enough poetry - what have we actually done here? Well, the interesting part of the exercise was extracting the &lt;code&gt;target_id&lt;/code&gt; values. You see: each position or object that &lt;em&gt;Swift&lt;/em&gt; observes in the sky gets its own unique target ID; and sometimes even more than one (for internal housekeeping reasons). In order to get all the information about the 21 or 19 or 190 individual observations associated with a target ID we need to query a second website with these very same IDs.&lt;/p&gt;
&lt;p&gt;Before that: a quick primer on &lt;strong&gt;scraping best practices:&lt;/strong&gt; first and foremost: &lt;strong&gt;respect the wishes and resources of the server&lt;/strong&gt;. This means don’t overwhelm the server with lots of scraping requests in a very short time. Besides being rather impolite, such high-frequency requests will likely get your IP blocked. Most websites list the rules for scrapers and other bots in their &lt;code&gt;robots.txt&lt;/code&gt; file (e.g. &lt;a href=&#34;https://twitter.com/robots.txt&#34;&gt;twitter&lt;/a&gt;). Some sites may disallow scraping entirely. As always: be kind.&lt;/p&gt;
&lt;p&gt;So this first exercise was primarily a preparation for building the following link:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obsid_link_prefix &amp;lt;- &amp;quot;https://www.swift.ac.uk/archive/selectseq.php?tid=&amp;quot;
obsid_link_suffix &amp;lt;- &amp;quot;&amp;amp;source=obs&amp;amp;name=m31n%202008-12a&amp;amp;reproc=1&amp;quot;

obsid_link_targets &amp;lt;- target %&amp;gt;% 
  mutate(target_id = str_sub(target_id, start = 4)) %&amp;gt;% 
  pull(target_id) %&amp;gt;% 
  str_c(collapse = &amp;quot;,&amp;quot;)

obsid_link &amp;lt;- str_c(obsid_link_prefix,
                    obsid_link_targets,
                    obsid_link_suffix)

obsid_link&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;https://www.swift.ac.uk/archive/selectseq.php?tid=10498,10965,12176,32613,33061,33800,34903,37722,88182,88184,33105&amp;amp;source=obs&amp;amp;name=m31n%202008-12a&amp;amp;reproc=1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You see that the link contains all the &lt;code&gt;target_id&lt;/code&gt; values that we just scraped from the big table. Following this link gets us to an even bigger table which lists all the individual observations for each &lt;code&gt;target_id&lt;/code&gt; together with some meta data. From that page, we could even directly download the data and start playing with it.&lt;/p&gt;
&lt;p&gt;For now, we will follow the exact same strategy as above: we scrape the page, extract the table using its XPath, turn it into a tidy tibble, and apply some slight adjustments. If you feel like doing a mini challenge then try to find the relevant XPath (or even build the entire extraction) without looking at the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# download the data
obsid_page &amp;lt;- read_html(obsid_link)

# copy the xpath from the site&amp;#39;s html, extract table
obsid &amp;lt;- obsid_page %&amp;gt;% 
  html_nodes(xpath = &amp;#39;/html/body/div/form/div/div[3]/table&amp;#39;) %&amp;gt;% 
  html_table()

# table is first element, turn into tibble
obsid &amp;lt;- obsid[[1]] %&amp;gt;% 
  as_tibble(.name_repair = &amp;quot;unique&amp;quot;)

# change names, remove rows for download all of 1 target ID
obsid &amp;lt;- obsid %&amp;gt;% 
  select(-...1) %&amp;gt;% 
  rename(obsid = ObsID,
         hea_ver = `HEASoft Ver`,
         start_time = `Start time (UT)`,
         exp_xrt = `XRT exposure (s)`) %&amp;gt;% 
  filter(!str_detect(hea_ver, &amp;quot;000&amp;quot;)) %&amp;gt;% 
  select(-hea_ver) %&amp;gt;% 
  mutate(obsid = str_c(&amp;quot;000&amp;quot;, as.character(obsid))) %&amp;gt;% 
  mutate(target_id = str_sub(obsid, 4, 8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And those are the first 5 rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obsid %&amp;gt;% 
  head(5) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
obsid
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
start_time
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
exp_xrt
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
target_id
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00010498001
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-01-01T05:07:09
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
990.4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
10498
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00010498002
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-01-02T08:26:27
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
995.4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
10498
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00010498003
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-01-03T19:20:57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
682.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
10498
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00010498004
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-01-04T11:31:57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1271.2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
10498
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
00010498005
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-01-05T11:22:57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5218.7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
10498
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We now have an &lt;code&gt;ObsID&lt;/code&gt;, which uniquely identifies each observation, the start date &amp;amp; time of the observation, the &lt;code&gt;target_id&lt;/code&gt; from the previous table (which is part of the &lt;code&gt;obsid&lt;/code&gt;), and also &lt;code&gt;exp_xrt&lt;/code&gt;: the exposure time of &lt;em&gt;Swift&lt;/em&gt;’s &lt;a href=&#34;https://swift.gsfc.nasa.gov/about_swift/xrt_desc.html&#34;&gt;X-ray Telescope&lt;/a&gt;. This exposure time measures how long the X-ray detector of the telescope was exposed to light from the star. Units are in seconds.&lt;/p&gt;
&lt;p&gt;Another fun fact: X-ray astronomers like me love it to measure (exposure) time in &lt;em&gt;kilo seconds&lt;/em&gt; (ks), i.e. &lt;span class=&#34;math inline&#34;&gt;\(10^3~s\)&lt;/span&gt;. This could be considered the ultimate triumph of the metric system (who needs minutes or hours anyway?), but then we’d also use weird units like &lt;a href=&#34;https://en.wikipedia.org/wiki/Erg&#34;&gt;erg&lt;/a&gt; or, and I kid you not, &lt;a href=&#34;https://en.wikipedia.org/wiki/Crab_(unit)&#34;&gt;Crab&lt;/a&gt;. So, overall it’s probably a tie when it comes to the progress of metric units … .&lt;/p&gt;
&lt;p&gt;Anyway, here’s some plots on what that data we just scraped tells us about the X-ray observations. First, the number of observations per year up to now:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;foo &amp;lt;- obsid %&amp;gt;% 
  mutate(start_date = date(start_time)) %&amp;gt;% 
  mutate(year = as.integer(year(start_date)),
         month = month(start_date, label = TRUE, abbr = TRUE))

foo %&amp;gt;% 
  count(year) %&amp;gt;% 
  ggplot(aes(year, n, fill = n)) +
  geom_col() +
  geom_label(aes(label = n), col = &amp;quot;black&amp;quot;) +
  scale_fill_viridis_c(option = &amp;quot;viridis&amp;quot;,
                       begin = 0.5, end = 0.9) +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  scale_x_continuous(breaks = seq(min(foo$year),
                                  max(foo$year), 1)) +
  labs(x = &amp;quot;Year&amp;quot;, y = &amp;quot;Number of observations&amp;quot;,
       caption = &amp;quot;The M31N 2008-12a monitoring collaboration&amp;quot;) +
  ggtitle(&amp;quot;Neil Gehrels Swift Observatory coverage of nova M31N 2008-12a&amp;quot;,
          subtitle = &amp;quot;Targeted observations began in 2013. In 2015 we conducted the\nX-ray flash monitoring campaign. In 2017 the eruption happened on NYE.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2020-01-23-rvest-intro-swift_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations prior to 2013 did not detect the eruptions; in 2013 our targeted campaign &lt;a href=&#34;https://arxiv.org/abs/1401.2904&#34;&gt;began&lt;/a&gt;. 2015 was an odd year, because we were running a separate high-frequency campaign trying to catch an elusive &lt;a href=&#34;https://arxiv.org/abs/1607.07985&#34;&gt;X-ray flash&lt;/a&gt;, which had never been observed in any object. Unfortunately, it still hasn’t. In 2017 the eruption had the nerves to happen on New Year’s Eve, so almost all observations happened in early 2018 (and our NYE parties briefly got a bit hectic). The 2020 eruption will probably happen in October, but don’t bet too much money on it.&lt;/p&gt;
&lt;p&gt;If you look at cumulative exposure time instead of number of observations you see that in 2015 most of those 112 observations were much shorter than usual:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;foo %&amp;gt;% 
  group_by(year) %&amp;gt;% 
  summarise(exp_xrt = sum(exp_xrt)/1e3) %&amp;gt;% 
  ggplot(aes(year, exp_xrt, fill = exp_xrt)) +
  geom_col() +
  #geom_label(aes(label = n), col = &amp;quot;black&amp;quot;) +
  scale_fill_viridis_c(option = &amp;quot;magma&amp;quot;,
                       begin = 0.5, end = 0.9) +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  scale_x_continuous(breaks = seq(min(foo$year),
                                  max(foo$year), 1)) +
  labs(x = &amp;quot;Year&amp;quot;, y = &amp;quot;Combined XRT Exposure [ks]&amp;quot;,
       caption = &amp;quot;The M31N 2008-12a monitoring collaboration&amp;quot;) +
  ggtitle(&amp;quot;Neil Gehrels Swift Observatory coverage of nova M31N 2008-12a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2020-01-23-rvest-intro-swift_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Up to Jan 2020, we have used &lt;em&gt;Swift&lt;/em&gt; to look at M31N 2008-12a in the X-rays for a total of almost exactly 600 ks. Which is more than half a &lt;em&gt;mega second&lt;/em&gt;. Now that’s an impressive unit. Coincidentally, with Rstudio::conf 2020 coming up in San Francisco next week, my next post might happen within the next mega second. Watch this space ;-)&lt;/p&gt;
&lt;p&gt;More info:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;While rvest works great for static websites, sometimes you encounter pages that are built dynamically or require interaction like filling in a form or clicking a button. In those cases, &lt;a href=&#34;https://github.com/ropensci/RSelenium&#34;&gt;Rselenium&lt;/a&gt; is the right tool. I might do a post on it in the future.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Shout-out to my good friend and colleague &lt;a href=&#34;https://twitter.com/mattdarnley&#34;&gt;Matt Darnley&lt;/a&gt;, together with whom I built and led this astro project for several years and who continues to do exciting work at the frontiers of astrophysics. He’s a great person to follow if you want to learn more about exploding stars.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you’re interested you can also check out our recent &lt;a href=&#34;https://www.nature.com/articles/s41586-018-0825-4?&#34;&gt;Nature publication&lt;/a&gt;, led by Matt, for another highlight produced by the mercurial nova M31N 2008-12a.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you are an astronomy enthusiast and want to contribute to cutting edge research on variable stars, here’s my advice: join the international &lt;a href=&#34;https://www.aavso.org/&#34;&gt;AAVSO&lt;/a&gt;. Whether you have a telescope, some binoculars, or no instruments at all; they have tons of advice on how anyone can get involved in variable star astronomy.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>R Shiny for beginners: annotated starter code</title>
      <link>https://heads0rtai1s.github.io/2019/12/05/shiny-starter-code/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2019/12/05/shiny-starter-code/</guid>
      <description>


&lt;p&gt;This week I decided to get started with the &lt;a href=&#34;https://shiny.rstudio.com&#34;&gt;R shiny&lt;/a&gt; package for interactive web applications. As an absolute beginner, I want to document my learning journey in the hope that it will be useful for other first-time shiny users.&lt;/p&gt;
&lt;p&gt;This post assumes some basic familiarity with &lt;a href=&#34;https://www.r-project.org&#34;&gt;R&lt;/a&gt; and the &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;tidyverse&lt;/a&gt;, but no prior knowledge of shiny is required. The content is digested from the &lt;a href=&#34;https://shiny.rstudio.com/tutorial/&#34;&gt;official shiny tutorial&lt;/a&gt; which is great and definitely worth checking out for more details. All credit goes to them; I’m just trying to boil it down to the essentials to get you started within minutes.&lt;/p&gt;
&lt;p&gt;Below is the complete code for my first shiny app. Only 56 lines (a good chunk of which are comments and styling) in hopefully readable formatting. I considered it fitting to base it on the classic coin flip experiment which results in either Heads or Tails:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# preparations; required libraries
library(shiny)
library(dplyr)
library(tibble)
library(stringr)
library(ggplot2)

# the post url
post &amp;lt;- &amp;quot;https://heads0rtai1s.github.io/2019/12/05/shiny-starter-code/&amp;quot;

# user interface elements and layout
ui &amp;lt;- fluidPage(
  titlePanel(&amp;quot;Heads or Tails&amp;quot;),
  sidebarLayout(
    sidebarPanel(
      
      sliderInput(inputId = &amp;quot;n&amp;quot;, label = &amp;quot;Number of flips:&amp;quot;,
                  min = 10, max = 1000, value = 500),
      sliderInput(inputId = &amp;quot;prob&amp;quot;, label = &amp;quot;Success rate:&amp;quot;,
                  min = 0, max = 1, value = 0.5),
      
      tags$div(tags$p(HTML(&amp;quot;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;
                        Find the annotated code&amp;quot;)),
               tags$a(href=post, &amp;quot;in this blog post.&amp;quot;))
      
    ),
    mainPanel(plotOutput(outputId = &amp;quot;bars&amp;quot;))
  )
)

# server-side computations
server &amp;lt;- function(input, output) {
  
  # the bar plot
  output$bars &amp;lt;- renderPlot({
    
    # most of this is for ggplot2; note the input$x syntax
    flips &amp;lt;- tibble(flips = rbinom(input$n, 1, input$prob)) %&amp;gt;% 
      mutate(flips = if_else(flips == 1, &amp;quot;Heads&amp;quot;, &amp;quot;Tails&amp;quot;))  
    
    flips %&amp;gt;% 
      count(flips) %&amp;gt;% 
      ggplot(aes(flips, n, fill = flips)) +
      geom_col() +
      geom_label(aes(flips, n, label = n), size = 5) +
      theme(legend.position = &amp;quot;none&amp;quot;,
            axis.text = element_text(size = 15)) +
      labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;) +
      ggtitle(str_c(&amp;quot;Results of &amp;quot;, input$n,
                    &amp;quot; flips with Heads probability &amp;quot;,
                    sprintf(&amp;quot;%.2f&amp;quot;, input$prob)))
  })
}

# run it all
shinyApp(ui = ui, server = server)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All you need to do at this stage is to (have the required libraries installed and) copy/paste the code above into an active R session. Try it out!&lt;/p&gt;
&lt;p&gt;This is the result you will get:&lt;/p&gt;
&lt;iframe src=&#34;https://headsortails.shinyapps.io/headsortails/&#34; width=&#34;900&#34; height=&#34;500&#34; frameborder=&#34;no&#34; scrolling=&#34;no&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;This app is embedded via &lt;a href=&#34;https://headsortails.shinyapps.io/headsortails/&#34;&gt;shinyapps.io&lt;/a&gt;. More about that later.&lt;/p&gt;
&lt;p&gt;The app allows you to choose the number of coin flips as well as the probability for Heads using slider bars. It visualises the resulting total numbers of Heads vs Tails as a reactive bar plot. Given the functionality of this app, 56 lines is not too bad, is it? Let’s dissect the code element by element!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Preparations:&lt;/strong&gt; Before we get to the interesting parts, the first five lines define and load the packages the script needs. This is unrelated to shiny (other than loading it):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(shiny)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note, that shiny web apps on shinyapps.io apparently need explicit &lt;code&gt;library&lt;/code&gt; calls and that my &lt;a href=&#34;https://heads0rtai1s.github.io/2019/11/07/tidy-curly-pivot-leaflet/&#34;&gt;normal approach&lt;/a&gt; of using &lt;code&gt;invisible(lapply())&lt;/code&gt; led to some confusing errors before I figured it out. Besides the libraries, I’m also including the url for this post as part of the preparation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The shiny code&lt;/strong&gt; is structured into two main elements: (i) a &lt;strong&gt;user interface (UI)&lt;/strong&gt; definition and layout, and (ii) the &lt;strong&gt;server-side computations&lt;/strong&gt; producing the data for plots (or tables, or other output elements). At the end, there is always a &lt;strong&gt;call to the &lt;code&gt;shinyApp&lt;/code&gt; function&lt;/strong&gt; which renders the whole thing.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;UI setup&lt;/strong&gt; starts with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ui &amp;lt;- fluidPage(&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which defines the internal name of the UI as &lt;code&gt;ui&lt;/code&gt; (&lt;em&gt;very surprising; I know&lt;/em&gt;). The &lt;a href=&#34;https://shiny.rstudio.com/reference/shiny/0.14/fluidPage.html&#34;&gt;fluidPage&lt;/a&gt; environment creates an output html that automatically adjusts to the size and shape of your viewer window. This seems to be the layout you would choose most often. The 2 alternatives are a &lt;a href=&#34;https://shiny.rstudio.com/reference/shiny/0.14/fixedPage.html&#34;&gt;fixedPage&lt;/a&gt; or a &lt;a href=&#34;https://shiny.rstudio.com/reference/shiny/0.14/navbarPage.html&#34;&gt;navbarPage&lt;/a&gt; which gives you a top-level navigation bar.&lt;/p&gt;
&lt;p&gt;Inside our &lt;code&gt;fluidPage&lt;/code&gt; we have the UI elements. The first one gives your app a title:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;titlePanel(&amp;quot;Heads or Tails&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nothing too complex here. The next element is the &lt;code&gt;sidebarLayout&lt;/code&gt;; as in “a layout that contains a sidebar” (as opposed to “a layout for the sidebar only”).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sidebarLayout(
    sidebarPanel(...),
    mainPanel(...)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This layout has always two elements: the &lt;code&gt;sidebarPanel&lt;/code&gt; and the &lt;code&gt;mainPanel&lt;/code&gt;. You can browse other layout options &lt;a href=&#34;https://shiny.rstudio.com/articles/layout-guide.html&#34;&gt;here&lt;/a&gt;, including grids and tabs.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;sidebarPanel&lt;/code&gt; typically contains the &lt;a href=&#34;https://shiny.rstudio.com/tutorial/written-tutorial/lesson3/&#34;&gt;control widgets&lt;/a&gt;. Those widgets are what the users interact with.&lt;/p&gt;
&lt;p&gt;Here, we are using a &lt;a href=&#34;https://shiny.rstudio.com/reference/shiny/latest/sliderInput.html&#34;&gt;sliderInput&lt;/a&gt; to allow the user to select the number of coin flips (in a range from 10 - 1000) and the probability for Heads (in a range from 0 - 1):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sidebarPanel(
  sliderInput(inputId = &amp;quot;n&amp;quot;, label = &amp;quot;Number of flips:&amp;quot;,
              min = 10, max = 1000, value = 500),
  sliderInput(inputId = &amp;quot;prob&amp;quot;, label = &amp;quot;Success rate:&amp;quot;,
              min = 0, max = 1, value = 0.5),
  
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both sliders have the same syntax:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;min&lt;/code&gt;, &lt;code&gt;max&lt;/code&gt;, and &lt;code&gt;value&lt;/code&gt; define the slider range and the default value at which the slider sits upon loading the app. Those parameters are specific to the slider widget.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;label&lt;/code&gt; is the text explaining to the user what the slider is being used for.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the &lt;code&gt;inputID&lt;/code&gt; is important, since it will be used in the server-side part of the app to assign inputs to outputs. Note, that we call the number of flips &lt;code&gt;n&lt;/code&gt; and the probability for Heads &lt;code&gt;prob&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other &lt;a href=&#34;https://shiny.rstudio.com/gallery/widget-gallery.html&#34;&gt;available widgets&lt;/a&gt; include checkboxes, radio buttons, or text input; each with their own specific parameters besides &lt;code&gt;InputID&lt;/code&gt; and &lt;code&gt;label&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Ǹote, that besides widgets and plots, &lt;a href=&#34;https://shiny.rstudio.com/articles/tag-glossary.html&#34;&gt;html content or formatting&lt;/a&gt; can be added inside a &lt;code&gt;Panel&lt;/code&gt; method. In the code I’m inserting a short paragraph and the hyperlink to this blog post:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tags$div(tags$p(HTML(&amp;quot;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;
                        Find the annotated code&amp;quot;)),
               tags$a(href=post, &amp;quot;in this blog post.&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Shiny tags like &lt;code&gt;tag$p&lt;/code&gt; or &lt;code&gt;tag$a&lt;/code&gt; are named after their HTML equivalents. Raw HTML needs to wrapped via the &lt;code&gt;HTML()&lt;/code&gt; function (thanks &lt;a href=&#34;https://stackoverflow.com/questions/46882025/r-shiny-break-line-in-button-label/46882196#46882196&#34;&gt;stackoverflow&lt;/a&gt;!). The line breaks are there for aesthetic reasons, to make the height of the sidebar and main boxes roughly the same.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;mainPanel&lt;/code&gt; typically contains the rendered reactive output. This object will change immediately when the user selects a different input (here via the sliders). We choose a plot because plots are awesome:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mainPanel(plotOutput(outputId = &amp;quot;bars&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;similar to the &lt;code&gt;inputID&lt;/code&gt; above, the &lt;code&gt;outputID&lt;/code&gt; connects UI elements to server computations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;besides the &lt;code&gt;plotOutput&lt;/code&gt; function, there are &lt;a href=&#34;https://shiny.rstudio.com/tutorial/written-tutorial/lesson4/&#34;&gt;other functions&lt;/a&gt; to produce tables, images, text, and more.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now the 2nd part: the &lt;strong&gt;server setup&lt;/strong&gt;. Here is where all the computations happen that produce the data for our output elements based on the input parameters. This part is close to a typical R workflow, in that you build your plots or tables to communicate insights. The only difference is that parameters are passed from the input UI, and that none of the possible parameters should break your plots.&lt;/p&gt;
&lt;p&gt;In the code, the &lt;code&gt;server&lt;/code&gt; function builds a list-like object &lt;code&gt;output&lt;/code&gt; based on the user &lt;code&gt;input&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;server &amp;lt;- function(input, output) {
  
  output$bars &amp;lt;- renderPlot({})

}&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We define a single output: a plot via &lt;code&gt;renderPlot&lt;/code&gt;. Other &lt;a href=&#34;https://shiny.rstudio.com/reference/shiny/1.0.5/&#34;&gt;render function&lt;/a&gt; include &lt;code&gt;renderImage&lt;/code&gt; or &lt;code&gt;renderTable&lt;/code&gt;. You can add as many output elements as you need.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The plot is assigned to &lt;code&gt;output$bars&lt;/code&gt;. This means that it becomes an element in the &lt;code&gt;output&lt;/code&gt; list (the only element in our case). The name &lt;code&gt;bars&lt;/code&gt; needs to match the &lt;code&gt;outputId = &amp;quot;bars&amp;quot;&lt;/code&gt; in our UI &lt;code&gt;mainPanel&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, the code inside &lt;code&gt;renderPlot&lt;/code&gt; is re-run every time the user changes the input parameters. In our example, I used some ggplot2 styling to make the plot look nicer. Here is an alternative one-liner using only base R, to emphasise the shiny elements. Go on and replace the &lt;code&gt;renderPlot&lt;/code&gt; call in the starter code with this one to see what happens:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;output$bars &amp;lt;- renderPlot({
    barplot(table( rbinom(input$n, 1, input$prob) ))
  })&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In both versions, the &lt;code&gt;rbinom&lt;/code&gt; function does all the work by creating a list of &lt;code&gt;n&lt;/code&gt; random numbers following a &lt;a href=&#34;https://en.wikipedia.org/wiki/Binomial_distribution&#34;&gt;binomial distribution&lt;/a&gt; with a success probability of &lt;code&gt;prob&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note, how the two input parameters are being passed as elements of the &lt;code&gt;input&lt;/code&gt; object. Their names, &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;prob&lt;/code&gt; need to match the respective &lt;code&gt;inputId&lt;/code&gt;s in the UI part.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, don’t forget the line that &lt;strong&gt;runs the whole thing:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;shinyApp(ui = ui, server = server)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that’s it! This is the main technical concept. The rest is the creative part: figuring out what to display with which user inputs. (Well, there’s also &lt;a href=&#34;https://shiny.rstudio.com/tutorial/written-tutorial/lesson5/&#34;&gt;loading datasets and R scripts&lt;/a&gt; as well as &lt;a href=&#34;https://shiny.rstudio.com/tutorial/written-tutorial/lesson6/&#34;&gt;streamlining bulky apps&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Except, we’re not quite done yet. Copy/pasting code into the R console is &lt;em&gt;not quite&lt;/em&gt; the best way to &lt;a href=&#34;https://shiny.rstudio.com/tutorial/written-tutorial/lesson7/&#34;&gt;showcase your app&lt;/a&gt;. Here’s how to do it properly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Paste all the starter code above into a single file called &lt;code&gt;app.R&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Put that file into a sub-directory of your choice (e.g. &lt;code&gt;./headsortails/&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;And call &lt;code&gt;shiny::runApp(&amp;quot;headsortails&amp;quot;)&lt;/code&gt; from an R session running in the parent directory of that subdirectory.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each &lt;code&gt;app.R&lt;/code&gt; should live in its own sub-directory. They are called via the names of their sub-directories. (Note, that the convenience of having both UI and server in the same file was not always possible. Old shiny versions required two separate &lt;code&gt;ui.R&lt;/code&gt; and &lt;code&gt;server.R&lt;/code&gt; files; a structure that’s still supported).&lt;/p&gt;
&lt;p&gt;Finally, shiny apps are ideal to be shared online since they are reactive HTML. You can run your own &lt;a href=&#34;https://github.com/rstudio/shiny-server/blob/master/README.md&#34;&gt;shiny server&lt;/a&gt; to do this, especially if you have many different apps to showcase. For your first steps, I recommend using &lt;a href=&#34;https://www.shinyapps.io&#34;&gt;shinyapps.io&lt;/a&gt;, run by the omipresent Rstudio folks. They have a &lt;a href=&#34;https://www.shinyapps.io/#pricing&#34;&gt;free tier&lt;/a&gt; allowing you to host 5 apps running for a maximum 25 hours per month. That’s plenty of resources to get your feet wet.&lt;/p&gt;
&lt;p&gt;As indicated at the beginning, I’m using shinyapps.io to host the &lt;a href=&#34;https://headsortails.shinyapps.io/headsortails/&#34;&gt;version of the app&lt;/a&gt; that is included above. However, you cannot embed shiny elements directly into a &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;blogdown&lt;/a&gt; post like this one, since those posts are static. Above, I used the &lt;a href=&#34;https://stackoverflow.com/questions/46136141/incorporating-interactive-shiny-apps-into-rmarkdown-document-for-blogdown-hugo-b&#34;&gt;little trick&lt;/a&gt; of embedding the link to my shiny app via the HTML &lt;code&gt;iframe&lt;/code&gt; tag. Like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;iframe src=&amp;quot;https://headsortails.shinyapps.io/headsortails/&amp;quot; width=&amp;quot;800&amp;quot; height=&amp;quot;500&amp;quot; frameborder=&amp;quot;no&amp;quot; scrolling=&amp;quot;no&amp;quot;&amp;gt;&amp;lt;/iframe&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;More info:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://shiny.rstudio.com/tutorial/&#34;&gt;official shiny tutorial&lt;/a&gt;, from which this post was digested, contains a list of &lt;a href=&#34;https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/#Go%20Further&#34;&gt;11 example apps&lt;/a&gt; that demonstrate various use cases.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Check out the pretty comprehensive &lt;a href=&#34;https://shiny.rstudio.com/gallery/&#34;&gt;shiny gallery&lt;/a&gt; for plenty of inspiration. As for many Rstudio/tidyverse tools there’s also a handy &lt;a href=&#34;https://shiny.rstudio.com/articles/cheatsheet.html&#34;&gt;cheat sheet&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you’re primarily interested in reactive dashboards have a look at &lt;a href=&#34;https://rstudio.github.io/shinydashboard/&#34;&gt;shiny dashboard&lt;/a&gt;. I played with it a bit and I like it so far.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Tidyverse evolutions: curly-curly operator and pivoting (feat. tidytuesday data &amp; leaflet visuals)</title>
      <link>https://heads0rtai1s.github.io/2019/11/07/tidy-curly-pivot-leaflet/</link>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2019/11/07/tidy-curly-pivot-leaflet/</guid>
      <description>
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/leaflet/leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/leaflet/leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/leafletfix/leafletfix.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/Proj4Leaflet/proj4-compressed.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/Proj4Leaflet/proj4leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/rstudio_leaflet/rstudio_leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/leaflet-binding/leaflet.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/leaflet-providers/leaflet-providers.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/leaflet-providers-plugin/leaflet-providers-plugin.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; ecosystem is steadily growing and adapting to the needs of its users. As part of this evolution, existing tools are being replaced by new and better methods. As useful as this flexibility is to the strength of the system, sometimes it can be hard to keep track of all the changes. &lt;strong&gt;This blogpost will deal with two new developments: the ‘curly-curly’ operator for tidy evaluation and the new ‘pivot’ functions for data reshaping.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will need the following libraries; in particular make sure that you have installed at least &lt;code&gt;tidyr&lt;/code&gt; version 1.0:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs &amp;lt;- c(&amp;#39;dplyr&amp;#39;, &amp;#39;tidyr&amp;#39;,          # wrangling
          &amp;#39;readr&amp;#39;, &amp;#39;stringr&amp;#39;,        # wrangling
          &amp;#39;knitr&amp;#39;,&amp;#39;kableExtra&amp;#39;,      # table styling
          &amp;#39;ggplot2&amp;#39;,&amp;#39;gridExtra&amp;#39;,     # plots
          &amp;#39;leaflet&amp;#39;)                 # interactive maps
invisible(lapply(libs, library, character.only = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this post’s dataset we’ll be doing something different than sampling the usual in-built tables. We will work with the &lt;a href=&#34;https://www.nationalgeographic.com/animals/2019/06/squirrel-census-new-york-city-central-park/&#34;&gt;famous&lt;/a&gt; &lt;a href=&#34;https://www.thesquirrelcensus.com&#34;&gt;NYC Squirrel Census&lt;/a&gt; - cataloguing the squirrel population of New York’s Central Park in October 2018. The squirrel census is a great example for a citizen science project that’s both accessible fun and useful scientific contribution.&lt;/p&gt;
&lt;p&gt;Talking about fun community projects: the specific dataset we’re using has been prepared by the &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;TidyTuesday project&lt;/a&gt;, a weekly social exercise to test our tidyverse skills and provide inspiration. TidyTuesday is run by the &lt;a href=&#34;https://www.rfordatasci.com&#34;&gt;R for Data Science&lt;/a&gt; Online Learning Community. All datasets are available on github:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;squirrels &amp;lt;- read_csv(str_c(
  &amp;quot;https://raw.githubusercontent.com/&amp;quot;,
  &amp;quot;rfordatascience/tidytuesday/master/&amp;quot;,
  &amp;quot;data/2019/2019-10-29/nyc_squirrels.csv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;squirrels %&amp;gt;% 
  select(lat, long, date, running, climbing, eating) %&amp;gt;% 
  head(5) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
lat
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
long
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
date
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
running
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
climbing
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
eating
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40.79408
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-73.95613
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10142018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40.79485
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-73.95704
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10062018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40.76672
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-73.97683
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10102018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40.76970
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-73.97572
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10182018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40.79753
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-73.95931
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10182018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Among other features, the squirrels data includes the latitude and longitude of the sighting, the date, and flags that indicate whether this squirrel was spotted running, climbing, or eating. True to tidy form, each row corresponds to one squirrel. There are many more features in the full dataset, but we will focus on the ones above in the following examples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;First, we will cover tidy evaluation with the &lt;code&gt;{{ }}&lt;/code&gt; operator aka ‘curly-curly’.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Introduced as part of &lt;a href=&#34;https://www.tidyverse.org/articles/2019/06/rlang-0-4-0/&#34;&gt;rlang version 0.4.0&lt;/a&gt; back in June, curly-curly replaces the approach of quoting with &lt;code&gt;enquo&lt;/code&gt; and unquoting with &lt;code&gt;!!&lt;/code&gt; (aka ‘bang-bang’). Instead of two operations it provides a compact shorthand for simple cases. Here is a brief example for counting groups of distinct feature values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new style
count_groups &amp;lt;- function(df, groupvar){
  df %&amp;gt;% 
    group_by({{ groupvar }}) %&amp;gt;% 
    count()
}

count_groups(squirrels, climbing) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
climbing
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2365
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
658
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In this example, &lt;code&gt;{{ groupvar }}&lt;/code&gt; splices the value of &lt;code&gt;groupvar&lt;/code&gt; into the &lt;code&gt;group_by&lt;/code&gt; call, rather than its name. This is equivalent to the quote-unquote style of &lt;code&gt;!! enquo(groupvar)&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# old style
count_groups_old &amp;lt;- function(df, groupvar){
  df %&amp;gt;% 
    group_by(!! enquo(groupvar)) %&amp;gt;% 
    count()
}

count_groups_old(squirrels, climbing) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
climbing
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2365
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
658
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The advantage of &lt;code&gt;{{ }}&lt;/code&gt; lies in its relative cognitive ease: think of it as inserting the value of the variable into the expression. No intermediate quoting or unquoting needed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next, we will look at how to reshape data with the &lt;a href=&#34;https://tidyr.tidyverse.org/articles/pivot.html&#34;&gt;new functions&lt;/a&gt; &lt;code&gt;pivot_wider&lt;/code&gt; and &lt;code&gt;pivot_longer&lt;/code&gt;&lt;/strong&gt;, which are replacing the previous &lt;code&gt;spread&lt;/code&gt; and &lt;code&gt;gather&lt;/code&gt; tools. Most importantly: &lt;code&gt;pivot_wider&lt;/code&gt; is the inverse function to &lt;code&gt;pivot_longer&lt;/code&gt;, and vice versa.&lt;/p&gt;
&lt;p&gt;As the name suggests, &lt;code&gt;pivot_wider&lt;/code&gt; makes a tibble wider by turning a single categorical column into multiple columns, one for each category.&lt;/p&gt;
&lt;p&gt;Let’s look at this aggregated data frame of sightings of climbing squirrels per day:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;squirrels %&amp;gt;% 
  count(date, climbing) %&amp;gt;% 
  head(4) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
date
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
climbing
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10062018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
253
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10062018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
84
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10072018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
283
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10072018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
122
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Pivoting to a wider format allows us to compare the numbers of climbing and non-climbing squirrels directly next to each other:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new style with pivot_wider
squirrels %&amp;gt;% 
  count(date, climbing) %&amp;gt;% 
  pivot_wider(names_from = climbing, values_from = n, names_prefix = &amp;quot;climbing_&amp;quot;) %&amp;gt;% 
  head(2) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
date
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
climbing_FALSE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
climbing_TRUE
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10062018
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
253
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
84
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10072018
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
283
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
122
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;names_from&lt;/code&gt; argument indicates which column supplies the new column names. The values of this column are being picked from the feature defined via the &lt;code&gt;values_from&lt;/code&gt; keyword. Here, we pick the names from the binary feature &lt;code&gt;climbing&lt;/code&gt; and the values from the count column &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Among the additional keywords, &lt;code&gt;names_prefix&lt;/code&gt; allows us to assign meaningful names to the new columns (which would otherwise simply be the rather generic ‘FALSE’ and ‘TRUE’ here). Another useful keyword is &lt;code&gt;values_fill&lt;/code&gt;, which specifies a global replacement for any missing values.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The new &lt;code&gt;pivot_wider&lt;/code&gt; replaces the old &lt;code&gt;spread&lt;/code&gt; function, which had comparable yet possibly more confusing parameters. For comparison, here is the same result with &lt;code&gt;spread&lt;/code&gt;:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# old style with spread
squirrels %&amp;gt;% 
  count(date, climbing) %&amp;gt;% 
  spread(key = climbing, value = n) %&amp;gt;% 
  head(2) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
date
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
FALSE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
TRUE
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10062018
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
253
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
84
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10072018
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
283
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
122
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The syntax is very similar, which should make it easy for those familiar with &lt;code&gt;spread&lt;/code&gt; to switch to &lt;code&gt;pivot_wider&lt;/code&gt;. Note, that &lt;code&gt;spread&lt;/code&gt; had no &lt;code&gt;names_prefix&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;In most tutorials, &lt;code&gt;pivot_wider&lt;/code&gt; is somewhat overshadowed by &lt;code&gt;pivot_longer&lt;/code&gt;; but I use it frequently to quickly compute proportions for grouped columns. For instance, here are the top 3 days for spotting squirrels climbing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;squirrels %&amp;gt;% 
  count(date, climbing) %&amp;gt;% 
  pivot_wider(names_from = climbing, values_from = n, names_prefix = &amp;quot;climbing_&amp;quot;) %&amp;gt;%
  mutate(climbing_percentage = climbing_TRUE/(climbing_TRUE + climbing_FALSE)*100) %&amp;gt;% 
  arrange(desc(climbing_percentage)) %&amp;gt;% 
  head(3) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
date
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
climbing_FALSE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
climbing_TRUE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
climbing_percentage
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10072018
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
283
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
122
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
30.12346
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10062018
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
253
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
84
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24.92582
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10082018
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
220
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22.80702
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Let’s move on to &lt;code&gt;pivot_longer&lt;/code&gt;&lt;/strong&gt;. Being the inverse function to &lt;code&gt;pivot_wider&lt;/code&gt;, this tool (often) reduces the number of columns by turning some of the columns into a single new (typically) categorical feature alongside an (often times) numerical feature. The different levels of the categorical column now describe the numerical column in a unique way (similarly to the wider column structure previously). This is all a bit abstract, so let’s do a quick example:&lt;/p&gt;
&lt;p&gt;Imagine we’ve extracted both the proportion of climbing squirrels and non-climbing squirrels for each day with the help of &lt;code&gt;pivot_wider&lt;/code&gt;. (Of course, one number determines the other but let’s use it as a simple illustration).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;squirrels %&amp;gt;% 
  count(date, climbing) %&amp;gt;% 
  pivot_wider(names_from = climbing, values_from = n, names_prefix = &amp;quot;climbing_&amp;quot;) %&amp;gt;%
  mutate(prop_true = climbing_TRUE/(climbing_TRUE + climbing_FALSE)*100,
         prop_false = climbing_FALSE/(climbing_TRUE + climbing_FALSE)*100) %&amp;gt;% 
  head(2) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
date
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
climbing_FALSE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
climbing_TRUE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
prop_true
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
prop_false
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10062018
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
253
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
84
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24.92582
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75.07418
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10072018
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
283
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
122
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
30.12346
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69.87654
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now we will use &lt;code&gt;pivot_longer&lt;/code&gt; to turn those two &lt;code&gt;prop_&lt;/code&gt; columns into a categorical &lt;code&gt;climbing&lt;/code&gt; and a numerical &lt;code&gt;percentage&lt;/code&gt; feature. Here, we drop the &lt;code&gt;climbing_FALSE&lt;/code&gt; and &lt;code&gt;climbing_TRUE&lt;/code&gt; columns, so the pivoting doesn’t change the number of columns but it changes the structure of the dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;squirrels %&amp;gt;% 
  count(date, climbing) %&amp;gt;% 
  pivot_wider(names_from = climbing, values_from = n, names_prefix = &amp;quot;climbing_&amp;quot;) %&amp;gt;%
  mutate(prop_true = climbing_TRUE/(climbing_TRUE + climbing_FALSE),
         prop_false = climbing_FALSE/(climbing_TRUE + climbing_FALSE)) %&amp;gt;%
  select(date, prop_true, prop_false) %&amp;gt;% 
  pivot_longer(cols = c(&amp;quot;prop_true&amp;quot;, &amp;quot;prop_false&amp;quot;),
               names_to = &amp;quot;climbing&amp;quot;, values_to = &amp;quot;percentage&amp;quot;) %&amp;gt;% 
  head(4)  %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
date
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
climbing
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
percentage
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10062018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
prop_true
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2492582
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10062018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
prop_false
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7507418
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10072018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
prop_true
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3012346
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10072018
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
prop_false
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6987654
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;cols&lt;/code&gt; argument specifies the columns that will be pivoted. Then, &lt;code&gt;names_to&lt;/code&gt; gives the name of the new feature that will hold the (categorical) names of the original columns. And &lt;code&gt;values_to&lt;/code&gt; is the new feature that hold their values (here the percentages).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note, that as with the older &lt;code&gt;gather&lt;/code&gt; method, the new column names have to be passed as strings.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The resulting data structure is often much better suited for plotting with &lt;code&gt;ggplot2&lt;/code&gt;. The categorical feature can directly become a colour, fill, or faceting variable.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Finally, let’s combine curly-curly and pivoting into a comprehensive example.&lt;/strong&gt; Here, we build a function that takes as arguments the name of a data frame and the name of a logical column, here a squirrel action, and then extracts the percentage of this action per coordinate bin. This is it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_percentage &amp;lt;- function(df, col) {
  
  df %&amp;gt;% 
    mutate(lat = round(lat*5, 2)/5,
         long = round(long*5, 2)/5) %&amp;gt;% 
    count(lat, long, {{ col }}) %&amp;gt;% 
    pivot_wider(names_from = {{ col }}, values_from = n,
                values_fill = list(n = 0)) %&amp;gt;%
    mutate(true = `TRUE`/(`TRUE` + `FALSE`)*100,
           false = `FALSE`/(`TRUE` + `FALSE`)*100) %&amp;gt;%
    select(lat, long, true, false) %&amp;gt;% 
    pivot_longer(cols = c(&amp;quot;true&amp;quot;, &amp;quot;false&amp;quot;),
                 names_to = &amp;quot;action&amp;quot;, values_to = &amp;quot;percentage&amp;quot;) %&amp;gt;% 
    filter(action == &amp;quot;true&amp;quot;)
}

extract_percentage(squirrels, climbing) %&amp;gt;% 
  head(3)  %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
lat
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
long
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
action
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
percentage
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40.764
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-73.974
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
true
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50.00000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40.766
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-73.978
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
true
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21.73913
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40.766
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-73.976
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
true
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19.23077
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The coordinates are rounded the nearest 0.02 degrees to provide the sample size for summary statistics.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We only keep the percentage of positive sightings for each action.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We’re now using this function to extract the proportions of squirrels that were observed eating, climbing, or running. Then we visualise those proportions on an interactive map of Manhattan centred on Central Park. The map is constructed using the wonderful &lt;a href=&#34;https://rstudio.github.io/leaflet/&#34;&gt;leaflet&lt;/a&gt; package. Such a map could be used to find locations in the park that might be more promising than others for spotting certain squirrel shenanigans.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loc &amp;lt;- extract_percentage(squirrels, eating)

pal &amp;lt;- colorNumeric(palette = &amp;quot;RdBu&amp;quot;, domain = seq(0,100), reverse = TRUE)

leaflet(loc) %&amp;gt;%
  setView(lng = median(loc$long), lat = median(loc$lat), zoom = 13) %&amp;gt;% 
  #addProviderTiles(&amp;quot;Esri.NatGeoWorldMap&amp;quot;) %&amp;gt;%
  addProviderTiles(providers$CartoDB.DarkMatter) %&amp;gt;%
  addCircleMarkers(~ long, ~ lat,
                   data = extract_percentage(squirrels, eating),
                   group = &amp;quot;Eating&amp;quot;,
                   color = ~ pal(percentage),
                   radius = 6, fillOpacity = 0.7, stroke = FALSE) %&amp;gt;% 
  addCircleMarkers(~ long, ~ lat,
                   data = extract_percentage(squirrels, climbing),
                   group = &amp;quot;Climbing&amp;quot;,
                   color = ~ pal(percentage),
                   radius = 6, fillOpacity = 0.7, stroke = FALSE) %&amp;gt;%
  addCircleMarkers(~ long, ~ lat,
                   data = extract_percentage(squirrels, running),
                   group = &amp;quot;Running&amp;quot;,
                   color = ~ pal(percentage),
                   radius = 6, fillOpacity = 0.7, stroke = FALSE) %&amp;gt;%
  addLayersControl(baseGroups = c(&amp;quot;Eating&amp;quot;, &amp;quot;Climbing&amp;quot;, &amp;quot;Running&amp;quot;),
                   options = layersControlOptions(collapsed = FALSE)) %&amp;gt;%
  addLegend(&amp;quot;bottomright&amp;quot;, pal = pal, values = ~ percentage,
            title = &amp;quot;Percentage&amp;quot;, labFormat = labelFormat(suffix = &amp;quot;%&amp;quot;),) %&amp;gt;% 
  addScaleBar(&amp;quot;bottomleft&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;leaflet html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;options&#34;:{&#34;crs&#34;:{&#34;crsClass&#34;:&#34;L.CRS.EPSG3857&#34;,&#34;code&#34;:null,&#34;proj4def&#34;:null,&#34;projectedBounds&#34;:null,&#34;options&#34;:{}}},&#34;setView&#34;:[[40.782,-73.966],13,[]],&#34;calls&#34;:[{&#34;method&#34;:&#34;addProviderTiles&#34;,&#34;args&#34;:[&#34;CartoDB.DarkMatter&#34;,null,null,{&#34;errorTileUrl&#34;:&#34;&#34;,&#34;noWrap&#34;:false,&#34;detectRetina&#34;:false}]},{&#34;method&#34;:&#34;addCircleMarkers&#34;,&#34;args&#34;:[[40.764,40.766,40.766,40.766,40.766,40.768,40.768,40.768,40.768,40.768,40.768,40.768,40.77,40.77,40.77,40.77,40.77,40.77,40.77,40.772,40.772,40.772,40.772,40.772,40.772,40.772,40.774,40.774,40.774,40.774,40.774,40.774,40.774,40.776,40.776,40.776,40.776,40.776,40.776,40.776,40.778,40.778,40.778,40.778,40.778,40.778,40.78,40.78,40.78,40.78,40.78,40.78,40.78,40.782,40.782,40.782,40.782,40.782,40.782,40.782,40.784,40.784,40.784,40.784,40.784,40.784,40.786,40.786,40.786,40.788,40.788,40.788,40.788,40.788,40.788,40.788,40.79,40.79,40.79,40.79,40.79,40.79,40.79,40.792,40.792,40.792,40.792,40.792,40.792,40.792,40.794,40.794,40.794,40.794,40.794,40.794,40.794,40.794,40.796,40.796,40.796,40.796,40.796,40.796,40.796,40.798,40.798,40.798,40.798,40.798,40.798,40.8,40.8],[-73.974,-73.978,-73.976,-73.974,-73.972,-73.982,-73.98,-73.978,-73.976,-73.974,-73.972,-73.97,-73.98,-73.978,-73.976,-73.974,-73.972,-73.97,-73.968,-73.98,-73.978,-73.976,-73.974,-73.972,-73.97,-73.968,-73.978,-73.976,-73.974,-73.972,-73.97,-73.968,-73.966,-73.976,-73.974,-73.972,-73.97,-73.968,-73.966,-73.964,-73.974,-73.972,-73.97,-73.968,-73.966,-73.964,-73.974,-73.972,-73.97,-73.968,-73.966,-73.964,-73.962,-73.972,-73.97,-73.968,-73.966,-73.964,-73.962,-73.96,-73.97,-73.968,-73.966,-73.964,-73.96,-73.958,-73.97,-73.968,-73.958,-73.968,-73.966,-73.964,-73.962,-73.96,-73.958,-73.956,-73.966,-73.964,-73.962,-73.96,-73.958,-73.956,-73.954,-73.964,-73.962,-73.96,-73.958,-73.956,-73.954,-73.952,-73.964,-73.962,-73.96,-73.958,-73.956,-73.954,-73.952,-73.95,-73.962,-73.96,-73.958,-73.956,-73.954,-73.952,-73.95,-73.96,-73.958,-73.956,-73.954,-73.952,-73.95,-73.958,-73.956],6,null,&#34;Eating&#34;,{&#34;interactive&#34;:true,&#34;className&#34;:&#34;&#34;,&#34;stroke&#34;:false,&#34;color&#34;:[&#34;#053061&#34;,&#34;#2D73B3&#34;,&#34;#E9F0F4&#34;,&#34;#8FC3DD&#34;,&#34;#4996C5&#34;,&#34;#3984BB&#34;,&#34;#94C6DF&#34;,&#34;#3780BA&#34;,&#34;#3C87BD&#34;,&#34;#96C7DF&#34;,&#34;#97C7DF&#34;,&#34;#A8D0E4&#34;,&#34;#408DC0&#34;,&#34;#C9E1EE&#34;,&#34;#75B0D3&#34;,&#34;#62A4CD&#34;,&#34;#6EACD1&#34;,&#34;#7BB4D5&#34;,&#34;#053061&#34;,&#34;#F8B799&#34;,&#34;#3279B6&#34;,&#34;#408DC0&#34;,&#34;#CAE1EE&#34;,&#34;#88BEDA&#34;,&#34;#6EACD1&#34;,&#34;#3D89BE&#34;,&#34;#A8D0E4&#34;,&#34;#73AFD2&#34;,&#34;#E0ECF3&#34;,&#34;#C0DCEB&#34;,&#34;#90C4DD&#34;,&#34;#4190C2&#34;,&#34;#6EACD1&#34;,&#34;#FAECE4&#34;,&#34;#3781BA&#34;,&#34;#529BC7&#34;,&#34;#E1ECF3&#34;,&#34;#61A4CC&#34;,&#34;#B7D7E8&#34;,&#34;#053061&#34;,&#34;#286DB0&#34;,&#34;#88BEDA&#34;,&#34;#96C7DF&#34;,&#34;#5EA1CB&#34;,&#34;#6EACD1&#34;,&#34;#88BEDA&#34;,&#34;#053061&#34;,&#34;#3E8BBF&#34;,&#34;#A8D0E4&#34;,&#34;#3279B6&#34;,&#34;#C9E1EE&#34;,&#34;#EFF3F5&#34;,&#34;#053061&#34;,&#34;#F7F7F7&#34;,&#34;#519AC7&#34;,&#34;#4996C5&#34;,&#34;#8EC2DC&#34;,&#34;#94C6DE&#34;,&#34;#357EB8&#34;,&#34;#4393C3&#34;,&#34;#76B1D3&#34;,&#34;#347CB8&#34;,&#34;#408FC1&#34;,&#34;#053061&#34;,&#34;#7AB4D5&#34;,&#34;#053061&#34;,&#34;#67001F&#34;,&#34;#357EB8&#34;,&#34;#3E8BBF&#34;,&#34;#053061&#34;,&#34;#7FB7D7&#34;,&#34;#3279B6&#34;,&#34;#4393C3&#34;,&#34;#053061&#34;,&#34;#1E61A5&#34;,&#34;#154F8C&#34;,&#34;#6EACD1&#34;,&#34;#E7EFF4&#34;,&#34;#7FB7D7&#34;,&#34;#A3CDE3&#34;,&#34;#6EACD1&#34;,&#34;#17518F&#34;,&#34;#1E61A5&#34;,&#34;#B9D8E9&#34;,&#34;#3D89BE&#34;,&#34;#94C6DF&#34;,&#34;#8FC3DD&#34;,&#34;#1E61A5&#34;,&#34;#3E8BBF&#34;,&#34;#B7D7E8&#34;,&#34;#053061&#34;,&#34;#64A6CD&#34;,&#34;#357DB8&#34;,&#34;#529BC7&#34;,&#34;#A8D0E4&#34;,&#34;#418FC1&#34;,&#34;#BEDBEA&#34;,&#34;#67001F&#34;,&#34;#3E8BBF&#34;,&#34;#5DA1CB&#34;,&#34;#8DC1DC&#34;,&#34;#134A86&#34;,&#34;#4A96C5&#34;,&#34;#C2DDEC&#34;,&#34;#3984BB&#34;,&#34;#579EC9&#34;,&#34;#4291C2&#34;,&#34;#3178B5&#34;,&#34;#3B86BD&#34;,&#34;#1E61A5&#34;,&#34;#3279B6&#34;,&#34;#286DB0&#34;,&#34;#3E8BBF&#34;],&#34;weight&#34;:5,&#34;opacity&#34;:0.5,&#34;fill&#34;:true,&#34;fillColor&#34;:[&#34;#053061&#34;,&#34;#2D73B3&#34;,&#34;#E9F0F4&#34;,&#34;#8FC3DD&#34;,&#34;#4996C5&#34;,&#34;#3984BB&#34;,&#34;#94C6DF&#34;,&#34;#3780BA&#34;,&#34;#3C87BD&#34;,&#34;#96C7DF&#34;,&#34;#97C7DF&#34;,&#34;#A8D0E4&#34;,&#34;#408DC0&#34;,&#34;#C9E1EE&#34;,&#34;#75B0D3&#34;,&#34;#62A4CD&#34;,&#34;#6EACD1&#34;,&#34;#7BB4D5&#34;,&#34;#053061&#34;,&#34;#F8B799&#34;,&#34;#3279B6&#34;,&#34;#408DC0&#34;,&#34;#CAE1EE&#34;,&#34;#88BEDA&#34;,&#34;#6EACD1&#34;,&#34;#3D89BE&#34;,&#34;#A8D0E4&#34;,&#34;#73AFD2&#34;,&#34;#E0ECF3&#34;,&#34;#C0DCEB&#34;,&#34;#90C4DD&#34;,&#34;#4190C2&#34;,&#34;#6EACD1&#34;,&#34;#FAECE4&#34;,&#34;#3781BA&#34;,&#34;#529BC7&#34;,&#34;#E1ECF3&#34;,&#34;#61A4CC&#34;,&#34;#B7D7E8&#34;,&#34;#053061&#34;,&#34;#286DB0&#34;,&#34;#88BEDA&#34;,&#34;#96C7DF&#34;,&#34;#5EA1CB&#34;,&#34;#6EACD1&#34;,&#34;#88BEDA&#34;,&#34;#053061&#34;,&#34;#3E8BBF&#34;,&#34;#A8D0E4&#34;,&#34;#3279B6&#34;,&#34;#C9E1EE&#34;,&#34;#EFF3F5&#34;,&#34;#053061&#34;,&#34;#F7F7F7&#34;,&#34;#519AC7&#34;,&#34;#4996C5&#34;,&#34;#8EC2DC&#34;,&#34;#94C6DE&#34;,&#34;#357EB8&#34;,&#34;#4393C3&#34;,&#34;#76B1D3&#34;,&#34;#347CB8&#34;,&#34;#408FC1&#34;,&#34;#053061&#34;,&#34;#7AB4D5&#34;,&#34;#053061&#34;,&#34;#67001F&#34;,&#34;#357EB8&#34;,&#34;#3E8BBF&#34;,&#34;#053061&#34;,&#34;#7FB7D7&#34;,&#34;#3279B6&#34;,&#34;#4393C3&#34;,&#34;#053061&#34;,&#34;#1E61A5&#34;,&#34;#154F8C&#34;,&#34;#6EACD1&#34;,&#34;#E7EFF4&#34;,&#34;#7FB7D7&#34;,&#34;#A3CDE3&#34;,&#34;#6EACD1&#34;,&#34;#17518F&#34;,&#34;#1E61A5&#34;,&#34;#B9D8E9&#34;,&#34;#3D89BE&#34;,&#34;#94C6DF&#34;,&#34;#8FC3DD&#34;,&#34;#1E61A5&#34;,&#34;#3E8BBF&#34;,&#34;#B7D7E8&#34;,&#34;#053061&#34;,&#34;#64A6CD&#34;,&#34;#357DB8&#34;,&#34;#529BC7&#34;,&#34;#A8D0E4&#34;,&#34;#418FC1&#34;,&#34;#BEDBEA&#34;,&#34;#67001F&#34;,&#34;#3E8BBF&#34;,&#34;#5DA1CB&#34;,&#34;#8DC1DC&#34;,&#34;#134A86&#34;,&#34;#4A96C5&#34;,&#34;#C2DDEC&#34;,&#34;#3984BB&#34;,&#34;#579EC9&#34;,&#34;#4291C2&#34;,&#34;#3178B5&#34;,&#34;#3B86BD&#34;,&#34;#1E61A5&#34;,&#34;#3279B6&#34;,&#34;#286DB0&#34;,&#34;#3E8BBF&#34;],&#34;fillOpacity&#34;:0.7},null,null,null,null,null,{&#34;interactive&#34;:false,&#34;permanent&#34;:false,&#34;direction&#34;:&#34;auto&#34;,&#34;opacity&#34;:1,&#34;offset&#34;:[0,0],&#34;textsize&#34;:&#34;10px&#34;,&#34;textOnly&#34;:false,&#34;className&#34;:&#34;&#34;,&#34;sticky&#34;:true},null]},{&#34;method&#34;:&#34;addCircleMarkers&#34;,&#34;args&#34;:[[40.764,40.766,40.766,40.766,40.766,40.768,40.768,40.768,40.768,40.768,40.768,40.768,40.77,40.77,40.77,40.77,40.77,40.77,40.77,40.772,40.772,40.772,40.772,40.772,40.772,40.772,40.774,40.774,40.774,40.774,40.774,40.774,40.774,40.776,40.776,40.776,40.776,40.776,40.776,40.776,40.778,40.778,40.778,40.778,40.778,40.778,40.78,40.78,40.78,40.78,40.78,40.78,40.78,40.782,40.782,40.782,40.782,40.782,40.782,40.782,40.784,40.784,40.784,40.784,40.784,40.784,40.786,40.786,40.786,40.788,40.788,40.788,40.788,40.788,40.788,40.788,40.79,40.79,40.79,40.79,40.79,40.79,40.79,40.792,40.792,40.792,40.792,40.792,40.792,40.792,40.794,40.794,40.794,40.794,40.794,40.794,40.794,40.794,40.796,40.796,40.796,40.796,40.796,40.796,40.796,40.798,40.798,40.798,40.798,40.798,40.798,40.8,40.8],[-73.974,-73.978,-73.976,-73.974,-73.972,-73.982,-73.98,-73.978,-73.976,-73.974,-73.972,-73.97,-73.98,-73.978,-73.976,-73.974,-73.972,-73.97,-73.968,-73.98,-73.978,-73.976,-73.974,-73.972,-73.97,-73.968,-73.978,-73.976,-73.974,-73.972,-73.97,-73.968,-73.966,-73.976,-73.974,-73.972,-73.97,-73.968,-73.966,-73.964,-73.974,-73.972,-73.97,-73.968,-73.966,-73.964,-73.974,-73.972,-73.97,-73.968,-73.966,-73.964,-73.962,-73.972,-73.97,-73.968,-73.966,-73.964,-73.962,-73.96,-73.97,-73.968,-73.966,-73.964,-73.96,-73.958,-73.97,-73.968,-73.958,-73.968,-73.966,-73.964,-73.962,-73.96,-73.958,-73.956,-73.966,-73.964,-73.962,-73.96,-73.958,-73.956,-73.954,-73.964,-73.962,-73.96,-73.958,-73.956,-73.954,-73.952,-73.964,-73.962,-73.96,-73.958,-73.956,-73.954,-73.952,-73.95,-73.962,-73.96,-73.958,-73.956,-73.954,-73.952,-73.95,-73.96,-73.958,-73.956,-73.954,-73.952,-73.95,-73.958,-73.956],6,null,&#34;Climbing&#34;,{&#34;interactive&#34;:true,&#34;className&#34;:&#34;&#34;,&#34;stroke&#34;:false,&#34;color&#34;:[&#34;#F7F7F7&#34;,&#34;#539BC8&#34;,&#34;#418FC1&#34;,&#34;#3F8CC0&#34;,&#34;#4996C5&#34;,&#34;#3984BB&#34;,&#34;#337AB7&#34;,&#34;#3780BA&#34;,&#34;#71AED2&#34;,&#34;#579EC9&#34;,&#34;#357EB8&#34;,&#34;#3984BB&#34;,&#34;#408DC0&#34;,&#34;#74B0D3&#34;,&#34;#579EC9&#34;,&#34;#154F8C&#34;,&#34;#2B71B2&#34;,&#34;#2C72B2&#34;,&#34;#DCEAF2&#34;,&#34;#F8B799&#34;,&#34;#D1E5F0&#34;,&#34;#6EACD1&#34;,&#34;#3077B5&#34;,&#34;#519AC7&#34;,&#34;#4E99C6&#34;,&#34;#A0CCE2&#34;,&#34;#E2EDF3&#34;,&#34;#68A8CF&#34;,&#34;#84BBD9&#34;,&#34;#4393C3&#34;,&#34;#3882BA&#34;,&#34;#2D73B3&#34;,&#34;#E68367&#34;,&#34;#1B599A&#34;,&#34;#84BBD9&#34;,&#34;#8EC2DC&#34;,&#34;#195696&#34;,&#34;#72AED2&#34;,&#34;#64A6CD&#34;,&#34;#DCEAF2&#34;,&#34;#7CB5D6&#34;,&#34;#3279B6&#34;,&#34;#4190C2&#34;,&#34;#5EA1CB&#34;,&#34;#F7F7F7&#34;,&#34;#DCEAF2&#34;,&#34;#A8D0E4&#34;,&#34;#5CA0CA&#34;,&#34;#4B97C5&#34;,&#34;#195696&#34;,&#34;#B5D6E8&#34;,&#34;#B1D4E7&#34;,&#34;#053061&#34;,&#34;#E68367&#34;,&#34;#053061&#34;,&#34;#A1CCE2&#34;,&#34;#62A4CD&#34;,&#34;#4C98C6&#34;,&#34;#97C7DF&#34;,&#34;#D1E5F0&#34;,&#34;#76B1D3&#34;,&#34;#2B71B2&#34;,&#34;#C5DFED&#34;,&#34;#A8D0E4&#34;,&#34;#A8D0E4&#34;,&#34;#F7F7F7&#34;,&#34;#053061&#34;,&#34;#5FA2CB&#34;,&#34;#7FB7D7&#34;,&#34;#2B71B2&#34;,&#34;#BBD9E9&#34;,&#34;#3279B6&#34;,&#34;#7AB4D5&#34;,&#34;#A8D0E4&#34;,&#34;#FAEAE1&#34;,&#34;#D6E7F1&#34;,&#34;#3984BB&#34;,&#34;#9BCAE1&#34;,&#34;#3E8BBF&#34;,&#34;#65A6CE&#34;,&#34;#6EACD1&#34;,&#34;#F7F7F7&#34;,&#34;#7FB7D7&#34;,&#34;#1C5D9F&#34;,&#34;#6EACD1&#34;,&#34;#7FB7D7&#34;,&#34;#3F8CC0&#34;,&#34;#BBD9E9&#34;,&#34;#5CA0CA&#34;,&#34;#3279B6&#34;,&#34;#053061&#34;,&#34;#77B2D4&#34;,&#34;#68A8CE&#34;,&#34;#408EC1&#34;,&#34;#2A6FB1&#34;,&#34;#2C72B2&#34;,&#34;#3780B9&#34;,&#34;#053061&#34;,&#34;#053061&#34;,&#34;#1D5EA1&#34;,&#34;#4291C2&#34;,&#34;#347CB8&#34;,&#34;#4A96C5&#34;,&#34;#2B71B2&#34;,&#34;#053061&#34;,&#34;#579EC9&#34;,&#34;#65A6CE&#34;,&#34;#3178B5&#34;,&#34;#3B86BD&#34;,&#34;#3E8BBF&#34;,&#34;#3279B6&#34;,&#34;#E9F0F4&#34;,&#34;#7FB7D7&#34;],&#34;weight&#34;:5,&#34;opacity&#34;:0.5,&#34;fill&#34;:true,&#34;fillColor&#34;:[&#34;#F7F7F7&#34;,&#34;#539BC8&#34;,&#34;#418FC1&#34;,&#34;#3F8CC0&#34;,&#34;#4996C5&#34;,&#34;#3984BB&#34;,&#34;#337AB7&#34;,&#34;#3780BA&#34;,&#34;#71AED2&#34;,&#34;#579EC9&#34;,&#34;#357EB8&#34;,&#34;#3984BB&#34;,&#34;#408DC0&#34;,&#34;#74B0D3&#34;,&#34;#579EC9&#34;,&#34;#154F8C&#34;,&#34;#2B71B2&#34;,&#34;#2C72B2&#34;,&#34;#DCEAF2&#34;,&#34;#F8B799&#34;,&#34;#D1E5F0&#34;,&#34;#6EACD1&#34;,&#34;#3077B5&#34;,&#34;#519AC7&#34;,&#34;#4E99C6&#34;,&#34;#A0CCE2&#34;,&#34;#E2EDF3&#34;,&#34;#68A8CF&#34;,&#34;#84BBD9&#34;,&#34;#4393C3&#34;,&#34;#3882BA&#34;,&#34;#2D73B3&#34;,&#34;#E68367&#34;,&#34;#1B599A&#34;,&#34;#84BBD9&#34;,&#34;#8EC2DC&#34;,&#34;#195696&#34;,&#34;#72AED2&#34;,&#34;#64A6CD&#34;,&#34;#DCEAF2&#34;,&#34;#7CB5D6&#34;,&#34;#3279B6&#34;,&#34;#4190C2&#34;,&#34;#5EA1CB&#34;,&#34;#F7F7F7&#34;,&#34;#DCEAF2&#34;,&#34;#A8D0E4&#34;,&#34;#5CA0CA&#34;,&#34;#4B97C5&#34;,&#34;#195696&#34;,&#34;#B5D6E8&#34;,&#34;#B1D4E7&#34;,&#34;#053061&#34;,&#34;#E68367&#34;,&#34;#053061&#34;,&#34;#A1CCE2&#34;,&#34;#62A4CD&#34;,&#34;#4C98C6&#34;,&#34;#97C7DF&#34;,&#34;#D1E5F0&#34;,&#34;#76B1D3&#34;,&#34;#2B71B2&#34;,&#34;#C5DFED&#34;,&#34;#A8D0E4&#34;,&#34;#A8D0E4&#34;,&#34;#F7F7F7&#34;,&#34;#053061&#34;,&#34;#5FA2CB&#34;,&#34;#7FB7D7&#34;,&#34;#2B71B2&#34;,&#34;#BBD9E9&#34;,&#34;#3279B6&#34;,&#34;#7AB4D5&#34;,&#34;#A8D0E4&#34;,&#34;#FAEAE1&#34;,&#34;#D6E7F1&#34;,&#34;#3984BB&#34;,&#34;#9BCAE1&#34;,&#34;#3E8BBF&#34;,&#34;#65A6CE&#34;,&#34;#6EACD1&#34;,&#34;#F7F7F7&#34;,&#34;#7FB7D7&#34;,&#34;#1C5D9F&#34;,&#34;#6EACD1&#34;,&#34;#7FB7D7&#34;,&#34;#3F8CC0&#34;,&#34;#BBD9E9&#34;,&#34;#5CA0CA&#34;,&#34;#3279B6&#34;,&#34;#053061&#34;,&#34;#77B2D4&#34;,&#34;#68A8CE&#34;,&#34;#408EC1&#34;,&#34;#2A6FB1&#34;,&#34;#2C72B2&#34;,&#34;#3780B9&#34;,&#34;#053061&#34;,&#34;#053061&#34;,&#34;#1D5EA1&#34;,&#34;#4291C2&#34;,&#34;#347CB8&#34;,&#34;#4A96C5&#34;,&#34;#2B71B2&#34;,&#34;#053061&#34;,&#34;#579EC9&#34;,&#34;#65A6CE&#34;,&#34;#3178B5&#34;,&#34;#3B86BD&#34;,&#34;#3E8BBF&#34;,&#34;#3279B6&#34;,&#34;#E9F0F4&#34;,&#34;#7FB7D7&#34;],&#34;fillOpacity&#34;:0.7},null,null,null,null,null,{&#34;interactive&#34;:false,&#34;permanent&#34;:false,&#34;direction&#34;:&#34;auto&#34;,&#34;opacity&#34;:1,&#34;offset&#34;:[0,0],&#34;textsize&#34;:&#34;10px&#34;,&#34;textOnly&#34;:false,&#34;className&#34;:&#34;&#34;,&#34;sticky&#34;:true},null]},{&#34;method&#34;:&#34;addCircleMarkers&#34;,&#34;args&#34;:[[40.764,40.766,40.766,40.766,40.766,40.768,40.768,40.768,40.768,40.768,40.768,40.768,40.77,40.77,40.77,40.77,40.77,40.77,40.77,40.772,40.772,40.772,40.772,40.772,40.772,40.772,40.774,40.774,40.774,40.774,40.774,40.774,40.774,40.776,40.776,40.776,40.776,40.776,40.776,40.776,40.778,40.778,40.778,40.778,40.778,40.778,40.78,40.78,40.78,40.78,40.78,40.78,40.78,40.782,40.782,40.782,40.782,40.782,40.782,40.782,40.784,40.784,40.784,40.784,40.784,40.784,40.786,40.786,40.786,40.788,40.788,40.788,40.788,40.788,40.788,40.788,40.79,40.79,40.79,40.79,40.79,40.79,40.79,40.792,40.792,40.792,40.792,40.792,40.792,40.792,40.794,40.794,40.794,40.794,40.794,40.794,40.794,40.794,40.796,40.796,40.796,40.796,40.796,40.796,40.796,40.798,40.798,40.798,40.798,40.798,40.798,40.8,40.8],[-73.974,-73.978,-73.976,-73.974,-73.972,-73.982,-73.98,-73.978,-73.976,-73.974,-73.972,-73.97,-73.98,-73.978,-73.976,-73.974,-73.972,-73.97,-73.968,-73.98,-73.978,-73.976,-73.974,-73.972,-73.97,-73.968,-73.978,-73.976,-73.974,-73.972,-73.97,-73.968,-73.966,-73.976,-73.974,-73.972,-73.97,-73.968,-73.966,-73.964,-73.974,-73.972,-73.97,-73.968,-73.966,-73.964,-73.974,-73.972,-73.97,-73.968,-73.966,-73.964,-73.962,-73.972,-73.97,-73.968,-73.966,-73.964,-73.962,-73.96,-73.97,-73.968,-73.966,-73.964,-73.96,-73.958,-73.97,-73.968,-73.958,-73.968,-73.966,-73.964,-73.962,-73.96,-73.958,-73.956,-73.966,-73.964,-73.962,-73.96,-73.958,-73.956,-73.954,-73.964,-73.962,-73.96,-73.958,-73.956,-73.954,-73.952,-73.964,-73.962,-73.96,-73.958,-73.956,-73.954,-73.952,-73.95,-73.962,-73.96,-73.958,-73.956,-73.954,-73.952,-73.95,-73.96,-73.958,-73.956,-73.954,-73.952,-73.95,-73.958,-73.956],6,null,&#34;Running&#34;,{&#34;interactive&#34;:true,&#34;className&#34;:&#34;&#34;,&#34;stroke&#34;:false,&#34;color&#34;:[&#34;#F7F7F7&#34;,&#34;#2D73B3&#34;,&#34;#5FA2CB&#34;,&#34;#3F8CC0&#34;,&#34;#79B3D4&#34;,&#34;#A8D0E4&#34;,&#34;#3D8ABE&#34;,&#34;#ACD2E5&#34;,&#34;#64A6CD&#34;,&#34;#3077B5&#34;,&#34;#C8E0ED&#34;,&#34;#D7E8F1&#34;,&#34;#367FB9&#34;,&#34;#B5D6E8&#34;,&#34;#3F8CC0&#34;,&#34;#79B3D4&#34;,&#34;#8CC1DC&#34;,&#34;#4E98C6&#34;,&#34;#88BEDA&#34;,&#34;#053061&#34;,&#34;#4393C3&#34;,&#34;#85BBD9&#34;,&#34;#6EACD1&#34;,&#34;#296EB0&#34;,&#34;#3B87BD&#34;,&#34;#88BEDA&#34;,&#34;#F8B799&#34;,&#34;#73AFD2&#34;,&#34;#296FB1&#34;,&#34;#4393C3&#34;,&#34;#408EC1&#34;,&#34;#2064A9&#34;,&#34;#F7F7F7&#34;,&#34;#C8E0ED&#34;,&#34;#66A7CE&#34;,&#34;#529BC7&#34;,&#34;#3984BB&#34;,&#34;#ACD2E5&#34;,&#34;#A8D0E4&#34;,&#34;#3279B6&#34;,&#34;#357EB8&#34;,&#34;#519AC7&#34;,&#34;#4190C2&#34;,&#34;#77B2D4&#34;,&#34;#F7F7F7&#34;,&#34;#408FC1&#34;,&#34;#053061&#34;,&#34;#3E8BBF&#34;,&#34;#6EACD1&#34;,&#34;#B7D7E8&#34;,&#34;#4190C2&#34;,&#34;#539BC8&#34;,&#34;#053061&#34;,&#34;#6EACD1&#34;,&#34;#3D89BE&#34;,&#34;#C6DFED&#34;,&#34;#154F8C&#34;,&#34;#3F8DC0&#34;,&#34;#5FA2CB&#34;,&#34;#D1E5F0&#34;,&#34;#2D73B3&#34;,&#34;#6EACD1&#34;,&#34;#408FC1&#34;,&#34;#A8D0E4&#34;,&#34;#2E75B4&#34;,&#34;#053061&#34;,&#34;#053061&#34;,&#34;#E9F0F4&#34;,&#34;#BBD9E9&#34;,&#34;#053061&#34;,&#34;#7FB7D7&#34;,&#34;#DCEAF2&#34;,&#34;#2E75B4&#34;,&#34;#053061&#34;,&#34;#3E8BBF&#34;,&#34;#3C88BE&#34;,&#34;#6EACD1&#34;,&#34;#88BEDA&#34;,&#34;#7FB7D7&#34;,&#34;#2D73B3&#34;,&#34;#88BEDA&#34;,&#34;#17518F&#34;,&#34;#5CA0CA&#34;,&#34;#4190C2&#34;,&#34;#519AC7&#34;,&#34;#68A8CE&#34;,&#34;#3F8CC0&#34;,&#34;#BBD9E9&#34;,&#34;#5CA0CA&#34;,&#34;#519AC7&#34;,&#34;#053061&#34;,&#34;#DCEAF2&#34;,&#34;#68A8CE&#34;,&#34;#2F76B4&#34;,&#34;#68A8CE&#34;,&#34;#4F99C6&#34;,&#34;#2368AD&#34;,&#34;#053061&#34;,&#34;#BBD9E9&#34;,&#34;#C0DCEB&#34;,&#34;#9DCAE1&#34;,&#34;#6EACD1&#34;,&#34;#E4EEF3&#34;,&#34;#C2DDEC&#34;,&#34;#F8B799&#34;,&#34;#579EC9&#34;,&#34;#3C87BD&#34;,&#34;#E8F0F4&#34;,&#34;#81B9D7&#34;,&#34;#E6EFF4&#34;,&#34;#DCEAF2&#34;,&#34;#E9F0F4&#34;,&#34;#053061&#34;],&#34;weight&#34;:5,&#34;opacity&#34;:0.5,&#34;fill&#34;:true,&#34;fillColor&#34;:[&#34;#F7F7F7&#34;,&#34;#2D73B3&#34;,&#34;#5FA2CB&#34;,&#34;#3F8CC0&#34;,&#34;#79B3D4&#34;,&#34;#A8D0E4&#34;,&#34;#3D8ABE&#34;,&#34;#ACD2E5&#34;,&#34;#64A6CD&#34;,&#34;#3077B5&#34;,&#34;#C8E0ED&#34;,&#34;#D7E8F1&#34;,&#34;#367FB9&#34;,&#34;#B5D6E8&#34;,&#34;#3F8CC0&#34;,&#34;#79B3D4&#34;,&#34;#8CC1DC&#34;,&#34;#4E98C6&#34;,&#34;#88BEDA&#34;,&#34;#053061&#34;,&#34;#4393C3&#34;,&#34;#85BBD9&#34;,&#34;#6EACD1&#34;,&#34;#296EB0&#34;,&#34;#3B87BD&#34;,&#34;#88BEDA&#34;,&#34;#F8B799&#34;,&#34;#73AFD2&#34;,&#34;#296FB1&#34;,&#34;#4393C3&#34;,&#34;#408EC1&#34;,&#34;#2064A9&#34;,&#34;#F7F7F7&#34;,&#34;#C8E0ED&#34;,&#34;#66A7CE&#34;,&#34;#529BC7&#34;,&#34;#3984BB&#34;,&#34;#ACD2E5&#34;,&#34;#A8D0E4&#34;,&#34;#3279B6&#34;,&#34;#357EB8&#34;,&#34;#519AC7&#34;,&#34;#4190C2&#34;,&#34;#77B2D4&#34;,&#34;#F7F7F7&#34;,&#34;#408FC1&#34;,&#34;#053061&#34;,&#34;#3E8BBF&#34;,&#34;#6EACD1&#34;,&#34;#B7D7E8&#34;,&#34;#4190C2&#34;,&#34;#539BC8&#34;,&#34;#053061&#34;,&#34;#6EACD1&#34;,&#34;#3D89BE&#34;,&#34;#C6DFED&#34;,&#34;#154F8C&#34;,&#34;#3F8DC0&#34;,&#34;#5FA2CB&#34;,&#34;#D1E5F0&#34;,&#34;#2D73B3&#34;,&#34;#6EACD1&#34;,&#34;#408FC1&#34;,&#34;#A8D0E4&#34;,&#34;#2E75B4&#34;,&#34;#053061&#34;,&#34;#053061&#34;,&#34;#E9F0F4&#34;,&#34;#BBD9E9&#34;,&#34;#053061&#34;,&#34;#7FB7D7&#34;,&#34;#DCEAF2&#34;,&#34;#2E75B4&#34;,&#34;#053061&#34;,&#34;#3E8BBF&#34;,&#34;#3C88BE&#34;,&#34;#6EACD1&#34;,&#34;#88BEDA&#34;,&#34;#7FB7D7&#34;,&#34;#2D73B3&#34;,&#34;#88BEDA&#34;,&#34;#17518F&#34;,&#34;#5CA0CA&#34;,&#34;#4190C2&#34;,&#34;#519AC7&#34;,&#34;#68A8CE&#34;,&#34;#3F8CC0&#34;,&#34;#BBD9E9&#34;,&#34;#5CA0CA&#34;,&#34;#519AC7&#34;,&#34;#053061&#34;,&#34;#DCEAF2&#34;,&#34;#68A8CE&#34;,&#34;#2F76B4&#34;,&#34;#68A8CE&#34;,&#34;#4F99C6&#34;,&#34;#2368AD&#34;,&#34;#053061&#34;,&#34;#BBD9E9&#34;,&#34;#C0DCEB&#34;,&#34;#9DCAE1&#34;,&#34;#6EACD1&#34;,&#34;#E4EEF3&#34;,&#34;#C2DDEC&#34;,&#34;#F8B799&#34;,&#34;#579EC9&#34;,&#34;#3C87BD&#34;,&#34;#E8F0F4&#34;,&#34;#81B9D7&#34;,&#34;#E6EFF4&#34;,&#34;#DCEAF2&#34;,&#34;#E9F0F4&#34;,&#34;#053061&#34;],&#34;fillOpacity&#34;:0.7},null,null,null,null,null,{&#34;interactive&#34;:false,&#34;permanent&#34;:false,&#34;direction&#34;:&#34;auto&#34;,&#34;opacity&#34;:1,&#34;offset&#34;:[0,0],&#34;textsize&#34;:&#34;10px&#34;,&#34;textOnly&#34;:false,&#34;className&#34;:&#34;&#34;,&#34;sticky&#34;:true},null]},{&#34;method&#34;:&#34;addLayersControl&#34;,&#34;args&#34;:[[&#34;Eating&#34;,&#34;Climbing&#34;,&#34;Running&#34;],[],{&#34;collapsed&#34;:false,&#34;autoZIndex&#34;:true,&#34;position&#34;:&#34;topright&#34;}]},{&#34;method&#34;:&#34;addLegend&#34;,&#34;args&#34;:[{&#34;colors&#34;:[&#34;#053061 , #053061 0%, #4393C3 20%, #D1E5F0 40%, #FDDBC7 60%, #D6604D 80%, #67001F 100%, #67001F &#34;],&#34;labels&#34;:[&#34;0%&#34;,&#34;20%&#34;,&#34;40%&#34;,&#34;60%&#34;,&#34;80%&#34;,&#34;100%&#34;],&#34;na_color&#34;:null,&#34;na_label&#34;:&#34;NA&#34;,&#34;opacity&#34;:0.5,&#34;position&#34;:&#34;bottomright&#34;,&#34;type&#34;:&#34;numeric&#34;,&#34;title&#34;:&#34;Percentage&#34;,&#34;extra&#34;:{&#34;p_1&#34;:0,&#34;p_n&#34;:1},&#34;layerId&#34;:null,&#34;className&#34;:&#34;info legend&#34;,&#34;group&#34;:null}]},{&#34;method&#34;:&#34;addScaleBar&#34;,&#34;args&#34;:[{&#34;maxWidth&#34;:100,&#34;metric&#34;:true,&#34;imperial&#34;:true,&#34;updateWhenIdle&#34;:true,&#34;position&#34;:&#34;bottomleft&#34;}]}],&#34;limits&#34;:{&#34;lat&#34;:[40.764,40.8],&#34;lng&#34;:[-73.982,-73.95]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This is an interactive map that is fully movable and zoomable, provided you are connected to the internet. &lt;em&gt;If you aren’t connected to the internet: how did you reach this website? Teach me your dark and mysterious skills.&lt;/em&gt; Speaking of dark: I chose a dark background map because I like the way it contrasts with the colours of the circles. Looks especially sweet in dark mode. Click on the subgroups in the upper right corner to see the spatial distributions of their percentages.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The code is pretty self explanatory, and I will save a more detailed exploration of leaflet for a future blog post. The different sub-datasets for each action are added as &lt;code&gt;CircleMarkers&lt;/code&gt; using our &lt;code&gt;èxtract_percentages&lt;/code&gt; function. For the colour-coding we define a diverging palette. Each dataset is a layer with a &lt;code&gt;group&lt;/code&gt; attribute which is used to define the switch between them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Among the things we see in the data, there is a tendency for sightings of squirrels eating to be located in the inner part of the park vs the edges. A lot of climbing happens on the east side; and there’s more running in the north. &lt;em&gt;Feel free to supply fitting stereotypes or jokes about Manhattan here - I’m blissfully unaware of any.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For a more serious analysis there would the caveat that some of the spatial bins will have small numbers of squirrels overall, which leads to large uncertainties for those percentages. This is something that would need to be taken into account in a deeper analysis.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Read the &lt;a href=&#34;https://www.tidyverse.org/articles/2019/06/rlang-0-4-0/&#34;&gt;announcement for curly-curly&lt;/a&gt; and the &lt;a href=&#34;https://tidyr.tidyverse.org/articles/pivot.html&#34;&gt;new pivoting vignette&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At the moment, situations where one or more variables are simply quoted are the best use cases for &lt;code&gt;{{ }}&lt;/code&gt;. Beyond that, &lt;a href=&#34;https://stackoverflow.com/questions/56936372/curly-curly-tidy-evaluation-and-modifying-inputs-or-their-names&#34;&gt;operations that require modification of variables&lt;/a&gt; still need &lt;code&gt;enquo&lt;/code&gt;. This might &lt;a href=&#34;http://rpubs.com/lionel-/superstache&#34;&gt;change in the near future&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A great introduction to the capabilities of leaflet can be found &lt;a href=&#34;https://rstudio.github.io/leaflet/&#34;&gt;here&lt;/a&gt;. It is also a popular tool for exploratory Kernels on Kaggle, for instance on &lt;a href=&#34;https://www.kaggle.com/headsortails/be-my-guest-recruit-restaurant-eda&#34;&gt;restaurants in Japan&lt;/a&gt; or &lt;a href=&#34;https://www.kaggle.com/headsortails/nyc-taxi-eda-update-the-fast-the-curious&#34;&gt;Taxi rides in NYC&lt;/a&gt; (&lt;em&gt;for an unknown number of squirrel passengers&lt;/em&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As you might expect, Tidy Tuesday is a weekly challenge with tons of interesting data and many talented participants. Make sure to check out #tidytuesday on Twitter for frequent examples of creative visuals.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The best of both worlds: R meets Python via reticulate</title>
      <link>https://heads0rtai1s.github.io/2019/10/03/reticulate-intro/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2019/10/03/reticulate-intro/</guid>
      <description>
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;As far as rivalries go, R vs Python can almost reach the levels of the glory days of Barca vs Madrid, Stones vs Beatles, or Sega vs Nintendo. Almost. Just dare to venture onto Twitter asking which language is best for data science to witness two tightly entrenched camps.&lt;/em&gt; Or at least that’s what seemingly hundreds of Medium articles would like you believe. In reality, beyond some good-natured and occasionally entertaining joshing, the whole debate is rather silly. Because the question itself is wrong. It’s the whole &lt;em&gt;“My kung fu is better than your kung fu”&lt;/em&gt; mindset that completely misses the point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Because what matters the most is choosing the best tool for the specific job.&lt;/strong&gt; Data challenges can be so diverse that no single language could possibly be best suited to solve them all. It’s like the &lt;a href=&#34;https://en.wikipedia.org/wiki/No_free_lunch_theorem&#34;&gt;no-free-lunch theorem&lt;/a&gt;, only for the tools that build those lunch tools. &lt;em&gt;Which makes it the no-free-kitchen theorem, I suppose … . I shall be working on this analogy.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I argue that data analysis needs to be problem-centric and language-agnostic to tap into its full potential.&lt;/strong&gt; Use whatever language gives you the best equipment to solve your problem. This also prevents you from only having a hammer and treating every problem like a nail. One recent development toward a problem-centric analysis style is the fantastic R package &lt;a href=&#34;https://rstudio.github.io/reticulate/&#34;&gt;reticulate&lt;/a&gt;. This package allows you to mix R and Python code in your data analysis, and to freely pass data between the two languages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The topic of this blog post will be an introductory example on how to use reticulate.&lt;/strong&gt; We will approach a simple supervised classification problem by first exploring the data with &lt;a href=&#34;https://cran.r-project.org/web/packages/ggplot2/index.html&#34;&gt;ggplot2&lt;/a&gt; plots, then turn to Python’s &lt;a href=&#34;https://scikit-learn.org/stable/&#34;&gt;scikit-learn&lt;/a&gt; for modelling, and finally visualise the results again in R.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note: you need at least RStudio version 1.2 to be able to pass objects between R and Python.&lt;/strong&gt; In addition, as always, here are the required packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs &amp;lt;- c(&amp;#39;dplyr&amp;#39;, &amp;#39;tidyr&amp;#39;, &amp;#39;stringr&amp;#39;,  # wrangling
          &amp;#39;knitr&amp;#39;,&amp;#39;kableExtra&amp;#39;,         # table styling
          &amp;#39;ggplot2&amp;#39;,&amp;#39;gridExtra&amp;#39;,        # plots
          &amp;#39;viridis&amp;#39;,                    # visuals styling
          &amp;#39;reticulate&amp;#39;)                 # e pluribus unum
invisible(lapply(libs, library, character.only = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll be using the famous &lt;a href=&#34;https://en.wikipedia.org/wiki/Iris_flower_data_set&#34;&gt;iris dataset&lt;/a&gt;, which is included in R as part of the &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html&#34;&gt;datasets&lt;/a&gt; package. Arguably the Hello World of supervised classification problems, this data describes the length and widths of sepals and petals from 3 different species of iris flower. Sepals are the green parts of a flower that first protect and then support the petals. Just in case you too were wondering that. Here are the first couple rows of the data:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Sepal.Length
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Sepal.Width
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Petal.Length
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Petal.Width
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Species
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is a small dataset with 50 instances each per species of iris flower:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris %&amp;gt;% 
  count(Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   Species        n
##   &amp;lt;fct&amp;gt;      &amp;lt;int&amp;gt;
## 1 setosa        50
## 2 versicolor    50
## 3 virginica     50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a simple example for exploratory data analysis plots we will look at the differences between those 3 species in terms of petal and sepal dimensions. Here, the &lt;a href=&#34;https://cran.r-project.org/web/packages/gridExtra/index.html&#34;&gt;gridExtra&lt;/a&gt; package provides the side-by-side layout:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- iris %&amp;gt;%  
  ggplot(aes(Petal.Length, Petal.Width, color = Species)) +
  geom_point(size = 4) +
  labs(x = &amp;quot;Petal Length&amp;quot;, y = &amp;quot;Petal Width&amp;quot;) +
  scale_color_viridis(discrete = TRUE, option = &amp;quot;viridis&amp;quot;) +
  theme_bw() +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  ggtitle(&amp;quot;Differences in Iris Species&amp;quot;,
          subtitle = str_c(&amp;quot;Petal and Sepal dimensions vary&amp;quot;,
                           &amp;quot;\n&amp;quot;,
                           &amp;quot;significantly between species&amp;quot;))

p2 &amp;lt;- iris %&amp;gt;% 
  ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) +
  geom_point(size = 4) +
  labs(x = &amp;quot;Sepal Length&amp;quot;, y = &amp;quot;Sepal Width&amp;quot;) +
  scale_color_viridis(discrete = TRUE, option = &amp;quot;viridis&amp;quot;) +
  theme_bw() +
  theme(legend.position = &amp;quot;top&amp;quot;)

grid.arrange(p1, p2, layout_matrix = rbind(c(1,2)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2019-10-03-reticulate-intro_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We find that there are clear clusters for each of the species - especially for setosa and in the petal dimensions. A well-trained classifier should be able to distinguish the three iris species. Now, R is perfectly capable of performing this classification task, but for the sake of the excercise we will turn to Python. Given the popularity of both &lt;code&gt;ggplot2&lt;/code&gt; and &lt;code&gt;scikit-learn&lt;/code&gt;, such a workflow is certainly realistic.&lt;/p&gt;
&lt;p&gt;First, we need to tell R where Python can be found. In &lt;code&gt;reticulate&lt;/code&gt;, the &lt;code&gt;use_python&lt;/code&gt; convenience function takes care of that; all we need is a path to the executable. On a Unix-based system, simply open a terminal and type &lt;code&gt;which python&lt;/code&gt;, then paste the resulting path below. (Or look for &lt;code&gt;python3&lt;/code&gt; instead, but this should really become your default version because for Python 2 the &lt;a href=&#34;https://pythonclock.org&#34;&gt;time is running out&lt;/a&gt;). This is my path:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_python(&amp;quot;/usr/bin/python&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you have the combined power of both R and Python at our fingertips. Use it wisely. In Rmarkdown, you can switch each invidual code chunk to the new language by putting &lt;code&gt;{python}&lt;/code&gt; instead of &lt;code&gt;{r}&lt;/code&gt; into the chunk header.&lt;/p&gt;
&lt;p&gt;So, what’s the easiest way to find out that you’re in Python? You suddenly find yourself starting to count from zero:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;foo = [1, 2, 3]
print(foo[0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The real advantage, however, is that we can now pass objects from R to Python, and vice versa. To use R objects in Python we access them using the &lt;code&gt;r&lt;/code&gt; object and Python’s &lt;code&gt;.&lt;/code&gt; (dot) notation. For instance, our &lt;code&gt;iris&lt;/code&gt; dataset will be represented by &lt;code&gt;r.iris&lt;/code&gt;, which is a &lt;code&gt;pandas&lt;/code&gt; data frame:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(r.iris.loc[:5, [&amp;quot;Sepal.Length&amp;quot;, &amp;quot;Species&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Sepal.Length Species
## 0           5.1  setosa
## 1           4.9  setosa
## 2           4.7  setosa
## 3           4.6  setosa
## 4           5.0  setosa
## 5           5.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s prepare a simple &lt;code&gt;scikit-learn&lt;/code&gt; &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html&#34;&gt;decision tree classifier&lt;/a&gt;. First, we import the necessary Python libraries:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we split our iris dataset into train vs test samples using the &lt;code&gt;train_test_split&lt;/code&gt; convenience method. Of course, in real life you want to do the train/test split before looking at the data. For the sake of clarity, we choose to explicitely separate out the predictor features vs the species labels:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;train, test = train_test_split(r.iris,
                test_size = 0.4, random_state = 4321)

X = train.drop(&amp;#39;Species&amp;#39;, axis = 1)
y = train.loc[:, &amp;#39;Species&amp;#39;].values
X_test = test.drop(&amp;#39;Species&amp;#39;, axis = 1)
y_test = test.loc[:, &amp;#39;Species&amp;#39;].values&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those are now Python objects. In order to see and handle them in R you have to use the &lt;code&gt;py$&lt;/code&gt; object. This is the equivalent of the &lt;code&gt;r.&lt;/code&gt; object for working with R variables in Python. For example, because &lt;code&gt;X&lt;/code&gt; is a Python object this R code doesn’t work:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X %&amp;gt;% head(5)  # doesn&amp;#39;t work&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But this R code does the trick:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;py$X %&amp;gt;% head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Sepal.Length Sepal.Width Petal.Length Petal.Width
## 41          4.5         2.3          1.3         0.3
## 16          5.4         3.9          1.3         0.4
## 26          5.0         3.4          1.6         0.4
## 99          5.7         2.8          4.1         1.3
## 5           5.4         3.9          1.7         0.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s switch back to Python code. We wil fit a simple decision tree with &lt;code&gt;sklearn&lt;/code&gt;, apply it to the test set, and visualise the results in R.&lt;/p&gt;
&lt;p&gt;First the fit and prediction. One major advantage of &lt;code&gt;sklearn&lt;/code&gt; is its intuitive and consistent syntax:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tree = DecisionTreeClassifier(random_state=4321)
clf = tree.fit(X, y)
pred = clf.predict(X_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we bring the test predictions back to R and plot some results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;foo &amp;lt;- py$test %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  rename(truth = Species) %&amp;gt;% 
  mutate(predicted = as.factor(py$pred),
         correct = (truth == predicted))

foo %&amp;gt;% 
  head(4) %&amp;gt;% 
  select(-Petal.Length, -Petal.Width) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Sepal.Length
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Sepal.Width
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
truth
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
predicted
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
correct
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
setosa
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
versicolor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
virginica
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
virginica
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
virginica
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1  &amp;lt;- foo %&amp;gt;% 
  select(-correct) %&amp;gt;% 
  gather(truth, predicted, key = type, value = species) %&amp;gt;% 
  ggplot(aes(Petal.Length, Petal.Width, color = species)) +
  geom_point(data = foo %&amp;gt;% filter(correct == FALSE),
             col = &amp;quot;black&amp;quot;, size = 5) +
  geom_point(size = 2) +
  scale_color_viridis(discrete = TRUE, option = &amp;quot;viridis&amp;quot;) +
  theme_bw() +
  theme(legend.position = &amp;quot;none&amp;quot;,
        text = element_text(size = 16)) +
  facet_wrap(~ type)

p2 &amp;lt;- foo %&amp;gt;% 
  select(-correct) %&amp;gt;% 
  gather(truth, predicted, key = type, value = species) %&amp;gt;% 
  ggplot(aes(Sepal.Length, Sepal.Width, color = species)) +
  geom_point(data = foo %&amp;gt;%
               filter(correct == FALSE),
             col = &amp;quot;black&amp;quot;, size = 5) +
  geom_point(size = 2) +
  scale_color_viridis(discrete = TRUE, option = &amp;quot;viridis&amp;quot;,
              guide = guide_legend(direction = &amp;quot;vertical&amp;quot;)) +
  labs(color = &amp;quot;Species&amp;quot;) +
  theme_bw() +
  theme(legend.position = &amp;quot;bottom&amp;quot;,
        text = element_text(size = 16)) +
  facet_wrap(~ type)

p3 &amp;lt;- foo %&amp;gt;% 
  count(truth, predicted) %&amp;gt;%
  complete(truth, predicted, fill = list(n = 0)) %&amp;gt;% 
  group_by(truth) %&amp;gt;% 
  add_tally(n, name = &amp;quot;true&amp;quot;) %&amp;gt;% 
  mutate(accuracy = n/true * 100) %&amp;gt;% 
  ggplot(aes(truth, predicted, fill = accuracy, label = n)) +
  geom_tile() +
  geom_text(size = 5, color = &amp;quot;grey60&amp;quot;) +
  labs(x = &amp;quot;True Species&amp;quot;, y = &amp;quot;Predicted Species&amp;quot;,
       fill = &amp;quot;Accuracy[%]&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;bottom&amp;quot;,
        text = element_text(size = 16),
        axis.text.x = element_text(
          angle=45, hjust=1, vjust=1.1),
        axis.text.y = element_text(
          angle = 45)) +
  scale_fill_viridis(option = &amp;quot;viridis&amp;quot;) +
  ggtitle(&amp;quot;Classification\nDiagnostics&amp;quot;,
          subtitle = str_c(&amp;quot;Left: confusion matrix&amp;quot;,
            &amp;quot;\n&amp;quot;,
            &amp;quot;Right: misclassified\ninstances&amp;quot;))

grid.arrange(p1, p2, p3,
        layout_matrix = cbind(c(3), c(rep(1,2), rep(2,3))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2019-10-03-reticulate-intro_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot layout provides diagnostics for the performance of the classifier:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;On the left, there is a confusion matrix which shows how many test instances of each species were classified as one of the 3 species. The numbers are absolute numbers (remember that this is a small dataset) and the colours encode percentages. For instance, 100% of the 19 setosa instances were correctly classified as setosa. This is the classification accuracy, i.e. the number of true positives. The accuracies for the other two species are pretty high, too; with iris virginica having the lowest proportion of 20 out of 24 instances correctly classified.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the right we show two sets of scatter plots that repeat the overview of petal (top) and sepal (bottom) properties from above. The difference is that now we (i) look at the test set only and (ii) plot the true classes on the right and the predicted classes on the left. The colour-coding is the same for both scatter plots (see legend at the bottom). In addition, all the misclassified instances have a black circle around them to highlight their position.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All in all, our simple classifier does a decent job. The setosas are clearly separated from the rest. And disentangling versicolor vs virginica is not trivial. Of course the performance could be improved, but this is not the topic of this post.&lt;/p&gt;
&lt;p&gt;Because more importantly we saw how the &lt;code&gt;reticulate&lt;/code&gt; approach allows us to seamlessly blend together R and Python code to use the combined power of both worlds.&lt;/p&gt;
&lt;p&gt;So, the next time somebody asks you “Python or R?” just reply with a simple “Yes.” (#inclusiveor).&lt;/p&gt;
&lt;p&gt;More resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For running R code in a Jupyter notebook with Python kernel there is the great &lt;a href=&#34;https://rpy2.bitbucket.io&#34;&gt;rpy2 library&lt;/a&gt; combined with Jupyter’s &lt;a href=&#34;https://ipython.readthedocs.io/en/stable/interactive/magics.html&#34;&gt;line or cell magic&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In R, decision trees are implemented via the &lt;a href=&#34;https://cran.r-project.org/web/packages/rpart/index.html&#34;&gt;rpart package&lt;/a&gt;. For general machine learning infrastructure there are the popular &lt;a href=&#34;https://cran.r-project.org/web/packages/caret/index.html&#34;&gt;caret&lt;/a&gt; and the new &lt;a href=&#34;https://cran.r-project.org/web/packages/tidymodels/index.html&#34;&gt;tidymodels&lt;/a&gt;; both led by developer &lt;a href=&#34;https://github.com/topepo&#34;&gt;Max Kuhn&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For creating visualisations in Python I recommend &lt;a href=&#34;https://seaborn.pydata.org&#34;&gt;seaborn&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Astronomy to Data Science - A Travel Journal</title>
      <link>https://heads0rtai1s.github.io/2019/09/19/first-six-months/</link>
      <pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2019/09/19/first-six-months/</guid>
      <description>


&lt;p&gt;The first six months as a Data Scientist have gone by in a blur of new tools, techniques, and experiences. In my new job, I’m lucky to be (once again) part of a great team of people who are not just smart, but always happy to share their knowledge and feedback in a productive and thoughtful way. A supportive environment helps a lot. &lt;em&gt;Surprising insight - I know.&lt;/em&gt; Worth repeating nonetheless. Now it’s time to look back and reflect on the main challenges and lessons learnt during this initial stage in my journey from &lt;a href=&#34;https://ui.adsabs.harvard.edu/search/fq=%7B!type%3Daqp%20v%3D%24fq_database%7D&amp;amp;fq_database=database%3A%20astronomy&amp;amp;q=author%3A(%22Henze%2C%20Martin%22)&amp;amp;sort=date%20desc%2C%20bibcode%20desc&amp;amp;p_=0&#34;&gt;Academic Astronomy&lt;/a&gt; to &lt;a href=&#34;https://www.linkedin.com/in/martin-henze&#34;&gt;Data Science in the Real World&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/55gMT6FcZiOHiauVt8/giphy.gif&#34; title=&#34;It&amp;#39;s not just fantasy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This post is mainly aimed at astronomers who are interested in making the switch. &lt;em&gt;As always, I’m doing a stellar job in choosing the largest possible target audience.&lt;/em&gt; Still, I think that my thoughts might also be interesting for other academics, young students, or anyone interested in Data Science or Machine Learning. &lt;em&gt;Yeah…, that should cover enough people to propel my fame.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First, let’s start with the tools of the trade:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Programming languages:&lt;/strong&gt; If you want to learn one language then learn &lt;a href=&#34;https://www.python.org/about/gettingstarted/&#34;&gt;Python&lt;/a&gt;. And that’s a hard thing for me to write, because personally I prefer the &lt;a href=&#34;https://www.rstudio.com/online-learning/#r-programming&#34;&gt;R language&lt;/a&gt; with its &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;tidyverse&lt;/a&gt; collection of packages. But Python has a lot going for it: firstly, it has the appeal of being a general purpose programming language and as such is accessible to a vast audience from many different backgrounds. Secondly, it also has an immense repository of specialised libraries. For instance, in astrophysics we have the great &lt;a href=&#34;https://www.astropy.org&#34;&gt;astropy&lt;/a&gt; package. For Machine Learning basics there’s the ubiquitous &lt;a href=&#34;https://scikit-learn.org/&#34;&gt;scikit-learn&lt;/a&gt;. And thirdly: a lot of cutting-edge methods from Neural Network research are being quickly ported into Python frameworks. Those factors mean that today most Data Science teams communicate in Python, and this trend is likely to continue in the near future.&lt;/p&gt;
&lt;p&gt;Straight talk: R is a powerful language but it’s a bit more idiosyncratic (with a somewhat steeper learning curve) and currently less widely used, which gives Python the &lt;a href=&#34;https://www.kaggle.com/headsortails/what-we-do-in-the-kernels-a-kaggle-survey-story&#34;&gt;edge in popularity&lt;/a&gt;. And now is a crucial time for popularity to drive adoption: &lt;a href=&#34;https://stackoverflow.blog/2017/09/06/incredible-growth-python/&#34;&gt;Both R and Python are growing&lt;/a&gt;, yet Python’s growth is exceptional.&lt;/p&gt;
&lt;p&gt;Objectively, R can do everything that Python can do (including e.g. &lt;a href=&#34;https://tensorflow.rstudio.com/&#34;&gt;Tensorflow&lt;/a&gt; and &lt;a href=&#34;https://keras.rstudio.com/&#34;&gt;Keras&lt;/a&gt; wrappers). Many things it can do better, such as beautiful visualisations (e.g. with &lt;a href=&#34;https://www.r-graph-gallery.com/ggplot2-package.html&#34;&gt;ggplot2&lt;/a&gt;) or all the statistics you could possibly ever need in your life. Sometimes it might do things a little slower. But if you really need speed then native Python won’t cut it either. I digress. More importantly: astronomers: forget about IDL. Outside astronomy, no one even knows it exists - and all that legacy code is a weakness rather than a strength. MATLAB is not very popular either. Forget about Fortran (unless you want to write your own Deep Learning Nets or Gradient Boosted Trees). Some C might come in handy. And certainly forget about IRAF and MIDAS. &lt;em&gt;A lot of 80s things are back in fashion but even nostalgia has its limits.&lt;/em&gt; Even if you plan to stay in academia, do your career a favour and learn Python. If you have the resources, learn R and Python.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/3o7aCRloybJlXpNjSU/giphy.gif&#34; title=&#34;See also the next post...&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data bases:&lt;/strong&gt; You will need at least basic skills in one data query language; ideally advanced skills in several ones to boost your productivity. An SQL flavour will be very useful; &lt;a href=&#34;https://www.sqlite.org/&#34;&gt;those&lt;/a&gt; &lt;a href=&#34;https://www.mysql.com/&#34;&gt;are&lt;/a&gt; &lt;a href=&#34;https://www.postgresql.org/&#34;&gt;relatively&lt;/a&gt; &lt;a href=&#34;https://www.microsoft.com/en-us/sql-server/default.aspx&#34;&gt;similar&lt;/a&gt; and transitions between them will be easy. NoSQL tools like &lt;a href=&#34;https://www.elastic.co/products/elasticsearch&#34;&gt;Elasticsearch&lt;/a&gt; can be required, too. Some astrophysics projects have used these frameworks for a while, but in my field we mainly relied on sending fits files back and forth. Having well-designed data base schemata is an immense advantage. Almost all industry jobs will involve the &lt;em&gt;Notorious BFD (Big Freaking Data)&lt;/em&gt; - which will be stored in those very data bases. The more of data selection, cleaning, and merging you can do on those servers, the more efficient is your work. This also includes software like &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Spark&lt;/a&gt;. Successful data preparation is more than half the battle.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Environments:&lt;/strong&gt; Most people in Data Science use &lt;a href=&#34;https://jupyter.org&#34;&gt;Jupyter Notebooks&lt;/a&gt; to share Exploratory Data Analysis, code samples, or proof of concept software snippets. I know that those tools have already achieved some popularity in Astronomy, but it’s worth emphasising how valuable they are for analysing data as a team. As an R user, in addition to using Jupyter, you can have an even smoother experience with &lt;a href=&#34;https://rmarkdown.rstudio.com&#34;&gt;Rmarkdown&lt;/a&gt; - ideally in &lt;a href=&#34;https://www.rstudio.com&#34;&gt;Rstudio&lt;/a&gt; which I can strongly recommend as an IDE. Here is also a good place to mention version control: &lt;a href=&#34;https://git-scm.com&#34;&gt;git&lt;/a&gt; has become the most popular tool, and places like &lt;a href=&#34;https://github.com&#34;&gt;github&lt;/a&gt; or &lt;a href=&#34;https://about.gitlab.com&#34;&gt;gitlab&lt;/a&gt; provide convenient hosting space. These shared spaces are pretty much indispensable for having multiple people working on the same project, but even for your individual projects &lt;code&gt;git&lt;/code&gt; is well worth the &lt;a href=&#34;https://www.codecademy.com/learn/learn-git&#34;&gt;couple hours investment to learn&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the second part, let’s talk about workflow:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Science projects&lt;/strong&gt; can span from a few hours to several weeks. They can also involve only yourself, several data scientists, or other parts of the company such as software engineering or sales. Which aspects dominate depends on your actual position in the team and on your company’s business. Similarly, you can spend different amounts of your time on data cleaning, exploratory analysis, visualisation, or machine learning. In my case, projects typically last weeks - with the occasional request for a couple of hours or days. Coming from an observational astrophysics field focussed on transient sources, I was already used to quickly switching my attention to a new project (when an interesting nova eruption was found). There are obvious parallels here in the shape of client requests, news stories, financial IPOs or similar (semi-) unexpected events that warrant immediate attention. Multi-tasking is important. Month-long projects are rare. The iteration loops are much quicker than what I had known in academia. It’s a bit of a cliché by now, but you really want to fail fast. Time is of the essence and it rarely seems to move linearly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/efU9WbFkGP9NAkLOWn/giphy.gif&#34; title=&#34;Trust me; I am a doctor&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time budgets:&lt;/strong&gt; Within a typical data science project, expect to spend more of your time on data cleaning and quality control than you would have liked. Probably upwards of 50%, including several iterations of identifying outliers and fixing data errors. Real-world data is considerably messier than astrophysical data. &lt;em&gt;Yes, that is possible.&lt;/em&gt; And yes: I have worked with multi-wavelength, multi-telescope projects that produced messy data. A random set of typical industry projects have less in common among them than their astronomical counterparts; and sources of uncertainty and systematics can be plentiful. Data exploration will take up another maybe 25% of your time, including visualisations. The remaining 25% will likely go towards documenting and communicating your findings. I’m including here the time it will take to include your results into a production environment. If you are aiming to be a Machine Learning engineer these tasks will take up a larger chunk of your time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Soft skills:&lt;/strong&gt; Let me stress that it is a valuable and non-trivial skill to be able to communicate your results clearly and succinctly to a variety of audiences. Your team members will care about which statistical tests you used - management or clients will probably not. At academic conferences, we’ve all sat through those presentations that had slides upon slides filled with text and equations and intricate findings, yet little mention of the bigger picture or what can be learned from it. This is what you want to avoid - &lt;em&gt;both as a speaker and an audience member, incidentally.&lt;/em&gt; Adjust your slides to your audience. Together with (well-documented) code samples, the major deliverable of your project is an efficient communication of its results.&lt;/p&gt;
&lt;p&gt;Thus concludes this communication of my impressions and experiences; at least for the moment. There will be more detailed updates on certain key aspects as time progresses. Let’s see what the next 6 months will bring.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/4XPaiZh5DchLG/giphy.gif&#34; title=&#34;Well, next week. Hopefully.&#34; /&gt;&lt;/p&gt;
&lt;p&gt;PS: I’m trying out &lt;em&gt;italics&lt;/em&gt; as a sarcasm/humour indicator. Since written text has no tone; and so on. &lt;em&gt;But I’m sure that was abundantly clear from context.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tidy evaluation in R: Part 2 - Complex use cases (feat. facet zoom)</title>
      <link>https://heads0rtai1s.github.io/2019/08/22/tidy-eval-examples-part2/</link>
      <pubDate>Thu, 22 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2019/08/22/tidy-eval-examples-part2/</guid>
      <description>
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In an &lt;a href=&#34;https://heads0rtai1s.github.io/2019/04/24/tidy-eval-examples/&#34;&gt;earlier post&lt;/a&gt; I gave a gentle introduction to &lt;a href=&#34;https://tidyeval.tidyverse.org/&#34;&gt;tidy evaluation&lt;/a&gt; in the R &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; using simple examples. I covered quoting with &lt;code&gt;enquo&lt;/code&gt; and unquoting with &lt;code&gt;!!&lt;/code&gt; in brief &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt; snippets. Today, I aim to build a collection of more complex use cases involving additional tools.&lt;/p&gt;
&lt;p&gt;Those are our libraries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs &amp;lt;- c(&amp;#39;dplyr&amp;#39;, &amp;#39;stringr&amp;#39;,             # wrangling
          &amp;#39;knitr&amp;#39;,&amp;#39;kableExtra&amp;#39;,           # table styling
          &amp;#39;ggplot2&amp;#39;,&amp;#39;ggforce&amp;#39;)            # plots
invisible(lapply(libs, library, character.only = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, the &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/diamonds.html&#34;&gt;Diamonds dataset&lt;/a&gt; will be our best friend in exploring the depths of tidy eval. Included in the &lt;a href=&#34;https://cran.r-project.org/web/packages/ggplot2/index.html&#34;&gt;ggplot2 package&lt;/a&gt;, this dataset describes the price of 54k diamonds along with their cut, weight, clarity, size, and other relevant properties. Here are the first 4 rows:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
carat
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
cut
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
color
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
clarity
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
depth
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
table
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
price
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
y
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
z
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Ideal
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
E
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SI2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
61.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
55
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
326
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.95
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.43
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Premium
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
E
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SI1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
59.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
61
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
326
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.89
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.84
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.31
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.23
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Good
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
E
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
VS1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
56.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
327
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.05
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.31
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Premium
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
I
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
VS2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
62.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
334
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.63
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Meet &lt;code&gt;enqous&lt;/code&gt; and &lt;code&gt;!!!&lt;/code&gt;:&lt;/strong&gt; The equivalent to &lt;code&gt;enquo&lt;/code&gt; for &lt;strong&gt;quoting more than one variable&lt;/strong&gt; is called &lt;code&gt;enquos&lt;/code&gt;. So far, so plural. The corresponding &lt;strong&gt;unquoting&lt;/strong&gt; method is &lt;code&gt;!!!&lt;/code&gt; - the &lt;em&gt;big bang&lt;/em&gt; operator (remember that &lt;code&gt;!!&lt;/code&gt; is &lt;em&gt;bang-bang&lt;/em&gt;). The tidyverse certainly doesn’t shy away from cosmological superlatives. (The &lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34;&gt;tidyeval cheat sheet&lt;/a&gt; calls it &lt;em&gt;bang-bang-bang&lt;/em&gt;, which makes more intuitive sense but is less poetic; as a trained astronomer my choice is clear.) Here we see both operators in action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group_mean &amp;lt;- function(df, g, x, y){
  
  group_cols &amp;lt;- enquos(x, y)
  mean_col &amp;lt;- enquo(g)
  df %&amp;gt;% 
    group_by(!!! group_cols) %&amp;gt;% 
    summarise(mean = mean(!! mean_col))
}

group_mean(diamonds, price, cut, color) %&amp;gt;% 
  head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 3
## # Groups:   cut [1]
##   cut   color  mean
##   &amp;lt;ord&amp;gt; &amp;lt;ord&amp;gt; &amp;lt;dbl&amp;gt;
## 1 Fair  D     4291.
## 2 Fair  E     3682.
## 3 Fair  F     3827.
## 4 Fair  G     4239.
## 5 Fair  H     5136.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Alternative: use &lt;code&gt;...&lt;/code&gt; aka dots:&lt;/strong&gt; Note, that if all you need to do is group together a bunch of variables (or to treat them as one group in any other way) then R offers the nifty &lt;code&gt;...&lt;/code&gt; operator. You might have seen this style in function definitions or help pages already. With the dots you can capture everything that is not explicitely named and refer to it as one entity. This simplifies our above function in the following way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group_mean &amp;lt;- function(df, g, ...){
  
  mean_col &amp;lt;- enquo(g)
  df %&amp;gt;% 
    group_by(...) %&amp;gt;% 
    summarise(mean = mean(!! mean_col))
}

group_mean(diamonds, price, cut, color) %&amp;gt;% 
  head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 3
## # Groups:   cut [1]
##   cut   color  mean
##   &amp;lt;ord&amp;gt; &amp;lt;ord&amp;gt; &amp;lt;dbl&amp;gt;
## 1 Fair  D     4291.
## 2 Fair  E     3682.
## 3 Fair  F     3827.
## 4 Fair  G     4239.
## 5 Fair  H     5136.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s important to note that &lt;code&gt;!!!&lt;/code&gt; currently doesn’t work in &lt;code&gt;ggplot(aes())&lt;/code&gt;. &lt;a href=&#34;https://stackoverflow.com/questions/55815963/tidyeval-splice-operator-fails-with-ggplots-aes&#34;&gt;There is a workaround&lt;/a&gt; and hopefully soon a fix that I will cover in a future post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The &lt;code&gt;:=&lt;/code&gt; operator:&lt;/strong&gt; to &lt;strong&gt;rename a variable to a quoted name&lt;/strong&gt; you need the &lt;code&gt;:=&lt;/code&gt; operator. Think of it as a maths-style definition if that helps you to remember the syntax. Here’s how it works, giving our mean price variable a custom name:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group_mean &amp;lt;- function(df, g, n, ...){
  
  mean_col &amp;lt;- enquo(g)
  new_name &amp;lt;- enquo(n)
  
  df %&amp;gt;% 
    group_by(...) %&amp;gt;% 
    summarise(!! new_name := mean(!! mean_col))
}

group_mean(diamonds, price, mean_price, cut, color) %&amp;gt;% 
  head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 3
## # Groups:   cut [1]
##   cut   color mean_price
##   &amp;lt;ord&amp;gt; &amp;lt;ord&amp;gt;      &amp;lt;dbl&amp;gt;
## 1 Fair  D          4291.
## 2 Fair  E          3682.
## 3 Fair  F          3827.
## 4 Fair  G          4239.
## 5 Fair  H          5136.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This operator becomes more useful in complex functions or when you are writing your own packages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Encoding strings with &lt;code&gt;ensym&lt;/code&gt;:&lt;/strong&gt; In some scenarios you want to quote your input not as an expression but a symbol. In the context of helper functions this will often involve strings - and a common use case is &lt;code&gt;ggplot2&lt;/code&gt; wrappers. The strings can then be further manipulated for instance with the tidy &lt;a href=&#34;https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html&#34;&gt;&lt;code&gt;stringr&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;In this final example of the post I will showcase the use of &lt;code&gt;ensym&lt;/code&gt; alongside the other main &lt;code&gt;tidyeval&lt;/code&gt; operators. The function will be a &lt;code&gt;ggplot2&lt;/code&gt; convenience wrapper that build a scatter plot of two numerical features colour-coded by a categorical variable. Custom axes labels and plot title will be added. For a little extra flourish, I will add a zoom view on one particular category using the powerful &lt;code&gt;facet_zoom&lt;/code&gt; function from the &lt;a href=&#34;https://cran.r-project.org/web/packages/ggforce/index.html&#34;&gt;&lt;code&gt;ggforce&lt;/code&gt;&lt;/a&gt; package. Here’s what it looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_xy &amp;lt;- function(df, x, y, col, var_zoom, ...){
  
  x &amp;lt;- enquo(x)
  y &amp;lt;- enquo(y)
  col &amp;lt;- enquo(col)
  group_vars &amp;lt;- enquos(...)
  
  dfname &amp;lt;- ensym(df) %&amp;gt;% str_to_sentence()
  xname &amp;lt;- ensym(x) %&amp;gt;% str_to_sentence()
  yname &amp;lt;- ensym(y) %&amp;gt;% str_to_sentence()
  colname &amp;lt;- ensym(col) %&amp;gt;% str_to_sentence()
  
  df %&amp;gt;% 
    mutate(!! col := as.factor(!! col)) %&amp;gt;% 
    group_by(!! col, !!! group_vars) %&amp;gt;% 
    summarise(mean_x = mean(!!x),
              mean_y = mean(!!y)) %&amp;gt;% 
    ungroup() %&amp;gt;% 
    ggplot(aes(mean_x, mean_y, col = !!col)) +
    geom_point() +
    scale_color_brewer(type = &amp;quot;qual&amp;quot;, palette = &amp;quot;Set1&amp;quot;) +
    labs(x = xname, y = yname, col = colname) +
    ggtitle(str_c(dfname, &amp;quot; dataset: &amp;quot;,
                  xname, &amp;quot; vs &amp;quot;, yname,
                  &amp;quot; with colour coding by &amp;quot;, colname),
            subtitle = str_c(&amp;quot;Zoom view to emphasise &amp;quot;,
                             colname, &amp;quot; = &amp;quot;, var_zoom)) +
    facet_zoom(x = (!! col == var_zoom))
}

plot_xy(diamonds, carat, price, clarity, &amp;quot;IF&amp;quot;, color, cut)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2019-08-22-tidy-eval-complex-examples_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s break it down:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; features are encoded using &lt;code&gt;enquo&lt;/code&gt; and &lt;code&gt;!!&lt;/code&gt;, as covered in the &lt;a href=&#34;https://heads0rtai1s.github.io/2019/04/24/tidy-eval-examples/&#34;&gt;previous post&lt;/a&gt;. Those variables will form our scatter plot. But now, they are also encoded using &lt;code&gt;ensym&lt;/code&gt; as &lt;code&gt;xname&lt;/code&gt; and &lt;code&gt;yname&lt;/code&gt;. Those are symbols that we can now use in string functions to build custom plot titles and labels.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;col&lt;/code&gt; feature is also encoded both as a quote and a symbol. This needs to be a categorical feature that we will use to colour-code the data points. The legend is the default style and position. Note, that we use &lt;code&gt;:=&lt;/code&gt; to preserve the column name when transforming this feature from character to factor.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;string_to_sentence&lt;/code&gt; tool, from the &lt;code&gt;stringr&lt;/code&gt; package, simply capitalises our input strings.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Additional grouping variables are encoded using &lt;code&gt;enquos&lt;/code&gt; and spliced into the &lt;code&gt;group_by&lt;/code&gt; call via &lt;code&gt;!!!&lt;/code&gt;. By using the dots &lt;code&gt;...&lt;/code&gt; in the function call we give ourselves the option to use an arbitrary number of grouping features in this function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What the function does, is to group the data by the grouping variables (here: Color and Cut) plus the colour-coding feature (here: Clarity). Then it computes the group mean for the x and y features (here: Carat and Price). It plots these group means in a colour-coded scatter plot.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, it zooms into one particular category of the colour-coding (here: Clarity = “IF”) and provides a magnified view. This zoom view is shown in the lower panel. The upper panel shows the entire data set. Note, that this upper panel has a darker background (and a connecting region) to indicate where the zoom view is located in the overall picture.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The zoom facet is provided by the &lt;code&gt;ggforce&lt;/code&gt; tool &lt;code&gt;facet_zoom&lt;/code&gt; which is very useful for examining specific data points. Here we only zoom into the x-axis, but it can also provide zooms on the y axis or for both axes simultaneously.&lt;/p&gt;
&lt;p&gt;More Resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Rstudio’s excellent &lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34;&gt;cheats sheets&lt;/a&gt; include a tidyeval specimen.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The prolific &lt;a href=&#34;https://community.rstudio.com/&#34;&gt;Rstudio Community&lt;/a&gt; has a tag for &lt;a href=&#34;https://community.rstudio.com/tags/c/tidyverse/tidyeval&#34;&gt;tidyeval questions and solutions&lt;/a&gt;, among many other interesting topics.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Data flow visuals - alluvial vs ggalluvial in R</title>
      <link>https://heads0rtai1s.github.io/2019/06/06/visuals-alluvial-ggalluvial/</link>
      <pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2019/06/06/visuals-alluvial-ggalluvial/</guid>
      <description>
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I have long been a fan of creative data visualisation techniques. For me, the choice of visual representation is driven by both the type of data and the kind of question one wants to examine.&lt;/p&gt;
&lt;p&gt;The power of its visualisation tools has been a major strength of the R language well before the &lt;a href=&#34;https://cran.r-project.org/web/packages/ggplot2/index.html&#34;&gt;ggplot2&lt;/a&gt; package and the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; burst onto the scene. Today’s post will be an introductory examination of two similar packages that allow us to study the connection and &lt;em&gt;flow&lt;/em&gt; of data between different categorical features via &lt;strong&gt;alluvial plots&lt;/strong&gt;. Those packages are &lt;a href=&#34;https://cran.r-project.org/web/packages/alluvial/vignettes/alluvial.html&#34;&gt;alluvial&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/ggalluvial/vignettes/ggalluvial.html&#34;&gt;ggalluvial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All in all we need the following libraries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs &amp;lt;- c(&amp;#39;dplyr&amp;#39;, &amp;#39;stringr&amp;#39;, &amp;#39;forcats&amp;#39;,     # wrangling
          &amp;#39;knitr&amp;#39;,&amp;#39;kableExtra&amp;#39;,               # table styling
          &amp;#39;ggplot2&amp;#39;,&amp;#39;alluvial&amp;#39;,&amp;#39;ggalluvial&amp;#39;,  # plots
          &amp;#39;nycflights13&amp;#39;)                     # data
invisible(lapply(libs, library, character.only = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alluvial plots are best explained by showing one. For illustrating the following examples we will take on board the flights data from the &lt;a href=&#34;https://cran.r-project.org/web/packages/nycflights13/index.html&#34;&gt;nycflights13 library&lt;/a&gt;. This comprehensive data set contains all flights that departed from the New York City airports JFK, LGA, and EWR in 2013. For this analysis, we will only look at three features - the 1st-class features if you will: airport of origin, destination airport, and carrier (i.e. airline code). From the metaphorical front of the cabin, here are the first 4 rows:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
origin
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
carrier
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
dest
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
EWR
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
UA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IAH
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
LGA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
UA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IAH
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
JFK
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
AA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
MIA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
JFK
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
B6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
BQN
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/mbojan/alluvial&#34;&gt;alluvial package&lt;/a&gt; was &lt;a href=&#34;http://bc.bojanorama.pl/2014/03/alluvial-diagrams/&#34;&gt;introduced in 2014&lt;/a&gt; to fill a niché in the landscape of visualisations. I have enjoyed using it in the past in &lt;a href=&#34;https://www.kaggle.com/headsortails/treemap-house-of-horror-spooky-eda-lda-features&#34;&gt;several&lt;/a&gt; &lt;a href=&#34;https://www.kaggle.com/headsortails/nyc-taxi-eda-update-the-fast-the-curious&#34;&gt;Kaggle&lt;/a&gt; &lt;a href=&#34;https://www.kaggle.com/headsortails/steering-wheel-of-fortune-porto-seguro-eda&#34;&gt;Kernels&lt;/a&gt;. Here’s what a plot looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_dest &amp;lt;- flights %&amp;gt;% 
  count(dest) %&amp;gt;% 
  top_n(5, n) %&amp;gt;% 
  pull(dest)

top_carrier &amp;lt;- flights %&amp;gt;% 
  filter(dest %in% top_dest) %&amp;gt;% 
  count(carrier) %&amp;gt;% 
  top_n(4, n) %&amp;gt;% 
  pull(carrier)

fly &amp;lt;- flights %&amp;gt;% 
  filter(dest %in% top_dest &amp;amp; carrier %in% top_carrier) %&amp;gt;% 
  count(origin, carrier, dest) %&amp;gt;% 
  mutate(origin = fct_relevel(as.factor(origin), c(&amp;quot;EWR&amp;quot;, &amp;quot;LGA&amp;quot;, &amp;quot;JFK&amp;quot;)))

alluvial(fly %&amp;gt;% select(-n),
         freq=fly$n, border=NA, alpha = 0.5,
         col=case_when(fly$origin == &amp;quot;JFK&amp;quot; ~ &amp;quot;red&amp;quot;,
                       fly$origin == &amp;quot;EWR&amp;quot; ~ &amp;quot;blue&amp;quot;,
                       TRUE ~ &amp;quot;orange&amp;quot;),
         cex=0.75,
         axis_labels = c(&amp;quot;Origin&amp;quot;, &amp;quot;Carrier&amp;quot;, &amp;quot;Destination&amp;quot;),
         hide = fly$n &amp;lt; 150)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2019-06-06-visuals_alluvial_ggalluvial_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The features are arranged horizontally, with their value counts stacked vertically. This corresponds to a stacked barplot: e.g. for the destinations “BOS” has fewer flights than “LAX”. Here we only look at the top 5 destination and their top 4 carriers (that’s the first two segments of the code above).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The “alluvia” are the bands that connect the features from left to right. Alluvia break down all feature combinations, with complexity increasing also from left to right. These sub-segments are called “flows”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This means that starting from the 3 origin airports on the left there are 4 “flows” each (i.e. 12 in total) connecting to the 4 main carriers. Between carrier and destination these then fan out into 5 flows each for a theoretical total of 60 different flows. In practice, we want to use the &lt;code&gt;hide&lt;/code&gt; parameter to exclude those flows that only have a few observations so that we can focus on the big picture.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For further styling, a &lt;code&gt;border&lt;/code&gt; colour can be assigned to each alluvium. This would allow us to distinguish the different flows on the left side that then break into sub-flows on the right side. Feel free to try it out. Personally, I think the plot looks better without border colours.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We chose a colour coding (argument &lt;code&gt;col&lt;/code&gt;) that puts focus on the origin airports. The first argument of the &lt;code&gt;alluvial&lt;/code&gt; function is the data set, followed by the frequency column (&lt;code&gt;freq&lt;/code&gt;). Note that &lt;code&gt;alluvial&lt;/code&gt; expects the data already to be in the shape of grouped counts (as prepared via &lt;code&gt;count&lt;/code&gt; in the third code segment above).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In my view, the best transparency for alluvia is the default &lt;code&gt;alpha = 0.5&lt;/code&gt;. As usual, &lt;code&gt;cex&lt;/code&gt; does the font scaling and &lt;code&gt;axis_lables&lt;/code&gt; is pretty self-explanatory.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;alluvial&lt;/code&gt; function has an &lt;code&gt;ordering&lt;/code&gt; parameter, but it’s generally better to do the ordering through factor re-levelling when preparing the data (via the tidyverse &lt;a href=&#34;https://cran.r-project.org/web/packages/forcats/&#34;&gt;forcats library&lt;/a&gt;). Here we only change the order for the &lt;code&gt;origin&lt;/code&gt; feature.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, other than looking pretty, what insights does it give us? Well, for instance we see that (for this subset) EWR is dominated by UA (United Airlines) and has almost no AA (American Airlines flights). In turn, UA flights are not frequent in LGA or JFK. Both Boston (BOS) and Los Angeles (LAX) are not connected to LGA (orange). &lt;strong&gt;Thus, the alluvial plot shows us - pretty literally in this case - the flow of flight volume between airports through airline carriers.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, the &lt;code&gt;alluvial&lt;/code&gt; tool has a rather specific syntax and doesn’t integrate seamlessly with the tidyverse. Enter the &lt;a href=&#34;https://github.com/corybrunson/ggalluvial&#34;&gt;&lt;code&gt;ggalluvial&lt;/code&gt; library&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fly %&amp;gt;% 
  mutate(origin = fct_rev(as.factor(origin)),
         carrier = fct_rev(as.factor(carrier)),
         dest = fct_rev(as.factor(dest))) %&amp;gt;% 
  filter(n &amp;gt; 150) %&amp;gt;% 
  ggplot(aes(y = n, axis1 = origin, axis2 = carrier, axis3 = dest)) +
  geom_alluvium(aes(fill = origin), aes.bind=TRUE, width = 1/12) +
  geom_stratum(width = 1/4, fill = &amp;quot;white&amp;quot;, color = &amp;quot;black&amp;quot;) +
  geom_text(stat = &amp;quot;stratum&amp;quot;, label.strata = TRUE) +
  scale_x_discrete(limits = c(&amp;quot;Origin&amp;quot;, &amp;quot;Carrier&amp;quot;, &amp;quot;Destination&amp;quot;),
                   expand = c(.05, .05)) +
  scale_fill_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;orange&amp;quot;, &amp;quot;blue&amp;quot;)) +
  labs(y = &amp;quot;Cases&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  ggtitle(&amp;quot;NYC flights volume for top destinations and airlines&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2019-06-06-visuals_alluvial_ggalluvial_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here I purposefully choose the styling parameters to (broadly) reproduce the above plot. It is evident that &lt;code&gt;ggalluvial&lt;/code&gt; integrates much more smoothly into the &lt;code&gt;ggplot2&lt;/code&gt; grammar. Specifically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The alluvia and the vertical features (the “strata”; here: origin, carrier, and destination) are implemented as different geometry layers. Note, that the default order of the strata features is reversed compared to &lt;code&gt;alluvial&lt;/code&gt;. Also: there are no gaps between the strata here compared to what &lt;code&gt;alluvial&lt;/code&gt; does. This makes it easier to add a y-axis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I decided not to change the default y-axis and subtle background grid lines, which provide quantitative information and guide the eye. Replace &lt;code&gt;theme_minimal()&lt;/code&gt; by &lt;code&gt;theme_void()&lt;/code&gt; to get very close to the &lt;code&gt;alluvial&lt;/code&gt; plot style.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By default, &lt;code&gt;ggalluvial&lt;/code&gt; plots the same number of flows between neighbouring strata. This behaviour can be changed by the &lt;code&gt;aes.bind=TRUE&lt;/code&gt; parameter in &lt;code&gt;geom_alluvial&lt;/code&gt;. Remove it to see the difference with a larger number of narrower flows between the origin and carrier strata.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We are setting the colours manually. One advantage of &lt;code&gt;ggalluvial&lt;/code&gt; is that instead of a manual setting you can use any &lt;code&gt;ggplot2&lt;/code&gt; (or add-on) scale such as &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/scale_brewer.html&#34;&gt;&lt;code&gt;brewer&lt;/code&gt;&lt;/a&gt; or &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/scale_viridis.html&#34;&gt;&lt;code&gt;viridis&lt;/code&gt;&lt;/a&gt;. Similarly we can modify the plot &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/theme.html&#34;&gt;&lt;code&gt;theme&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Instead of &lt;code&gt;geom_text&lt;/code&gt; you can use &lt;code&gt;geom_label&lt;/code&gt;, e.g. in combination with a different &lt;code&gt;fill&lt;/code&gt; colour in &lt;code&gt;geom_stratum&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In closing: both packages are versatile and provide somewhat different approaches to creating alluvial plots. If you are frequently working within the tidyverse then &lt;code&gt;ggalluvial&lt;/code&gt; might be more intuitive for you. Specific (edge) cases might be better handled by one tool than the other.&lt;/p&gt;
&lt;p&gt;For more information check out the respective vignettes for &lt;a href=&#34;https://cran.r-project.org/web/packages/ggalluvial/vignettes/ggalluvial.html&#34;&gt;&lt;code&gt;ggalluvial&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/alluvial/vignettes/alluvial.html&#34;&gt;&lt;code&gt;alluvial&lt;/code&gt;&lt;/a&gt; as well as their &lt;a href=&#34;https://github.com/corybrunson/ggalluvial&#34;&gt;pages&lt;/a&gt; on &lt;a href=&#34;https://github.com/mbojan/alluvial&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Have fun!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tidy evaluation in R - Simple Examples</title>
      <link>https://heads0rtai1s.github.io/2019/04/24/tidy-eval-examples/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2019/04/24/tidy-eval-examples/</guid>
      <description>
&lt;script src=&#34;https://heads0rtai1s.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; philosophy introduced by &lt;a href=&#34;http://hadley.nz/&#34;&gt;Hadley Wickham&lt;/a&gt; has been a game changer for the &lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt; community. It is based on intuitive rules of what a &lt;em&gt;tidy&lt;/em&gt; data set should look like: &lt;em&gt;each variable is a column, each observation is a row&lt;/em&gt; (&lt;a href=&#34;https://www.jstatsoft.org/article/view/v059i10&#34;&gt;Wickham 2014&lt;/a&gt;). At its core, the tidyverse collection of R packages is powered by a consistent grammar of data manipulation and visualisation.&lt;/p&gt;
&lt;p&gt;The tidyverse grammar makes it easier to manipulate data sets using simple expressions that reduce the syntactic overhead and allow you to focus on the data. Thus, packages like &lt;code&gt;dplyr&lt;/code&gt; or &lt;code&gt;tidyr&lt;/code&gt; are great for exploratory data analysis (EDA) and hands-on data wrangling. A small downside of this approach is that these tools require a bit more effort when using them in functions with variable parameters. In general you want to use functions to improve the reusability and reproducibility of your code.&lt;/p&gt;
&lt;p&gt;This is where the &lt;em&gt;tidy evaluation&lt;/em&gt; comes in. A few additional methods and concepts are sufficient to make all your tidy code run smoothly in a function context. Here I will go through some relatively simple examples to get you started.&lt;/p&gt;
&lt;p&gt;Before we begin we will need the following libraries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;libs &amp;lt;- c(&amp;#39;dplyr&amp;#39;, &amp;#39;tibble&amp;#39;,              # wrangling
          &amp;#39;datasets&amp;#39;,                     # data
          &amp;#39;knitr&amp;#39;,&amp;#39;kableExtra&amp;#39;,           # table styling
          &amp;#39;ggplot2&amp;#39;,&amp;#39;gridExtra&amp;#39;)          # plots, panels
invisible(lapply(libs, library, character.only = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use the &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/Orange.html&#34;&gt;Orange data set&lt;/a&gt;, which is part of the &lt;a href=&#34;https://www.rdocumentation.org/packages/datasets/versions/3.5.3&#34;&gt;datasets&lt;/a&gt; package and records the growth of 5 orange trees. Here are the first 5 rows:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Tree
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
age
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
circumference
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
118
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
30
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
484
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
51
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
664
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1004
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
108
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Personally, I’m learning most efficiently by first looking at examples that show the code in action and then tweaking them to fit my needs. After playing with the code for a bit and inevitably breaking something I turn to the docs to understand more about the syntax and additional arguments of the function. Thus, all my posts on tools or methodology will follow the same pattern: I will jump right into the action by looking at a useful yet simple example or two. Next, I dissect this example, maybe break something, and explain the arguments. In closing, there will be a few more complex examples, caveats, pointers, and/or resources. Sounds good? Here we go:&lt;/p&gt;
&lt;p&gt;The first example is a function that takes as &lt;em&gt;input&lt;/em&gt; a data frame &lt;code&gt;df&lt;/code&gt; and a variable &lt;code&gt;var&lt;/code&gt; from that data frame (i.e. a column/feature). The &lt;em&gt;output&lt;/em&gt; is the difference between the (global) median and the mean of the variable. This is a realistic example of a concise helper function, since it goes beyond basic in-built tools and provides a quick check on whether a distribution is symmetric&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median_minus_mean &amp;lt;- function(df, var){
  
  var &amp;lt;- enquo(var)
  
  df %&amp;gt;% 
    summarise(foo = median(!!var) - mean(!!var)) %&amp;gt;% 
    .$foo
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we apply it to the circumference of trees to find that the mean is larger than the median:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median_minus_mean(Orange, circumference)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.8571429&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To understand how it works here are the 2 key concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Quoting:&lt;/strong&gt; In the body of the function, the variable &lt;code&gt;var&lt;/code&gt; is being quoted by the &lt;code&gt;enquo&lt;/code&gt; function (borrowed from the &lt;a href=&#34;https://cran.r-project.org/web/packages/rlang/index.html&#34;&gt;&lt;code&gt;rlang&lt;/code&gt; package&lt;/a&gt;). This essentially means that the &lt;em&gt;content&lt;/em&gt; (or &lt;em&gt;argument&lt;/em&gt;) of the variable is being encoded. The quotation stops this variable from being immediately evaluated. Instead, its content is being treated as a functional R expression.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Unquoting:&lt;/strong&gt; In order to tell a tidyverse verb like &lt;code&gt;summarise&lt;/code&gt; that you are passing it the content of a quoted variable you need to unquote it. Practically you are copy-pasting the variable expression into the verb. This is done using the &lt;code&gt;!!&lt;/code&gt; operator which Hadley wants to be pronounced &lt;em&gt;bang-bang&lt;/em&gt;. I can only surmise that he said that because it makes boring conversations about code sound like wild-west movie fights.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In most situations &lt;code&gt;enquo&lt;/code&gt; and &lt;code&gt;!!&lt;/code&gt; are all you need. Conceptually, there’s a bit more to it since &lt;code&gt;enquo&lt;/code&gt; encodes the current state of the environment along with the variable. This is a useful property, which makes &lt;code&gt;enquo&lt;/code&gt; aware of parameters defined outside a function, but for now you can ignore these finer details.&lt;/p&gt;
&lt;p&gt;(Talking about details: &lt;code&gt;foo&lt;/code&gt; or &lt;code&gt;bar&lt;/code&gt; are popular names for dummy variables in many programming languages. It’s just something that needs a name for the moment but can immediately be forgotten once its time-limited purpose is fulfilled.)&lt;/p&gt;
&lt;p&gt;Also: yes, this works:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median_minus_mean &amp;lt;- function(df, var){
  df %&amp;gt;% 
    summarise(foo = median(!!enquo(var)) - mean(!!enquo(var))) %&amp;gt;% 
    .$foo
}
median_minus_mean(Orange, circumference)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.8571429&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can quote and unquote in the same step. Let’s go a bit further and include grouping by another variable, here the &lt;code&gt;age&lt;/code&gt; of the trees:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median_minus_mean &amp;lt;- function(df, var, gvar){
  
  var &amp;lt;- enquo(var)
  gvar &amp;lt;- enquo(gvar)
  
  df %&amp;gt;% 
    group_by(!!gvar) %&amp;gt;% 
    summarise(foo = median(!!var) - mean(!!var)) %&amp;gt;% 
    .$foo
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median_minus_mean(Orange, circumference, age)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1.0  0.2 -6.2 -9.2 -3.6  0.6  1.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Turns out that for some ages the mean circumference is smaller than the median.&lt;/p&gt;
&lt;p&gt;Good news: quote/unquote also works for ggplot2. Here we quote the x, y, and colour-group variables of our plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_growth_tree &amp;lt;- function(df, xvar, yvar, gvar){
  
  xvar &amp;lt;- enquo(xvar)
  yvar &amp;lt;- enquo(yvar)
  gvar &amp;lt;- enquo(gvar)
  
  df %&amp;gt;% 
    ggplot(aes(!!xvar, !!yvar, col = !!gvar)) +
    geom_line()
}

plot_growth_tree(Orange, age, circumference, Tree)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2019-04-24-tidy-eval-examples_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Some trees grow faster than others.&lt;/p&gt;
&lt;p&gt;In fact, ggplot2 is a great use case because it allows us to quickly built helper functions if we need to repeat a certain plot for many similar features. Individual modification to those templates can be added using the ggplot2 grammar. Here is a histogram example where we add a custom title to the second plot&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_hist &amp;lt;- function(df, var, bins, bcol){
  
  var &amp;lt;- enquo(var)
  
  df %&amp;gt;% 
    ggplot(aes(!!var)) +
    geom_histogram(bins = bins, fill = bcol, col = &amp;quot;black&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- plot_hist(Orange, age, 4, &amp;quot;blue&amp;quot;)
p2 &amp;lt;- plot_hist(Orange, circumference, 7, &amp;quot;red&amp;quot;) +
  ggtitle(&amp;quot;A custom title&amp;quot;)

grid.arrange(p1, p2, layout_matrix = rbind(c(1,2)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://heads0rtai1s.github.io/post/2019-04-24-tidy-eval-examples_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You see that here the number of histogram bins and the plot colour are being passed to the function as normal integer and string - without need of being quoted. This works because these parameters are not R expressions.&lt;/p&gt;
&lt;p&gt;There will be a second post soon about more complex tidy evaluation examples. If you’re interested, watch this space.&lt;/p&gt;
&lt;p&gt;In the meantime: Curious about the bigger picture?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://tidyeval.tidyverse.org/&#34;&gt;tidy evaluation book&lt;/a&gt; is a great starting guide into the concepts.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This &lt;a href=&#34;https://community.rstudio.com/t/interesting-tidy-eval-use-cases/21121/31&#34;&gt;thread&lt;/a&gt; collects some typical use cases for tidy evaluation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For a concise 5 minute intro to the main concepts by the man himself watch Hadley here:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/nERXS3ssntw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;If you are actually interested in the skewness of a distribution you can find a &lt;code&gt;skewness&lt;/code&gt; function in the &lt;a href=&#34;https://cran.r-project.org/package=e1071&#34;&gt;&lt;code&gt;e1071&lt;/code&gt; package&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The arranging of plots into panel layouts is done by the &lt;a href=&#34;https://cran.r-project.org/web/packages/gridExtra/vignettes/arrangeGrob.html&#34;&gt;&lt;code&gt;grid.arrange&lt;/code&gt;&lt;/a&gt; function of the &lt;a href=&#34;https://cran.r-project.org/web/packages/gridExtra/index.html&#34;&gt;&lt;code&gt;gridExtra&lt;/code&gt;&lt;/a&gt; package.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What makes a community great?</title>
      <link>https://heads0rtai1s.github.io/2019/04/19/great-community-kaggledays19/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2019/04/19/great-community-kaggledays19/</guid>
      <description>


&lt;p&gt;The easy answer to this question is: the people. Great people build great communities. Case not quite closed yet, though, because there is more to it. Even the most promising group of individuals needs certain conditions in order to grow into a strong and thriving community. The kind of community that lifts up its members beyond their individual capabilities and becomes more than the sum of their proverbial skills and contributions. I believe that such communities are the cornerstones of all scientific fields, including data science, and that those fields succeed or fail depending on their communities.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://heads0rtai1s.github.io/pics/kdays19_1.jpg&#34; alt=&#34;Kaggle Days audience for welcome talks&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;em&gt;Kaggle Days audience for welcome talks&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Here is the case study that prompted this post: last week I took part in the &lt;a href=&#34;https://kaggledays.com/sanfrancisco/&#34;&gt;Kaggle Days meeting in San Francisco&lt;/a&gt;. Kaggle Days are a new series of local meetings which aim to bring together members of the international, virtual &lt;a href=&#34;https://www.kaggle.com/&#34;&gt;Kaggle&lt;/a&gt; community for in-person workshops, presentations, and competitions. Over the last few years, Kaggle itself has transformed from being “merely” the go-to place for sophisticated machine learning competitions to a multi-faceted online community. The range and depth of user-hosted datasets continues to grow rapidly from month to month, as does a unique repository of machine learning and data science code templates in the form of Kaggle &lt;a href=&#34;https://www.kaggle.com/kernels&#34;&gt;Kernels&lt;/a&gt;: reproducible R/Python notebooks or scripts in a self-contained, cloud-based environment. There really is something for everyone.&lt;/p&gt;
&lt;p&gt;It is fair to say that Kaggle has been a main catalyst for my career change. Joining the platform in early 2017, two years ago, opened my eyes to the multitude of fascinating challenges and problem-solving strategies beyond my narrow academic field. One year later, I had become the first ever &lt;a href=&#34;http://blog.kaggle.com/2018/06/19/tales-from-my-first-year-inside-the-head-of-a-recent-kaggle-addict/&#34;&gt;Kaggle Kernels Grandmaster&lt;/a&gt; - a journey that I plan to revisit in a future post. What drew me into Kaggle, beyond the fun competitions, was a remarkably friendly and supportive community. It’s a rare occurrence to find people who are both extremely smart and happy to help a newcomer in an approachable and relaxed way. Machine Learning can be intimidating, but the people on Kaggle made it fun. Sooner than expected, I started to feel that I had become part of a community which, despite being very competitive, was remarkably efficient at working together to solve hard problems. Especially considering that we all worked on these challenges in our free-time and in different corners of the world.&lt;/p&gt;
&lt;p&gt;All this back story might help to illustrate why I felt rather excited to finally encounter many of my fellow Kagglers in person at the Kaggle Days meetup. Excited and a bit nervous as to how the virtual collaboration would translate to the real world. I had high expectations - which were exceeded spectacularly. Kaggle Days was a blast! Almost the entire &lt;a href=&#34;https://www.kaggle.com/about/team&#34;&gt;Kaggle Team&lt;/a&gt; was present, including CEO &lt;a href=&#34;https://twitter.com/antgoldbloom&#34;&gt;Anthony Goldblum&lt;/a&gt; and Co-Founder &lt;a href=&#34;https://twitter.com/benhamner&#34;&gt;Ben Hamner&lt;/a&gt;. Top Kagglers such as &lt;a href=&#34;https://twitter.com/tunguz&#34;&gt;Bojan Tunguz&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/DmitryLarko&#34;&gt;Dmitry Larko&lt;/a&gt; gave presentations and workshops alongside Machine Learning gurus like &lt;a href=&#34;https://twitter.com/fchollet&#34;&gt;Francois Chollet&lt;/a&gt; (Keras) or &lt;a href=&#34;https://twitter.com/quocleix&#34;&gt;Quoc Le&lt;/a&gt; (Google Brain / AutoML). So many super smart people to listen and talk to! It was great fun.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://heads0rtai1s.github.io/pics/kdays19_2.jpg&#34; alt=&#34;Kaggle Days brainstorming sessions&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;em&gt;Kaggle Days brainstorming sessions&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, as a side effect of my initial Kaggle anonymity (I did not use my real name at all during my first year) I quickly found it more useful to introduce myself as “Heads or Tails”, even though my badge had my actual name on it. I only forgot this when I first met almost the entire Kaggle team at once and needed a second take for a more useful introduction. As this blog shows, I still prefer the “Heads or Tails” moniker for my Data Science personality. Let’s hope that no psychologists read this.&lt;/p&gt;
&lt;p&gt;Ever the hands-on community, the second (and final) day of the Kaggle Days meetup gave us the opportunity to form small teams to participate in an on-site competition. This was particularly interesting for me, since I had never teamed up with others to tackle a Kaggle problem. In a team, there is much more code sharing and discussion than in the open Kaggle forum. And even though there were a few small hiccups in this particular competition (Want a change in metric plus additional data halfway through? Say no more!) working with my team mates was a lot of fun. Shout-out to &lt;a href=&#34;https://www.kaggle.com/thawatt&#34;&gt;Michael&lt;/a&gt; and &lt;a href=&#34;https://www.kaggle.com/gtoubassi&#34;&gt;Garrick&lt;/a&gt; - you guys rock! As a result of Kaggle Days I’m definitely more motivated to team up with others in future competitions. Not just that: I’m more motivated in general to spend time on Kaggle.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://heads0rtai1s.github.io/pics/kdays19_3.jpg&#34; alt=&#34;Kaggle Days competition winners&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;em&gt;Kaggle Days competition winners&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now, why is that exactly? What makes the Kaggle community such a fun place? For me, there are several different factors that all enhance each other when combined:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Kagglers are smart yet down to earth.&lt;/em&gt; &lt;strong&gt;Not only are they happy to share their insights, they really make an effort to do so in an accessible way.&lt;/strong&gt; I recommend anyone who joins a competition not to immediately abandon it after the results are in, but to read the write ups of the top teams which are usually of high detail and quality. There is lots to learn from such a post mortem.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;No big egos in the community.&lt;/em&gt; This is related to the previous point but touches on a different aspect. Even though our community has its own big names who’s opinions (deservedly) have weight in discussions, nobody thinks they are more important than others. This is crucial because it lowers the threshold for beginners (and anyone) to ask questions. Asking questions is what drives the improvement of individuals and the community. As a side note: In my time in academia I have come across some really big egos, although luckily never in immediate collaboration. Although these people are very smart you really don’t want to be around them for longer than absolutely necessary. No gossip here - moving on:&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;A common goal.&lt;/em&gt; As soon as you meet other Kagglers you have something to talk about; be it an ongoing competition, the new Kernels interface, or the most recent Machine Learning tools. But it goes beyond that: competitions are the best example for creating a specific goal that everyone can focus on and contribute to, to the best of their abilities. And while sharing is encouraged, there is plenty of competition at the highest level. The last days of a competition are one of the most intense examples of a singular focus in an online community of hundreds to ten thousands of people from all over the world.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Diversity.&lt;/em&gt; Speaking of ‘all over the world’. Kagglers come from a large number of countries and have many different backgrounds. It is true that we are still predominantly male and STEM based, and we are working on becoming more inclusive towards many other groups. You can think about it this way: When doing Machine Learning, diversity is a big advantage. If you average over several models then your results will be better the less similarity these models have (i.e. the less collinearity there is). An insight that is missed by one model might be picked up by a different method. The likelihood that all models will overfit in the same direction is smaller. And the same is true for communities. Different points of view help us to challenge pre-conceived beliefs and broaden our horizons. For deeper insights into the diversity of Kagglers you can check out my analysis, and those of many others, of the latest &lt;a href=&#34;https://www.kaggle.com/headsortails/what-we-do-in-the-kernels-a-kaggle-survey-story&#34;&gt;Kaggle Survey&lt;/a&gt; which, true to form, is a detailed annual assessment of the state of the community.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;People care about the community.&lt;/em&gt; This is one of the most important factors. It might somewhat derive from the points above but it’s by no means a given. I have been part of (and witness to) passionate discussions in the Kaggle forums about difficult issues in the community. Often Kagglers themselves have suggested solutions to problems that the administrative team might not have been aware of. And even if tempers flare up, which is more understandable in a competitive context, there is mutual respect and usually a (virtual) handshake once the dust has settled. Kaggle is &lt;em&gt;our&lt;/em&gt; community and we care about keeping it friendly and welcoming.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Infrastructure for collaboration and communication.&lt;/em&gt; Last but not least, for a community to function well there need to be tools and environments in place that allow for efficient communication. The lower the thresholds are for exchanging information the better it will work. Ideally, the infrastructure should be designed in a way that encourages different ways of interaction for the community members. This further promotes a welcoming and inclusive atmosphere. Kaggle provides all this through discussion forums (general ones and those specific to each competition or data set). In addition, the aforementioned Kernel notebooks have comments enabled, which is a great way to show appreciation to an author’s work or ask for clarification. From my point of view, commenting on Kernels is great, low-threshold way of starting to actively participate in the community. And I can guarantee that Kernel authors appreciated feedback. My own Kernels have frequently been improved by people kind enough to post their ideas and suggestions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A special kind of infrastructure is an in-person meetup like Kaggle Days. Remote interaction works fine, but from my experience there is a certain extra factor in face-to-face meetings. During my time in academia I had the privilege to be part of many successful teams and to work with many smart people. The highlights of these collaborations were always our team meetings in which we could brainstorm new ideas and strategies. Sometimes at a hotel pool, sometimes late at night over drinks or pizza; but always with extra energy and creativity. And when your creativity goes into overdrive then you have found a great community.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Finally, this post doesn’t feel complete without highlighting the remarkable way in which the Data Science community (and especially the R community) is responding to the disgraceful case of sexual harassment and the botched attempt at a cover up at &lt;a href=&#34;https://dhavide.github.io/a-note-to-our-commuity-on-building-trust.html&#34;&gt;DataCamp&lt;/a&gt;. The community is supporting the victim and former employees who spoke out and were fired. Many content creators are pulling their courses from DataCamp to push for necessary change. Here is a community actively working to transform bad practices that those who are primarily responsible are repeatedly failing to address. Because I’m an optimist at heart, I want to close by pointing to one of the most remarkable products of this sorry situation: a free natural language programming (NLP) course plus a great interactive app template build by &lt;a href=&#34;https://twitter.com/_inesmontani&#34;&gt;Ines Montani&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Like many of you, I&amp;#39;m incredibly disappointed by DataCamp. I wanted to make a free version of my spaCy course so you don&amp;#39;t have to sign up for their service – and ended up building my own interactive app. Powered by the awesome &lt;a href=&#34;https://twitter.com/mybinderteam?ref_src=twsrc%5Etfw&#34;&gt;@mybinderteam&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/GatsbyJS?ref_src=twsrc%5Etfw&#34;&gt;@gatsbyjs&lt;/a&gt; 💖 &lt;a href=&#34;https://t.co/2QOuDPoZEX&#34;&gt;https://t.co/2QOuDPoZEX&lt;/a&gt;&lt;/p&gt;&amp;mdash; Ines Montani 〰️ (@_inesmontani) &lt;a href=&#34;https://twitter.com/_inesmontani/status/1118508634357604353?ref_src=twsrc%5Etfw&#34;&gt;April 17, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</description>
    </item>
    
    <item>
      <title>Prologue</title>
      <link>https://heads0rtai1s.github.io/2019/04/08/prologue/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://heads0rtai1s.github.io/2019/04/08/prologue/</guid>
      <description>


&lt;p&gt;This one is all about &lt;em&gt;change&lt;/em&gt;. In many ways life is about change. Not all change is good, but without it there would be no evolution. No progress. And as much as we sometimes want certain moments to last, stagnation is rarely a good thing. Without attempting something new it is impossible to say whether it will succeed. I guess that’s a verbose way of saying: I decided to start a blog.&lt;/p&gt;
&lt;p&gt;The blog is far from the only change, though. After a good ten years of working in astronomy and astrophysics I have just made the transition into data science. Instead of exploding stars in nearby galaxies I will now be working on topics that are much closer to our every day experience; such as travel, consumer trends, or demographics. Yeah, this is a bigger change than the blog. Maybe I should have led with that.&lt;/p&gt;
&lt;p&gt;My reasons for changing careers are manifold. First and foremost I’m driven by curiosity. I want to understand the world, and beyond, and figure out how everything works. What motivates people, makes economies flourish, or lightens up entire suns like a cosmic firework. And after gazing out into the universe for quite some time, maybe it is time to work a little closer to home.&lt;/p&gt;
&lt;p&gt;In my field of academia, anything beyond a traditional research career is still regarded as a somewhat unusual alternative path; even though only a small percentage of astrophysics PhDs will end up in a tenured (professor) position at a university. Thus, I hope that these Chronicles of an Astronomer’s Adventures in Data Science (hopefully the title of the eventual movie; take note Hollywood) will be useful for other young astronomers who are contemplating their career options. Or it might even make them aware of the fact that they have career options. Academia, for all it’s advantages, can still be rather insular and does not always do the best job in emphasising transferable skills.&lt;/p&gt;
&lt;p&gt;And many astronomers have a whole bag of skills they can transfer. Observational astronomers, like me, turns out are rather good at analysing data. Sometimes big data, often messy data. Different telescopes can have very different ways of data collection and storage. I might share some X-ray imaging data in a future throwback post. While a typical astronomer’s education focusses heavily on the (astro-) physics and the specific instrument software, many of us have taught themselves programming, statistics, data bases, or other useful things practically on the side.&lt;/p&gt;
&lt;p&gt;Which brings me to another of my motivations: to understand the tools that help us understand the world. I’m a big fan of data visualisation to extract its secrets and hidden insights. There will be many pictures here; hopefully some pretty ones among them. And while the best tool for a certain job might be a very specific one, many tools are surprisingly versatile. Languages like Python and R. Data base tools like the various SQL flavours. Data viz libraries like ggplot2, matplotlib, or D3. Those will be among the frequent Dramatis Personae in this blog.&lt;/p&gt;
&lt;p&gt;I will aim for at least weekly posts, describing my impressions of working in data science and the contrast to academia. I’ll give it a decent try not to reveal any industry secrets. There aren’t really many “academia secrets”, since everything interesting get published sooner rather than later. I guess that’s one contrast. There will be the occassional update about new astro results, some throwback posts, and any other insights I find noteworthy.&lt;/p&gt;
&lt;p&gt;The world, after all, is always changing. I was born in East Germany when it was still a separate country - locked in ideological contest with its western brother. When I was a kid, a peaceful revolution swept away the old, out-of-touch system and then, from one day to the next, the borders were open and change was as fundamental as it was inevitable. Down the line, this taught me that there aren’t many things you can take for granted, but as long as you are prepared for change and willing to adapt there can be much potential in every new twist and turn.&lt;/p&gt;
&lt;p&gt;So this is where I come from. Now let’s see what happens next.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
